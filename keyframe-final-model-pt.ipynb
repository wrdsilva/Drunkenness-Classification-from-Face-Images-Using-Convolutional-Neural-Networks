{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7456024f",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-06-14T19:26:16.378080Z",
     "iopub.status.busy": "2022-06-14T19:26:16.377601Z",
     "iopub.status.idle": "2022-06-14T19:30:12.215683Z",
     "shell.execute_reply": "2022-06-14T19:30:12.214881Z"
    },
    "papermill": {
     "duration": 235.844405,
     "end_time": "2022-06-14T19:30:12.217851",
     "exception": false,
     "start_time": "2022-06-14T19:26:16.373446",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-14 19:26:23.204247: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-14 19:26:23.261749: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-14 19:26:23.360240: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-14 19:26:23.361004: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-14 19:26:25.704504: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-14 19:26:25.705372: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-14 19:26:25.706110: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-14 19:26:25.706710: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /device:GPU:0 with 15403 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n",
      "2022-06-14 19:26:25.712237: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-14 19:26:25.712979: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-14 19:26:25.713621: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 9152157816673871935\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 16152002560\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 9824952314838417804\n",
      "physical_device_desc: \"device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\"\n",
      "]\n",
      "Num GPUs Available:  1\n",
      "Loading Sober-Drunk Face Dataset, from Patras University\n",
      "\n",
      "Samples total: 160\n",
      "\n",
      "Dataset splitting: \n",
      "\n",
      "Training set: 140\n",
      "Test set: 20\n",
      "\n",
      "MinMax normalization\n",
      "dataset shape:  (140, 128, 160, 3)\n",
      "min:  0.0\n",
      "max:  1.0\n",
      "\n",
      "MinMax normalization\n",
      "dataset shape:  (20, 128, 160, 3)\n",
      "min:  0.0\n",
      "max:  1.0\n",
      "\n",
      " Loading pre-trained model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-14 19:26:26.168864: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-14 19:26:26.169836: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-14 19:26:26.170523: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-14 19:26:26.171293: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-14 19:26:26.172027: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-14 19:26:26.172659: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15403 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 128, 160, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 128, 160, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 128, 160, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 64, 80, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 64, 80, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 64, 80, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 32, 40, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 32, 40, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 32, 40, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 32, 40, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 16, 20, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 16, 20, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 16, 20, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 16, 20, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 8, 10, 512)        0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 8, 10, 512)        2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 8, 10, 512)        2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 8, 10, 512)        2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 4, 5, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 10240)             0         \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 1)                 10241     \n",
      "=================================================================\n",
      "Total params: 14,724,929\n",
      "Trainable params: 12,989,441\n",
      "Non-trainable params: 1,735,488\n",
      "_________________________________________________________________\n",
      "\n",
      "Removing top layers\n",
      "block1_conv1: False\n",
      "block1_conv2: False\n",
      "block1_pool: False\n",
      "block2_conv1: False\n",
      "block2_conv2: False\n",
      "block2_pool: False\n",
      "block3_conv1: False\n",
      "block3_conv2: False\n",
      "block3_conv3: False\n",
      "block3_pool: False\n",
      "block4_conv1: False\n",
      "block4_conv2: False\n",
      "block4_conv3: False\n",
      "block4_pool: False\n",
      "block5_conv1: False\n",
      "block5_conv2: False\n",
      "block5_conv3: False\n",
      "block5_pool: False\n",
      "\n",
      "Adding new dense layers\n",
      "\n",
      "block1_conv1: False\n",
      "block1_conv2: False\n",
      "block1_pool: False\n",
      "block2_conv1: False\n",
      "block2_conv2: False\n",
      "block2_pool: False\n",
      "block3_conv1: False\n",
      "block3_conv2: False\n",
      "block3_conv3: False\n",
      "block3_pool: False\n",
      "block4_conv1: False\n",
      "block4_conv2: False\n",
      "block4_conv3: False\n",
      "block4_pool: False\n",
      "block5_conv1: False\n",
      "block5_conv2: False\n",
      "block5_conv3: False\n",
      "block5_pool: False\n",
      "flatten: True\n",
      "dense_layer_1: True\n",
      "output: True\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "block1_conv1 (Conv2D)        (None, 128, 160, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 128, 160, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 64, 80, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 64, 80, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 64, 80, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 32, 40, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 32, 40, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 32, 40, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 32, 40, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 16, 20, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 16, 20, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 16, 20, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 16, 20, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 8, 10, 512)        0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 8, 10, 512)        2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 8, 10, 512)        2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 8, 10, 512)        2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 4, 5, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 10240)             0         \n",
      "_________________________________________________________________\n",
      "dense_layer_1 (Dense)        (None, 10)                102410    \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 14,817,109\n",
      "Trainable params: 102,421\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n",
      "\n",
      "\n",
      "\n",
      "Fold  0\n",
      "\n",
      "Training with 120 samples and validating with 20 samples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-14 19:26:28.922672: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-14 19:26:30.484625: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 11s 1s/step - loss: 0.7522 - accuracy: 0.7500 - val_loss: 0.5910 - val_accuracy: 0.7500\n",
      "Epoch 2/205\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 0.7419 - accuracy: 0.7500 - val_loss: 0.5989 - val_accuracy: 0.7500\n",
      "Epoch 3/205\n",
      "2/2 [==============================] - 0s 111ms/step - loss: 0.7332 - accuracy: 0.7333 - val_loss: 0.6078 - val_accuracy: 0.7500\n",
      "Epoch 4/205\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.7254 - accuracy: 0.7500 - val_loss: 0.6171 - val_accuracy: 0.7500\n",
      "Epoch 5/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.7187 - accuracy: 0.7583 - val_loss: 0.6268 - val_accuracy: 0.7000\n",
      "Epoch 6/205\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 0.7131 - accuracy: 0.7500 - val_loss: 0.6364 - val_accuracy: 0.7500\n",
      "Epoch 7/205\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 0.7085 - accuracy: 0.7000 - val_loss: 0.6456 - val_accuracy: 0.7500\n",
      "Epoch 8/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.7046 - accuracy: 0.6833 - val_loss: 0.6543 - val_accuracy: 0.7000\n",
      "Epoch 9/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.7013 - accuracy: 0.6583 - val_loss: 0.6623 - val_accuracy: 0.6000\n",
      "Epoch 10/205\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 0.6984 - accuracy: 0.6417 - val_loss: 0.6696 - val_accuracy: 0.6000\n",
      "Epoch 11/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.6960 - accuracy: 0.6333 - val_loss: 0.6760 - val_accuracy: 0.6000\n",
      "Epoch 12/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.6938 - accuracy: 0.6250 - val_loss: 0.6814 - val_accuracy: 0.5000\n",
      "Epoch 13/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.6919 - accuracy: 0.6167 - val_loss: 0.6859 - val_accuracy: 0.4500\n",
      "Epoch 14/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.6902 - accuracy: 0.6000 - val_loss: 0.6896 - val_accuracy: 0.4500\n",
      "Epoch 15/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.6887 - accuracy: 0.5750 - val_loss: 0.6926 - val_accuracy: 0.5000\n",
      "Epoch 16/205\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 0.6873 - accuracy: 0.5417 - val_loss: 0.6949 - val_accuracy: 0.4500\n",
      "Epoch 17/205\n",
      "2/2 [==============================] - 0s 126ms/step - loss: 0.6859 - accuracy: 0.5500 - val_loss: 0.6967 - val_accuracy: 0.5000\n",
      "Epoch 18/205\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.6845 - accuracy: 0.5583 - val_loss: 0.6980 - val_accuracy: 0.5000\n",
      "Epoch 19/205\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.6831 - accuracy: 0.5667 - val_loss: 0.6990 - val_accuracy: 0.4500\n",
      "Epoch 20/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.6817 - accuracy: 0.5667 - val_loss: 0.6997 - val_accuracy: 0.4500\n",
      "Epoch 21/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.6802 - accuracy: 0.5750 - val_loss: 0.6999 - val_accuracy: 0.4500\n",
      "Epoch 22/205\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.6788 - accuracy: 0.5750 - val_loss: 0.6997 - val_accuracy: 0.4500\n",
      "Epoch 23/205\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 0.6773 - accuracy: 0.5750 - val_loss: 0.6990 - val_accuracy: 0.4500\n",
      "Epoch 24/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.6758 - accuracy: 0.5833 - val_loss: 0.6977 - val_accuracy: 0.4500\n",
      "Epoch 25/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.6744 - accuracy: 0.5833 - val_loss: 0.6962 - val_accuracy: 0.4500\n",
      "Epoch 26/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.6729 - accuracy: 0.5833 - val_loss: 0.6946 - val_accuracy: 0.4000\n",
      "Epoch 27/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.6715 - accuracy: 0.6000 - val_loss: 0.6932 - val_accuracy: 0.4500\n",
      "Epoch 28/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.6701 - accuracy: 0.6083 - val_loss: 0.6918 - val_accuracy: 0.5000\n",
      "Epoch 29/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.6686 - accuracy: 0.6167 - val_loss: 0.6905 - val_accuracy: 0.5000\n",
      "Epoch 30/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.6672 - accuracy: 0.6250 - val_loss: 0.6893 - val_accuracy: 0.5000\n",
      "Epoch 31/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.6658 - accuracy: 0.6417 - val_loss: 0.6879 - val_accuracy: 0.5000\n",
      "Epoch 32/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.6644 - accuracy: 0.6417 - val_loss: 0.6867 - val_accuracy: 0.5000\n",
      "Epoch 33/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.6630 - accuracy: 0.6417 - val_loss: 0.6855 - val_accuracy: 0.5500\n",
      "Epoch 34/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.6616 - accuracy: 0.6417 - val_loss: 0.6844 - val_accuracy: 0.6000\n",
      "Epoch 35/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.6602 - accuracy: 0.6500 - val_loss: 0.6835 - val_accuracy: 0.6000\n",
      "Epoch 36/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.6588 - accuracy: 0.6583 - val_loss: 0.6828 - val_accuracy: 0.6000\n",
      "Epoch 37/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.6573 - accuracy: 0.6833 - val_loss: 0.6822 - val_accuracy: 0.6000\n",
      "Epoch 38/205\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.6559 - accuracy: 0.7000 - val_loss: 0.6816 - val_accuracy: 0.6000\n",
      "Epoch 39/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.6545 - accuracy: 0.7000 - val_loss: 0.6809 - val_accuracy: 0.6000\n",
      "Epoch 40/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.6530 - accuracy: 0.7083 - val_loss: 0.6803 - val_accuracy: 0.6000\n",
      "Epoch 41/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.6516 - accuracy: 0.7167 - val_loss: 0.6799 - val_accuracy: 0.6000\n",
      "Epoch 42/205\n",
      "2/2 [==============================] - 0s 127ms/step - loss: 0.6502 - accuracy: 0.7167 - val_loss: 0.6795 - val_accuracy: 0.6000\n",
      "Epoch 43/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.6488 - accuracy: 0.7333 - val_loss: 0.6787 - val_accuracy: 0.6000\n",
      "Epoch 44/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.6474 - accuracy: 0.7333 - val_loss: 0.6780 - val_accuracy: 0.6000\n",
      "Epoch 45/205\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 0.6460 - accuracy: 0.7333 - val_loss: 0.6776 - val_accuracy: 0.6500\n",
      "Epoch 46/205\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.6446 - accuracy: 0.7333 - val_loss: 0.6774 - val_accuracy: 0.6500\n",
      "Epoch 47/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.6431 - accuracy: 0.7250 - val_loss: 0.6768 - val_accuracy: 0.6500\n",
      "Epoch 48/205\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 0.6417 - accuracy: 0.7250 - val_loss: 0.6761 - val_accuracy: 0.6500\n",
      "Epoch 49/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.6402 - accuracy: 0.7417 - val_loss: 0.6755 - val_accuracy: 0.6500\n",
      "Epoch 50/205\n",
      "2/2 [==============================] - 0s 124ms/step - loss: 0.6388 - accuracy: 0.7417 - val_loss: 0.6750 - val_accuracy: 0.6500\n",
      "Epoch 51/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.6374 - accuracy: 0.7583 - val_loss: 0.6744 - val_accuracy: 0.6500\n",
      "Epoch 52/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.6360 - accuracy: 0.7583 - val_loss: 0.6739 - val_accuracy: 0.6500\n",
      "Epoch 53/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.6346 - accuracy: 0.7667 - val_loss: 0.6733 - val_accuracy: 0.6500\n",
      "Epoch 54/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.6332 - accuracy: 0.7833 - val_loss: 0.6729 - val_accuracy: 0.6500\n",
      "Epoch 55/205\n",
      "2/2 [==============================] - 0s 125ms/step - loss: 0.6318 - accuracy: 0.7833 - val_loss: 0.6725 - val_accuracy: 0.6500\n",
      "Epoch 56/205\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.6305 - accuracy: 0.7833 - val_loss: 0.6716 - val_accuracy: 0.6500\n",
      "Epoch 57/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.6292 - accuracy: 0.7833 - val_loss: 0.6706 - val_accuracy: 0.6500\n",
      "Epoch 58/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.6278 - accuracy: 0.7833 - val_loss: 0.6702 - val_accuracy: 0.6500\n",
      "Epoch 59/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.6264 - accuracy: 0.7917 - val_loss: 0.6701 - val_accuracy: 0.6500\n",
      "Epoch 60/205\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.6251 - accuracy: 0.8000 - val_loss: 0.6695 - val_accuracy: 0.6500\n",
      "Epoch 61/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.6238 - accuracy: 0.8000 - val_loss: 0.6685 - val_accuracy: 0.6500\n",
      "Epoch 62/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.6225 - accuracy: 0.8000 - val_loss: 0.6677 - val_accuracy: 0.6500\n",
      "Epoch 63/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.6211 - accuracy: 0.8000 - val_loss: 0.6674 - val_accuracy: 0.6500\n",
      "Epoch 64/205\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.6198 - accuracy: 0.7917 - val_loss: 0.6669 - val_accuracy: 0.6000\n",
      "Epoch 65/205\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.6185 - accuracy: 0.8000 - val_loss: 0.6662 - val_accuracy: 0.6000\n",
      "Epoch 66/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.6172 - accuracy: 0.8083 - val_loss: 0.6657 - val_accuracy: 0.6000\n",
      "Epoch 67/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.6159 - accuracy: 0.8167 - val_loss: 0.6656 - val_accuracy: 0.6000\n",
      "Epoch 68/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.6147 - accuracy: 0.8167 - val_loss: 0.6652 - val_accuracy: 0.6000\n",
      "Epoch 69/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.6134 - accuracy: 0.8167 - val_loss: 0.6642 - val_accuracy: 0.6000\n",
      "Epoch 70/205\n",
      "2/2 [==============================] - 0s 128ms/step - loss: 0.6122 - accuracy: 0.8167 - val_loss: 0.6635 - val_accuracy: 0.6000\n",
      "Epoch 71/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.6109 - accuracy: 0.8167 - val_loss: 0.6632 - val_accuracy: 0.6000\n",
      "Epoch 72/205\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.6097 - accuracy: 0.8250 - val_loss: 0.6628 - val_accuracy: 0.6000\n",
      "Epoch 73/205\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.6085 - accuracy: 0.8250 - val_loss: 0.6622 - val_accuracy: 0.6000\n",
      "Epoch 74/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.6073 - accuracy: 0.8250 - val_loss: 0.6617 - val_accuracy: 0.6000\n",
      "Epoch 75/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.6060 - accuracy: 0.8250 - val_loss: 0.6617 - val_accuracy: 0.6000\n",
      "Epoch 76/205\n",
      "2/2 [==============================] - 0s 118ms/step - loss: 0.6048 - accuracy: 0.8250 - val_loss: 0.6617 - val_accuracy: 0.6000\n",
      "Epoch 77/205\n",
      "2/2 [==============================] - 0s 157ms/step - loss: 0.6037 - accuracy: 0.8250 - val_loss: 0.6610 - val_accuracy: 0.6000\n",
      "Epoch 78/205\n",
      "2/2 [==============================] - 0s 168ms/step - loss: 0.6025 - accuracy: 0.8167 - val_loss: 0.6603 - val_accuracy: 0.6500\n",
      "Epoch 79/205\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.6013 - accuracy: 0.8167 - val_loss: 0.6604 - val_accuracy: 0.6000\n",
      "Epoch 80/205\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.6001 - accuracy: 0.8167 - val_loss: 0.6606 - val_accuracy: 0.6000\n",
      "Epoch 81/205\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.5990 - accuracy: 0.8083 - val_loss: 0.6602 - val_accuracy: 0.6000\n",
      "Epoch 82/205\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 0.5978 - accuracy: 0.8083 - val_loss: 0.6593 - val_accuracy: 0.6500\n",
      "Epoch 83/205\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.5968 - accuracy: 0.8083 - val_loss: 0.6587 - val_accuracy: 0.6500\n",
      "Epoch 84/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.5956 - accuracy: 0.8167 - val_loss: 0.6588 - val_accuracy: 0.6500\n",
      "Epoch 85/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.5944 - accuracy: 0.8167 - val_loss: 0.6589 - val_accuracy: 0.6000\n",
      "Epoch 86/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.5933 - accuracy: 0.8167 - val_loss: 0.6587 - val_accuracy: 0.6000\n",
      "Epoch 87/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.5922 - accuracy: 0.8167 - val_loss: 0.6582 - val_accuracy: 0.5500\n",
      "Epoch 88/205\n",
      "2/2 [==============================] - 0s 126ms/step - loss: 0.5911 - accuracy: 0.8167 - val_loss: 0.6575 - val_accuracy: 0.6000\n",
      "Epoch 89/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.5899 - accuracy: 0.8167 - val_loss: 0.6573 - val_accuracy: 0.5500\n",
      "Epoch 90/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.5888 - accuracy: 0.8250 - val_loss: 0.6571 - val_accuracy: 0.5500\n",
      "Epoch 91/205\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 0.5877 - accuracy: 0.8333 - val_loss: 0.6566 - val_accuracy: 0.6000\n",
      "Epoch 92/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.5867 - accuracy: 0.8333 - val_loss: 0.6562 - val_accuracy: 0.6000\n",
      "Epoch 93/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.5856 - accuracy: 0.8333 - val_loss: 0.6558 - val_accuracy: 0.6000\n",
      "Epoch 94/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.5845 - accuracy: 0.8250 - val_loss: 0.6552 - val_accuracy: 0.6000\n",
      "Epoch 95/205\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 0.5834 - accuracy: 0.8250 - val_loss: 0.6548 - val_accuracy: 0.6000\n",
      "Epoch 96/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.5824 - accuracy: 0.8250 - val_loss: 0.6543 - val_accuracy: 0.6000\n",
      "Epoch 97/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.5813 - accuracy: 0.8250 - val_loss: 0.6540 - val_accuracy: 0.6000\n",
      "Epoch 98/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.5802 - accuracy: 0.8250 - val_loss: 0.6533 - val_accuracy: 0.6000\n",
      "Epoch 99/205\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 0.5792 - accuracy: 0.8333 - val_loss: 0.6528 - val_accuracy: 0.6000\n",
      "Epoch 100/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.5781 - accuracy: 0.8333 - val_loss: 0.6526 - val_accuracy: 0.6000\n",
      "Epoch 101/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.5771 - accuracy: 0.8333 - val_loss: 0.6519 - val_accuracy: 0.6500\n",
      "Epoch 102/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.5761 - accuracy: 0.8333 - val_loss: 0.6513 - val_accuracy: 0.6500\n",
      "Epoch 103/205\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 0.5750 - accuracy: 0.8333 - val_loss: 0.6513 - val_accuracy: 0.6500\n",
      "Epoch 104/205\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 0.5740 - accuracy: 0.8333 - val_loss: 0.6508 - val_accuracy: 0.6500\n",
      "Epoch 105/205\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 0.5730 - accuracy: 0.8333 - val_loss: 0.6498 - val_accuracy: 0.6500\n",
      "Epoch 106/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.5720 - accuracy: 0.8333 - val_loss: 0.6492 - val_accuracy: 0.6500\n",
      "Epoch 107/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.5709 - accuracy: 0.8333 - val_loss: 0.6492 - val_accuracy: 0.6500\n",
      "Epoch 108/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.5699 - accuracy: 0.8250 - val_loss: 0.6489 - val_accuracy: 0.6500\n",
      "Epoch 109/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.5689 - accuracy: 0.8167 - val_loss: 0.6480 - val_accuracy: 0.6500\n",
      "Epoch 110/205\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.5679 - accuracy: 0.8167 - val_loss: 0.6475 - val_accuracy: 0.6500\n",
      "Epoch 111/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.5669 - accuracy: 0.8167 - val_loss: 0.6472 - val_accuracy: 0.6500\n",
      "Epoch 112/205\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.5659 - accuracy: 0.8167 - val_loss: 0.6468 - val_accuracy: 0.6500\n",
      "Epoch 113/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.5649 - accuracy: 0.8167 - val_loss: 0.6461 - val_accuracy: 0.6500\n",
      "Epoch 114/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.5639 - accuracy: 0.8167 - val_loss: 0.6458 - val_accuracy: 0.6500\n",
      "Epoch 115/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.5628 - accuracy: 0.8167 - val_loss: 0.6454 - val_accuracy: 0.6500\n",
      "Epoch 116/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.5619 - accuracy: 0.8167 - val_loss: 0.6448 - val_accuracy: 0.6500\n",
      "Epoch 117/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.5608 - accuracy: 0.8167 - val_loss: 0.6445 - val_accuracy: 0.6500\n",
      "Epoch 118/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.5599 - accuracy: 0.8250 - val_loss: 0.6439 - val_accuracy: 0.6500\n",
      "Epoch 119/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.5589 - accuracy: 0.8250 - val_loss: 0.6435 - val_accuracy: 0.6500\n",
      "Epoch 120/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.5579 - accuracy: 0.8250 - val_loss: 0.6432 - val_accuracy: 0.6000\n",
      "Epoch 121/205\n",
      "2/2 [==============================] - 0s 123ms/step - loss: 0.5569 - accuracy: 0.8250 - val_loss: 0.6427 - val_accuracy: 0.6000\n",
      "Epoch 122/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.5560 - accuracy: 0.8250 - val_loss: 0.6425 - val_accuracy: 0.6000\n",
      "Epoch 123/205\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 0.5550 - accuracy: 0.8250 - val_loss: 0.6422 - val_accuracy: 0.6000\n",
      "Epoch 124/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.5540 - accuracy: 0.8250 - val_loss: 0.6416 - val_accuracy: 0.6000\n",
      "Epoch 125/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.5531 - accuracy: 0.8250 - val_loss: 0.6413 - val_accuracy: 0.6000\n",
      "Epoch 126/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.5521 - accuracy: 0.8250 - val_loss: 0.6410 - val_accuracy: 0.6000\n",
      "Epoch 127/205\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.5512 - accuracy: 0.8250 - val_loss: 0.6402 - val_accuracy: 0.6000\n",
      "Epoch 128/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.5502 - accuracy: 0.8250 - val_loss: 0.6395 - val_accuracy: 0.6000\n",
      "Epoch 129/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.5492 - accuracy: 0.8250 - val_loss: 0.6391 - val_accuracy: 0.6000\n",
      "Epoch 130/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.5483 - accuracy: 0.8250 - val_loss: 0.6386 - val_accuracy: 0.6000\n",
      "Epoch 131/205\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 0.5473 - accuracy: 0.8250 - val_loss: 0.6379 - val_accuracy: 0.6000\n",
      "Epoch 132/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.5464 - accuracy: 0.8333 - val_loss: 0.6373 - val_accuracy: 0.6000\n",
      "Epoch 133/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.5454 - accuracy: 0.8333 - val_loss: 0.6366 - val_accuracy: 0.6000\n",
      "Epoch 134/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.5445 - accuracy: 0.8333 - val_loss: 0.6360 - val_accuracy: 0.6000\n",
      "Epoch 135/205\n",
      "2/2 [==============================] - 0s 126ms/step - loss: 0.5435 - accuracy: 0.8333 - val_loss: 0.6358 - val_accuracy: 0.6000\n",
      "Epoch 136/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.5425 - accuracy: 0.8333 - val_loss: 0.6354 - val_accuracy: 0.6000\n",
      "Epoch 137/205\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 0.5416 - accuracy: 0.8333 - val_loss: 0.6348 - val_accuracy: 0.6000\n",
      "Epoch 138/205\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 0.5407 - accuracy: 0.8417 - val_loss: 0.6343 - val_accuracy: 0.6000\n",
      "Epoch 139/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.5397 - accuracy: 0.8417 - val_loss: 0.6340 - val_accuracy: 0.6000\n",
      "Epoch 140/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.5388 - accuracy: 0.8417 - val_loss: 0.6334 - val_accuracy: 0.6000\n",
      "Epoch 141/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.5379 - accuracy: 0.8500 - val_loss: 0.6327 - val_accuracy: 0.6000\n",
      "Epoch 142/205\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 0.5369 - accuracy: 0.8500 - val_loss: 0.6323 - val_accuracy: 0.6000\n",
      "Epoch 143/205\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.5360 - accuracy: 0.8500 - val_loss: 0.6319 - val_accuracy: 0.6000\n",
      "Epoch 144/205\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 0.5350 - accuracy: 0.8500 - val_loss: 0.6314 - val_accuracy: 0.6000\n",
      "Epoch 145/205\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 0.5341 - accuracy: 0.8500 - val_loss: 0.6311 - val_accuracy: 0.6000\n",
      "Epoch 146/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.5332 - accuracy: 0.8500 - val_loss: 0.6303 - val_accuracy: 0.6000\n",
      "Epoch 147/205\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 0.5323 - accuracy: 0.8500 - val_loss: 0.6297 - val_accuracy: 0.6000\n",
      "Epoch 148/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.5314 - accuracy: 0.8500 - val_loss: 0.6293 - val_accuracy: 0.6000\n",
      "Epoch 149/205\n",
      "2/2 [==============================] - 0s 114ms/step - loss: 0.5305 - accuracy: 0.8500 - val_loss: 0.6290 - val_accuracy: 0.6000\n",
      "Epoch 150/205\n",
      "2/2 [==============================] - 0s 106ms/step - loss: 0.5296 - accuracy: 0.8500 - val_loss: 0.6283 - val_accuracy: 0.6000\n",
      "Epoch 151/205\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 0.5287 - accuracy: 0.8500 - val_loss: 0.6273 - val_accuracy: 0.6000\n",
      "Epoch 152/205\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 0.5278 - accuracy: 0.8500 - val_loss: 0.6265 - val_accuracy: 0.6000\n",
      "Epoch 153/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.5268 - accuracy: 0.8500 - val_loss: 0.6259 - val_accuracy: 0.6500\n",
      "Epoch 154/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.5259 - accuracy: 0.8500 - val_loss: 0.6253 - val_accuracy: 0.6500\n",
      "Epoch 155/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.5250 - accuracy: 0.8500 - val_loss: 0.6248 - val_accuracy: 0.6500\n",
      "Epoch 156/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.5241 - accuracy: 0.8500 - val_loss: 0.6242 - val_accuracy: 0.6500\n",
      "Epoch 157/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.5232 - accuracy: 0.8500 - val_loss: 0.6237 - val_accuracy: 0.6500\n",
      "Epoch 158/205\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 0.5222 - accuracy: 0.8500 - val_loss: 0.6232 - val_accuracy: 0.6500\n",
      "Epoch 159/205\n",
      "2/2 [==============================] - 0s 127ms/step - loss: 0.5213 - accuracy: 0.8500 - val_loss: 0.6224 - val_accuracy: 0.6500\n",
      "Epoch 160/205\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.5204 - accuracy: 0.8500 - val_loss: 0.6215 - val_accuracy: 0.6500\n",
      "Epoch 161/205\n",
      "2/2 [==============================] - 0s 125ms/step - loss: 0.5195 - accuracy: 0.8500 - val_loss: 0.6204 - val_accuracy: 0.6500\n",
      "Epoch 162/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.5186 - accuracy: 0.8500 - val_loss: 0.6194 - val_accuracy: 0.6500\n",
      "Epoch 163/205\n",
      "2/2 [==============================] - 0s 128ms/step - loss: 0.5177 - accuracy: 0.8500 - val_loss: 0.6185 - val_accuracy: 0.6500\n",
      "Epoch 164/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.5168 - accuracy: 0.8500 - val_loss: 0.6178 - val_accuracy: 0.6500\n",
      "Epoch 165/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.5158 - accuracy: 0.8500 - val_loss: 0.6174 - val_accuracy: 0.6500\n",
      "Epoch 166/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.5149 - accuracy: 0.8500 - val_loss: 0.6174 - val_accuracy: 0.6500\n",
      "Epoch 167/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.5140 - accuracy: 0.8500 - val_loss: 0.6172 - val_accuracy: 0.6500\n",
      "Epoch 168/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.5131 - accuracy: 0.8500 - val_loss: 0.6170 - val_accuracy: 0.6500\n",
      "Epoch 169/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.5122 - accuracy: 0.8583 - val_loss: 0.6165 - val_accuracy: 0.6500\n",
      "Epoch 170/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.5113 - accuracy: 0.8583 - val_loss: 0.6159 - val_accuracy: 0.6500\n",
      "Epoch 171/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.5103 - accuracy: 0.8583 - val_loss: 0.6154 - val_accuracy: 0.6500\n",
      "Epoch 172/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.5094 - accuracy: 0.8667 - val_loss: 0.6150 - val_accuracy: 0.6500\n",
      "Epoch 173/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.5085 - accuracy: 0.8667 - val_loss: 0.6147 - val_accuracy: 0.6500\n",
      "Epoch 174/205\n",
      "2/2 [==============================] - 0s 127ms/step - loss: 0.5076 - accuracy: 0.8667 - val_loss: 0.6142 - val_accuracy: 0.6500\n",
      "Epoch 175/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.5067 - accuracy: 0.8667 - val_loss: 0.6136 - val_accuracy: 0.6500\n",
      "Epoch 176/205\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 0.5058 - accuracy: 0.8667 - val_loss: 0.6130 - val_accuracy: 0.6500\n",
      "Epoch 177/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.5049 - accuracy: 0.8667 - val_loss: 0.6124 - val_accuracy: 0.6500\n",
      "Epoch 178/205\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.5040 - accuracy: 0.8667 - val_loss: 0.6121 - val_accuracy: 0.6500\n",
      "Epoch 179/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.5031 - accuracy: 0.8667 - val_loss: 0.6114 - val_accuracy: 0.6500\n",
      "Epoch 180/205\n",
      "2/2 [==============================] - 0s 123ms/step - loss: 0.5022 - accuracy: 0.8667 - val_loss: 0.6111 - val_accuracy: 0.6500\n",
      "Epoch 181/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.5014 - accuracy: 0.8667 - val_loss: 0.6105 - val_accuracy: 0.6500\n",
      "Epoch 182/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.5005 - accuracy: 0.8667 - val_loss: 0.6099 - val_accuracy: 0.6500\n",
      "Epoch 183/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.4996 - accuracy: 0.8667 - val_loss: 0.6097 - val_accuracy: 0.6500\n",
      "Epoch 184/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.4988 - accuracy: 0.8667 - val_loss: 0.6091 - val_accuracy: 0.6500\n",
      "Epoch 185/205\n",
      "2/2 [==============================] - 0s 124ms/step - loss: 0.4979 - accuracy: 0.8667 - val_loss: 0.6084 - val_accuracy: 0.6500\n",
      "Epoch 186/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.4971 - accuracy: 0.8667 - val_loss: 0.6081 - val_accuracy: 0.6500\n",
      "Epoch 187/205\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 0.4962 - accuracy: 0.8667 - val_loss: 0.6079 - val_accuracy: 0.6500\n",
      "Epoch 188/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.4953 - accuracy: 0.8667 - val_loss: 0.6073 - val_accuracy: 0.6500\n",
      "Epoch 189/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.4945 - accuracy: 0.8750 - val_loss: 0.6067 - val_accuracy: 0.6500\n",
      "Epoch 190/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.4936 - accuracy: 0.8750 - val_loss: 0.6067 - val_accuracy: 0.6500\n",
      "Epoch 191/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.4928 - accuracy: 0.8750 - val_loss: 0.6062 - val_accuracy: 0.6500\n",
      "Epoch 192/205\n",
      "2/2 [==============================] - 0s 126ms/step - loss: 0.4919 - accuracy: 0.8750 - val_loss: 0.6054 - val_accuracy: 0.6500\n",
      "Epoch 193/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.4911 - accuracy: 0.8750 - val_loss: 0.6051 - val_accuracy: 0.6500\n",
      "Epoch 194/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.4902 - accuracy: 0.8750 - val_loss: 0.6049 - val_accuracy: 0.6500\n",
      "Epoch 195/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.4894 - accuracy: 0.8750 - val_loss: 0.6042 - val_accuracy: 0.6000\n",
      "Epoch 196/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.4885 - accuracy: 0.8750 - val_loss: 0.6035 - val_accuracy: 0.6000\n",
      "Epoch 197/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.4877 - accuracy: 0.8750 - val_loss: 0.6036 - val_accuracy: 0.6000\n",
      "Epoch 198/205\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 0.4868 - accuracy: 0.8750 - val_loss: 0.6033 - val_accuracy: 0.6000\n",
      "Epoch 199/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.4860 - accuracy: 0.8750 - val_loss: 0.6023 - val_accuracy: 0.6000\n",
      "Epoch 200/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.4852 - accuracy: 0.8750 - val_loss: 0.6019 - val_accuracy: 0.6000\n",
      "Epoch 201/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.4843 - accuracy: 0.8750 - val_loss: 0.6017 - val_accuracy: 0.6000\n",
      "Epoch 202/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.4835 - accuracy: 0.8833 - val_loss: 0.6010 - val_accuracy: 0.6000\n",
      "Epoch 203/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.4827 - accuracy: 0.8833 - val_loss: 0.6006 - val_accuracy: 0.6000\n",
      "Epoch 204/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.4818 - accuracy: 0.8833 - val_loss: 0.6003 - val_accuracy: 0.6000\n",
      "Epoch 205/205\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.4810 - accuracy: 0.8833 - val_loss: 0.5998 - val_accuracy: 0.6000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.5772 - accuracy: 0.8000\n",
      "\n",
      "Fold  0\n",
      "\n",
      "Test accuracy:  0.800000011920929\n",
      "Test loss:  0.5771666765213013\n",
      "\n",
      "Fold  1\n",
      "\n",
      "Training with 120 samples and validating with 20 samples\n",
      "\n",
      "Epoch 1/205\n",
      "2/2 [==============================] - 0s 115ms/step - loss: 0.5112 - accuracy: 0.8333 - val_loss: 0.4615 - val_accuracy: 0.9000\n",
      "Epoch 2/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.5103 - accuracy: 0.8250 - val_loss: 0.4617 - val_accuracy: 0.9000\n",
      "Epoch 3/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.5093 - accuracy: 0.8333 - val_loss: 0.4622 - val_accuracy: 0.9000\n",
      "Epoch 4/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.5082 - accuracy: 0.8333 - val_loss: 0.4628 - val_accuracy: 0.9000\n",
      "Epoch 5/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.5071 - accuracy: 0.8333 - val_loss: 0.4636 - val_accuracy: 0.9000\n",
      "Epoch 6/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.5059 - accuracy: 0.8417 - val_loss: 0.4646 - val_accuracy: 0.9000\n",
      "Epoch 7/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.5047 - accuracy: 0.8417 - val_loss: 0.4656 - val_accuracy: 0.9000\n",
      "Epoch 8/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.5035 - accuracy: 0.8333 - val_loss: 0.4666 - val_accuracy: 0.9000\n",
      "Epoch 9/205\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.5022 - accuracy: 0.8333 - val_loss: 0.4676 - val_accuracy: 0.8500\n",
      "Epoch 10/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.5010 - accuracy: 0.8333 - val_loss: 0.4686 - val_accuracy: 0.8000\n",
      "Epoch 11/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.4997 - accuracy: 0.8333 - val_loss: 0.4695 - val_accuracy: 0.8000\n",
      "Epoch 12/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.4985 - accuracy: 0.8333 - val_loss: 0.4704 - val_accuracy: 0.8000\n",
      "Epoch 13/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.4973 - accuracy: 0.8333 - val_loss: 0.4712 - val_accuracy: 0.8000\n",
      "Epoch 14/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.4960 - accuracy: 0.8333 - val_loss: 0.4720 - val_accuracy: 0.8000\n",
      "Epoch 15/205\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 0.4948 - accuracy: 0.8250 - val_loss: 0.4728 - val_accuracy: 0.8000\n",
      "Epoch 16/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.4936 - accuracy: 0.8250 - val_loss: 0.4736 - val_accuracy: 0.8000\n",
      "Epoch 17/205\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.4924 - accuracy: 0.8333 - val_loss: 0.4744 - val_accuracy: 0.8000\n",
      "Epoch 18/205\n",
      "2/2 [==============================] - 0s 108ms/step - loss: 0.4912 - accuracy: 0.8333 - val_loss: 0.4751 - val_accuracy: 0.8000\n",
      "Epoch 19/205\n",
      "2/2 [==============================] - 0s 106ms/step - loss: 0.4900 - accuracy: 0.8333 - val_loss: 0.4758 - val_accuracy: 0.8000\n",
      "Epoch 20/205\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 0.4889 - accuracy: 0.8333 - val_loss: 0.4765 - val_accuracy: 0.8000\n",
      "Epoch 21/205\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 0.4877 - accuracy: 0.8333 - val_loss: 0.4770 - val_accuracy: 0.8000\n",
      "Epoch 22/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.4866 - accuracy: 0.8417 - val_loss: 0.4775 - val_accuracy: 0.8000\n",
      "Epoch 23/205\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.4855 - accuracy: 0.8417 - val_loss: 0.4779 - val_accuracy: 0.8000\n",
      "Epoch 24/205\n",
      "2/2 [==============================] - 0s 124ms/step - loss: 0.4843 - accuracy: 0.8417 - val_loss: 0.4781 - val_accuracy: 0.8000\n",
      "Epoch 25/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.4832 - accuracy: 0.8417 - val_loss: 0.4783 - val_accuracy: 0.8000\n",
      "Epoch 26/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.4821 - accuracy: 0.8417 - val_loss: 0.4783 - val_accuracy: 0.8000\n",
      "Epoch 27/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.4810 - accuracy: 0.8417 - val_loss: 0.4783 - val_accuracy: 0.8500\n",
      "Epoch 28/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.4799 - accuracy: 0.8417 - val_loss: 0.4785 - val_accuracy: 0.8500\n",
      "Epoch 29/205\n",
      "2/2 [==============================] - 0s 126ms/step - loss: 0.4789 - accuracy: 0.8417 - val_loss: 0.4788 - val_accuracy: 0.8500\n",
      "Epoch 30/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.4778 - accuracy: 0.8417 - val_loss: 0.4790 - val_accuracy: 0.8500\n",
      "Epoch 31/205\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 0.4767 - accuracy: 0.8417 - val_loss: 0.4793 - val_accuracy: 0.8500\n",
      "Epoch 32/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.4757 - accuracy: 0.8417 - val_loss: 0.4794 - val_accuracy: 0.8500\n",
      "Epoch 33/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.4747 - accuracy: 0.8417 - val_loss: 0.4795 - val_accuracy: 0.8500\n",
      "Epoch 34/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.4736 - accuracy: 0.8417 - val_loss: 0.4796 - val_accuracy: 0.8500\n",
      "Epoch 35/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.4726 - accuracy: 0.8417 - val_loss: 0.4799 - val_accuracy: 0.8500\n",
      "Epoch 36/205\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.4716 - accuracy: 0.8417 - val_loss: 0.4802 - val_accuracy: 0.8500\n",
      "Epoch 37/205\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 0.4706 - accuracy: 0.8417 - val_loss: 0.4804 - val_accuracy: 0.8500\n",
      "Epoch 38/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.4696 - accuracy: 0.8417 - val_loss: 0.4807 - val_accuracy: 0.8500\n",
      "Epoch 39/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.4687 - accuracy: 0.8333 - val_loss: 0.4809 - val_accuracy: 0.8500\n",
      "Epoch 40/205\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 0.4677 - accuracy: 0.8333 - val_loss: 0.4811 - val_accuracy: 0.8500\n",
      "Epoch 41/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.4667 - accuracy: 0.8333 - val_loss: 0.4814 - val_accuracy: 0.8500\n",
      "Epoch 42/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.4658 - accuracy: 0.8333 - val_loss: 0.4817 - val_accuracy: 0.8500\n",
      "Epoch 43/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.4648 - accuracy: 0.8333 - val_loss: 0.4820 - val_accuracy: 0.8500\n",
      "Epoch 44/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.4639 - accuracy: 0.8333 - val_loss: 0.4822 - val_accuracy: 0.8500\n",
      "Epoch 45/205\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 0.4629 - accuracy: 0.8417 - val_loss: 0.4825 - val_accuracy: 0.8500\n",
      "Epoch 46/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.4620 - accuracy: 0.8417 - val_loss: 0.4826 - val_accuracy: 0.8500\n",
      "Epoch 47/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.4611 - accuracy: 0.8500 - val_loss: 0.4828 - val_accuracy: 0.8500\n",
      "Epoch 48/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.4602 - accuracy: 0.8500 - val_loss: 0.4830 - val_accuracy: 0.8500\n",
      "Epoch 49/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.4592 - accuracy: 0.8583 - val_loss: 0.4832 - val_accuracy: 0.8500\n",
      "Epoch 50/205\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.4583 - accuracy: 0.8583 - val_loss: 0.4835 - val_accuracy: 0.8500\n",
      "Epoch 51/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.4574 - accuracy: 0.8583 - val_loss: 0.4836 - val_accuracy: 0.8500\n",
      "Epoch 52/205\n",
      "2/2 [==============================] - 0s 127ms/step - loss: 0.4565 - accuracy: 0.8583 - val_loss: 0.4836 - val_accuracy: 0.8500\n",
      "Epoch 53/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.4556 - accuracy: 0.8667 - val_loss: 0.4838 - val_accuracy: 0.8500\n",
      "Epoch 54/205\n",
      "2/2 [==============================] - 0s 128ms/step - loss: 0.4548 - accuracy: 0.8667 - val_loss: 0.4837 - val_accuracy: 0.8500\n",
      "Epoch 55/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.4539 - accuracy: 0.8750 - val_loss: 0.4839 - val_accuracy: 0.8000\n",
      "Epoch 56/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.4530 - accuracy: 0.8750 - val_loss: 0.4841 - val_accuracy: 0.8000\n",
      "Epoch 57/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.4521 - accuracy: 0.8750 - val_loss: 0.4843 - val_accuracy: 0.8000\n",
      "Epoch 58/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.4513 - accuracy: 0.8750 - val_loss: 0.4842 - val_accuracy: 0.8000\n",
      "Epoch 59/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.4504 - accuracy: 0.8750 - val_loss: 0.4842 - val_accuracy: 0.8000\n",
      "Epoch 60/205\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 0.4495 - accuracy: 0.8667 - val_loss: 0.4845 - val_accuracy: 0.8000\n",
      "Epoch 61/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.4487 - accuracy: 0.8667 - val_loss: 0.4848 - val_accuracy: 0.8000\n",
      "Epoch 62/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.4479 - accuracy: 0.8667 - val_loss: 0.4847 - val_accuracy: 0.8000\n",
      "Epoch 63/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.4470 - accuracy: 0.8667 - val_loss: 0.4846 - val_accuracy: 0.8000\n",
      "Epoch 64/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.4462 - accuracy: 0.8667 - val_loss: 0.4847 - val_accuracy: 0.8000\n",
      "Epoch 65/205\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.4453 - accuracy: 0.8667 - val_loss: 0.4849 - val_accuracy: 0.8000\n",
      "Epoch 66/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.4445 - accuracy: 0.8667 - val_loss: 0.4852 - val_accuracy: 0.8000\n",
      "Epoch 67/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.4437 - accuracy: 0.8667 - val_loss: 0.4851 - val_accuracy: 0.8000\n",
      "Epoch 68/205\n",
      "2/2 [==============================] - 0s 124ms/step - loss: 0.4429 - accuracy: 0.8667 - val_loss: 0.4852 - val_accuracy: 0.8000\n",
      "Epoch 69/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.4421 - accuracy: 0.8667 - val_loss: 0.4851 - val_accuracy: 0.8000\n",
      "Epoch 70/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.4413 - accuracy: 0.8750 - val_loss: 0.4853 - val_accuracy: 0.8000\n",
      "Epoch 71/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.4404 - accuracy: 0.8750 - val_loss: 0.4855 - val_accuracy: 0.8000\n",
      "Epoch 72/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.4397 - accuracy: 0.8750 - val_loss: 0.4855 - val_accuracy: 0.8000\n",
      "Epoch 73/205\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.4389 - accuracy: 0.8750 - val_loss: 0.4854 - val_accuracy: 0.8000\n",
      "Epoch 74/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.4381 - accuracy: 0.8833 - val_loss: 0.4856 - val_accuracy: 0.8000\n",
      "Epoch 75/205\n",
      "2/2 [==============================] - 0s 126ms/step - loss: 0.4372 - accuracy: 0.8833 - val_loss: 0.4858 - val_accuracy: 0.8000\n",
      "Epoch 76/205\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 0.4365 - accuracy: 0.8833 - val_loss: 0.4858 - val_accuracy: 0.8000\n",
      "Epoch 77/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.4357 - accuracy: 0.8833 - val_loss: 0.4857 - val_accuracy: 0.8000\n",
      "Epoch 78/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.4349 - accuracy: 0.8833 - val_loss: 0.4858 - val_accuracy: 0.8000\n",
      "Epoch 79/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.4341 - accuracy: 0.8833 - val_loss: 0.4859 - val_accuracy: 0.8000\n",
      "Epoch 80/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.4334 - accuracy: 0.8917 - val_loss: 0.4859 - val_accuracy: 0.8000\n",
      "Epoch 81/205\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.4326 - accuracy: 0.8917 - val_loss: 0.4860 - val_accuracy: 0.8000\n",
      "Epoch 82/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.4318 - accuracy: 0.8917 - val_loss: 0.4861 - val_accuracy: 0.8000\n",
      "Epoch 83/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.4311 - accuracy: 0.9000 - val_loss: 0.4860 - val_accuracy: 0.8000\n",
      "Epoch 84/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.4303 - accuracy: 0.9000 - val_loss: 0.4859 - val_accuracy: 0.8000\n",
      "Epoch 85/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.4295 - accuracy: 0.9000 - val_loss: 0.4861 - val_accuracy: 0.8000\n",
      "Epoch 86/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.4288 - accuracy: 0.9000 - val_loss: 0.4861 - val_accuracy: 0.8000\n",
      "Epoch 87/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.4281 - accuracy: 0.9000 - val_loss: 0.4860 - val_accuracy: 0.8000\n",
      "Epoch 88/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.4273 - accuracy: 0.9000 - val_loss: 0.4862 - val_accuracy: 0.8000\n",
      "Epoch 89/205\n",
      "2/2 [==============================] - 0s 110ms/step - loss: 0.4265 - accuracy: 0.9000 - val_loss: 0.4863 - val_accuracy: 0.8000\n",
      "Epoch 90/205\n",
      "2/2 [==============================] - 0s 116ms/step - loss: 0.4258 - accuracy: 0.9000 - val_loss: 0.4862 - val_accuracy: 0.8000\n",
      "Epoch 91/205\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.4251 - accuracy: 0.9000 - val_loss: 0.4862 - val_accuracy: 0.8000\n",
      "Epoch 92/205\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.4243 - accuracy: 0.9000 - val_loss: 0.4862 - val_accuracy: 0.8000\n",
      "Epoch 93/205\n",
      "2/2 [==============================] - 0s 154ms/step - loss: 0.4236 - accuracy: 0.9000 - val_loss: 0.4861 - val_accuracy: 0.8000\n",
      "Epoch 94/205\n",
      "2/2 [==============================] - 0s 198ms/step - loss: 0.4229 - accuracy: 0.9000 - val_loss: 0.4861 - val_accuracy: 0.8000\n",
      "Epoch 95/205\n",
      "2/2 [==============================] - 0s 156ms/step - loss: 0.4222 - accuracy: 0.9000 - val_loss: 0.4860 - val_accuracy: 0.8000\n",
      "Epoch 96/205\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 0.4214 - accuracy: 0.9000 - val_loss: 0.4860 - val_accuracy: 0.8000\n",
      "Epoch 97/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.4207 - accuracy: 0.9083 - val_loss: 0.4862 - val_accuracy: 0.8000\n",
      "Epoch 98/205\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.4200 - accuracy: 0.9083 - val_loss: 0.4860 - val_accuracy: 0.8000\n",
      "Epoch 99/205\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 0.4193 - accuracy: 0.9083 - val_loss: 0.4860 - val_accuracy: 0.8000\n",
      "Epoch 100/205\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 0.4186 - accuracy: 0.9083 - val_loss: 0.4861 - val_accuracy: 0.8000\n",
      "Epoch 101/205\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.4178 - accuracy: 0.9083 - val_loss: 0.4863 - val_accuracy: 0.8000\n",
      "Epoch 102/205\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 0.4172 - accuracy: 0.9083 - val_loss: 0.4862 - val_accuracy: 0.8000\n",
      "Epoch 103/205\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 0.4165 - accuracy: 0.9083 - val_loss: 0.4861 - val_accuracy: 0.8000\n",
      "Epoch 104/205\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 0.4158 - accuracy: 0.9083 - val_loss: 0.4862 - val_accuracy: 0.8000\n",
      "Epoch 105/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.4150 - accuracy: 0.9083 - val_loss: 0.4863 - val_accuracy: 0.8000\n",
      "Epoch 106/205\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 0.4144 - accuracy: 0.9083 - val_loss: 0.4862 - val_accuracy: 0.8000\n",
      "Epoch 107/205\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 0.4137 - accuracy: 0.9083 - val_loss: 0.4861 - val_accuracy: 0.8000\n",
      "Epoch 108/205\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.4130 - accuracy: 0.9083 - val_loss: 0.4863 - val_accuracy: 0.8000\n",
      "Epoch 109/205\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 0.4123 - accuracy: 0.9083 - val_loss: 0.4864 - val_accuracy: 0.8000\n",
      "Epoch 110/205\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 0.4116 - accuracy: 0.9083 - val_loss: 0.4862 - val_accuracy: 0.8000\n",
      "Epoch 111/205\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.4109 - accuracy: 0.9083 - val_loss: 0.4862 - val_accuracy: 0.8000\n",
      "Epoch 112/205\n",
      "2/2 [==============================] - 0s 124ms/step - loss: 0.4102 - accuracy: 0.9083 - val_loss: 0.4862 - val_accuracy: 0.8000\n",
      "Epoch 113/205\n",
      "2/2 [==============================] - 0s 128ms/step - loss: 0.4096 - accuracy: 0.9083 - val_loss: 0.4862 - val_accuracy: 0.8000\n",
      "Epoch 114/205\n",
      "2/2 [==============================] - 0s 125ms/step - loss: 0.4089 - accuracy: 0.9083 - val_loss: 0.4860 - val_accuracy: 0.8000\n",
      "Epoch 115/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.4082 - accuracy: 0.9083 - val_loss: 0.4860 - val_accuracy: 0.8000\n",
      "Epoch 116/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.4075 - accuracy: 0.9083 - val_loss: 0.4862 - val_accuracy: 0.8000\n",
      "Epoch 117/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.4069 - accuracy: 0.9083 - val_loss: 0.4860 - val_accuracy: 0.8000\n",
      "Epoch 118/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.4062 - accuracy: 0.9083 - val_loss: 0.4859 - val_accuracy: 0.8000\n",
      "Epoch 119/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.4055 - accuracy: 0.9083 - val_loss: 0.4860 - val_accuracy: 0.8000\n",
      "Epoch 120/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.4048 - accuracy: 0.9167 - val_loss: 0.4861 - val_accuracy: 0.8000\n",
      "Epoch 121/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.4042 - accuracy: 0.9167 - val_loss: 0.4860 - val_accuracy: 0.8000\n",
      "Epoch 122/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.4035 - accuracy: 0.9167 - val_loss: 0.4859 - val_accuracy: 0.8000\n",
      "Epoch 123/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.4029 - accuracy: 0.9167 - val_loss: 0.4858 - val_accuracy: 0.8000\n",
      "Epoch 124/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.4022 - accuracy: 0.9167 - val_loss: 0.4859 - val_accuracy: 0.8000\n",
      "Epoch 125/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.4016 - accuracy: 0.9167 - val_loss: 0.4860 - val_accuracy: 0.8000\n",
      "Epoch 126/205\n",
      "2/2 [==============================] - 0s 128ms/step - loss: 0.4009 - accuracy: 0.9167 - val_loss: 0.4859 - val_accuracy: 0.8500\n",
      "Epoch 127/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.4003 - accuracy: 0.9167 - val_loss: 0.4857 - val_accuracy: 0.8500\n",
      "Epoch 128/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.3996 - accuracy: 0.9167 - val_loss: 0.4858 - val_accuracy: 0.8500\n",
      "Epoch 129/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.3990 - accuracy: 0.9167 - val_loss: 0.4858 - val_accuracy: 0.8500\n",
      "Epoch 130/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.3984 - accuracy: 0.9167 - val_loss: 0.4857 - val_accuracy: 0.8500\n",
      "Epoch 131/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.3977 - accuracy: 0.9167 - val_loss: 0.4856 - val_accuracy: 0.8500\n",
      "Epoch 132/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.3971 - accuracy: 0.9167 - val_loss: 0.4857 - val_accuracy: 0.8500\n",
      "Epoch 133/205\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 0.3964 - accuracy: 0.9167 - val_loss: 0.4856 - val_accuracy: 0.8500\n",
      "Epoch 134/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.3958 - accuracy: 0.9167 - val_loss: 0.4855 - val_accuracy: 0.8500\n",
      "Epoch 135/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.3952 - accuracy: 0.9167 - val_loss: 0.4855 - val_accuracy: 0.8500\n",
      "Epoch 136/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.3946 - accuracy: 0.9167 - val_loss: 0.4854 - val_accuracy: 0.8500\n",
      "Epoch 137/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.3939 - accuracy: 0.9167 - val_loss: 0.4854 - val_accuracy: 0.8500\n",
      "Epoch 138/205\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.3933 - accuracy: 0.9167 - val_loss: 0.4853 - val_accuracy: 0.8500\n",
      "Epoch 139/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.3927 - accuracy: 0.9167 - val_loss: 0.4852 - val_accuracy: 0.8500\n",
      "Epoch 140/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.3921 - accuracy: 0.9167 - val_loss: 0.4851 - val_accuracy: 0.8500\n",
      "Epoch 141/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.3914 - accuracy: 0.9167 - val_loss: 0.4852 - val_accuracy: 0.8500\n",
      "Epoch 142/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.3908 - accuracy: 0.9167 - val_loss: 0.4850 - val_accuracy: 0.8500\n",
      "Epoch 143/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.3902 - accuracy: 0.9167 - val_loss: 0.4850 - val_accuracy: 0.8500\n",
      "Epoch 144/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.3896 - accuracy: 0.9167 - val_loss: 0.4849 - val_accuracy: 0.8500\n",
      "Epoch 145/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.3890 - accuracy: 0.9167 - val_loss: 0.4848 - val_accuracy: 0.8500\n",
      "Epoch 146/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.3884 - accuracy: 0.9167 - val_loss: 0.4848 - val_accuracy: 0.8500\n",
      "Epoch 147/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.3878 - accuracy: 0.9167 - val_loss: 0.4848 - val_accuracy: 0.8500\n",
      "Epoch 148/205\n",
      "2/2 [==============================] - 0s 126ms/step - loss: 0.3872 - accuracy: 0.9167 - val_loss: 0.4847 - val_accuracy: 0.8500\n",
      "Epoch 149/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.3865 - accuracy: 0.9167 - val_loss: 0.4847 - val_accuracy: 0.8500\n",
      "Epoch 150/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.3859 - accuracy: 0.9167 - val_loss: 0.4846 - val_accuracy: 0.8500\n",
      "Epoch 151/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.3854 - accuracy: 0.9167 - val_loss: 0.4846 - val_accuracy: 0.8500\n",
      "Epoch 152/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.3848 - accuracy: 0.9167 - val_loss: 0.4845 - val_accuracy: 0.8500\n",
      "Epoch 153/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.3842 - accuracy: 0.9167 - val_loss: 0.4844 - val_accuracy: 0.8500\n",
      "Epoch 154/205\n",
      "2/2 [==============================] - 0s 126ms/step - loss: 0.3836 - accuracy: 0.9167 - val_loss: 0.4844 - val_accuracy: 0.8500\n",
      "Epoch 155/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.3830 - accuracy: 0.9167 - val_loss: 0.4843 - val_accuracy: 0.8500\n",
      "Epoch 156/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.3824 - accuracy: 0.9167 - val_loss: 0.4843 - val_accuracy: 0.8500\n",
      "Epoch 157/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.3818 - accuracy: 0.9167 - val_loss: 0.4842 - val_accuracy: 0.8500\n",
      "Epoch 158/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.3812 - accuracy: 0.9167 - val_loss: 0.4842 - val_accuracy: 0.8500\n",
      "Epoch 159/205\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.3806 - accuracy: 0.9167 - val_loss: 0.4842 - val_accuracy: 0.8500\n",
      "Epoch 160/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.3801 - accuracy: 0.9167 - val_loss: 0.4841 - val_accuracy: 0.8500\n",
      "Epoch 161/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.3795 - accuracy: 0.9167 - val_loss: 0.4840 - val_accuracy: 0.8500\n",
      "Epoch 162/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.3789 - accuracy: 0.9167 - val_loss: 0.4840 - val_accuracy: 0.8500\n",
      "Epoch 163/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.3783 - accuracy: 0.9167 - val_loss: 0.4839 - val_accuracy: 0.8000\n",
      "Epoch 164/205\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.3778 - accuracy: 0.9167 - val_loss: 0.4839 - val_accuracy: 0.8000\n",
      "Epoch 165/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.3772 - accuracy: 0.9167 - val_loss: 0.4838 - val_accuracy: 0.8000\n",
      "Epoch 166/205\n",
      "2/2 [==============================] - 0s 119ms/step - loss: 0.3766 - accuracy: 0.9167 - val_loss: 0.4836 - val_accuracy: 0.8000\n",
      "Epoch 167/205\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 0.3760 - accuracy: 0.9167 - val_loss: 0.4837 - val_accuracy: 0.8000\n",
      "Epoch 168/205\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 0.3755 - accuracy: 0.9167 - val_loss: 0.4837 - val_accuracy: 0.8000\n",
      "Epoch 169/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.3749 - accuracy: 0.9250 - val_loss: 0.4836 - val_accuracy: 0.8000\n",
      "Epoch 170/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.3743 - accuracy: 0.9250 - val_loss: 0.4835 - val_accuracy: 0.8000\n",
      "Epoch 171/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.3738 - accuracy: 0.9250 - val_loss: 0.4834 - val_accuracy: 0.8000\n",
      "Epoch 172/205\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 0.3732 - accuracy: 0.9250 - val_loss: 0.4834 - val_accuracy: 0.8000\n",
      "Epoch 173/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.3726 - accuracy: 0.9250 - val_loss: 0.4833 - val_accuracy: 0.8000\n",
      "Epoch 174/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.3721 - accuracy: 0.9250 - val_loss: 0.4832 - val_accuracy: 0.8000\n",
      "Epoch 175/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.3715 - accuracy: 0.9250 - val_loss: 0.4832 - val_accuracy: 0.8000\n",
      "Epoch 176/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.3710 - accuracy: 0.9250 - val_loss: 0.4831 - val_accuracy: 0.8000\n",
      "Epoch 177/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.3704 - accuracy: 0.9250 - val_loss: 0.4830 - val_accuracy: 0.8000\n",
      "Epoch 178/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.3699 - accuracy: 0.9250 - val_loss: 0.4829 - val_accuracy: 0.8000\n",
      "Epoch 179/205\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.3693 - accuracy: 0.9250 - val_loss: 0.4828 - val_accuracy: 0.8000\n",
      "Epoch 180/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.3688 - accuracy: 0.9250 - val_loss: 0.4828 - val_accuracy: 0.8000\n",
      "Epoch 181/205\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.3682 - accuracy: 0.9250 - val_loss: 0.4828 - val_accuracy: 0.8000\n",
      "Epoch 182/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.3677 - accuracy: 0.9250 - val_loss: 0.4827 - val_accuracy: 0.8000\n",
      "Epoch 183/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.3671 - accuracy: 0.9250 - val_loss: 0.4826 - val_accuracy: 0.8000\n",
      "Epoch 184/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.3666 - accuracy: 0.9250 - val_loss: 0.4826 - val_accuracy: 0.8000\n",
      "Epoch 185/205\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.3660 - accuracy: 0.9250 - val_loss: 0.4825 - val_accuracy: 0.8000\n",
      "Epoch 186/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.3655 - accuracy: 0.9250 - val_loss: 0.4825 - val_accuracy: 0.8000\n",
      "Epoch 187/205\n",
      "2/2 [==============================] - 0s 123ms/step - loss: 0.3650 - accuracy: 0.9250 - val_loss: 0.4824 - val_accuracy: 0.8000\n",
      "Epoch 188/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.3644 - accuracy: 0.9250 - val_loss: 0.4823 - val_accuracy: 0.8000\n",
      "Epoch 189/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.3639 - accuracy: 0.9250 - val_loss: 0.4823 - val_accuracy: 0.8000\n",
      "Epoch 190/205\n",
      "2/2 [==============================] - 0s 127ms/step - loss: 0.3634 - accuracy: 0.9250 - val_loss: 0.4822 - val_accuracy: 0.8000\n",
      "Epoch 191/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.3628 - accuracy: 0.9250 - val_loss: 0.4821 - val_accuracy: 0.8000\n",
      "Epoch 192/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.3623 - accuracy: 0.9250 - val_loss: 0.4821 - val_accuracy: 0.8000\n",
      "Epoch 193/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.3617 - accuracy: 0.9250 - val_loss: 0.4822 - val_accuracy: 0.8000\n",
      "Epoch 194/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.3612 - accuracy: 0.9250 - val_loss: 0.4820 - val_accuracy: 0.8000\n",
      "Epoch 195/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.3607 - accuracy: 0.9250 - val_loss: 0.4818 - val_accuracy: 0.8000\n",
      "Epoch 196/205\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.3602 - accuracy: 0.9250 - val_loss: 0.4818 - val_accuracy: 0.8000\n",
      "Epoch 197/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.3596 - accuracy: 0.9250 - val_loss: 0.4818 - val_accuracy: 0.8000\n",
      "Epoch 198/205\n",
      "2/2 [==============================] - 0s 125ms/step - loss: 0.3591 - accuracy: 0.9250 - val_loss: 0.4819 - val_accuracy: 0.8000\n",
      "Epoch 199/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.3586 - accuracy: 0.9250 - val_loss: 0.4817 - val_accuracy: 0.8000\n",
      "Epoch 200/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.3581 - accuracy: 0.9250 - val_loss: 0.4813 - val_accuracy: 0.8000\n",
      "Epoch 201/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.3576 - accuracy: 0.9250 - val_loss: 0.4812 - val_accuracy: 0.8000\n",
      "Epoch 202/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.3571 - accuracy: 0.9250 - val_loss: 0.4811 - val_accuracy: 0.8000\n",
      "Epoch 203/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.3565 - accuracy: 0.9250 - val_loss: 0.4811 - val_accuracy: 0.8000\n",
      "Epoch 204/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.3560 - accuracy: 0.9250 - val_loss: 0.4812 - val_accuracy: 0.8000\n",
      "Epoch 205/205\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.3555 - accuracy: 0.9250 - val_loss: 0.4814 - val_accuracy: 0.8000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.5295 - accuracy: 0.8000\n",
      "\n",
      "Fold  1\n",
      "\n",
      "Test accuracy:  0.800000011920929\n",
      "Test loss:  0.529462993144989\n",
      "\n",
      "Fold  2\n",
      "\n",
      "Training with 120 samples and validating with 20 samples\n",
      "\n",
      "Epoch 1/205\n",
      "2/2 [==============================] - 0s 153ms/step - loss: 0.3796 - accuracy: 0.9000 - val_loss: 0.3657 - val_accuracy: 0.9500\n",
      "Epoch 2/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.3791 - accuracy: 0.9000 - val_loss: 0.3655 - val_accuracy: 0.9500\n",
      "Epoch 3/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.3785 - accuracy: 0.9000 - val_loss: 0.3654 - val_accuracy: 0.9500\n",
      "Epoch 4/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.3779 - accuracy: 0.9000 - val_loss: 0.3654 - val_accuracy: 0.9500\n",
      "Epoch 5/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.3771 - accuracy: 0.9000 - val_loss: 0.3655 - val_accuracy: 0.9500\n",
      "Epoch 6/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.3764 - accuracy: 0.9000 - val_loss: 0.3657 - val_accuracy: 0.9500\n",
      "Epoch 7/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.3756 - accuracy: 0.9000 - val_loss: 0.3659 - val_accuracy: 0.9500\n",
      "Epoch 8/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.3748 - accuracy: 0.9083 - val_loss: 0.3663 - val_accuracy: 0.9500\n",
      "Epoch 9/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.3740 - accuracy: 0.9083 - val_loss: 0.3667 - val_accuracy: 0.9500\n",
      "Epoch 10/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.3732 - accuracy: 0.9083 - val_loss: 0.3671 - val_accuracy: 0.9500\n",
      "Epoch 11/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.3724 - accuracy: 0.9083 - val_loss: 0.3675 - val_accuracy: 0.9500\n",
      "Epoch 12/205\n",
      "2/2 [==============================] - 0s 123ms/step - loss: 0.3716 - accuracy: 0.9083 - val_loss: 0.3678 - val_accuracy: 0.9000\n",
      "Epoch 13/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.3708 - accuracy: 0.9083 - val_loss: 0.3681 - val_accuracy: 0.9000\n",
      "Epoch 14/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.3700 - accuracy: 0.9083 - val_loss: 0.3684 - val_accuracy: 0.9000\n",
      "Epoch 15/205\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 0.3692 - accuracy: 0.9083 - val_loss: 0.3687 - val_accuracy: 0.9000\n",
      "Epoch 16/205\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 0.3684 - accuracy: 0.9083 - val_loss: 0.3690 - val_accuracy: 0.9000\n",
      "Epoch 17/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.3676 - accuracy: 0.9083 - val_loss: 0.3693 - val_accuracy: 0.9000\n",
      "Epoch 18/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.3668 - accuracy: 0.9083 - val_loss: 0.3697 - val_accuracy: 0.9000\n",
      "Epoch 19/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.3660 - accuracy: 0.9083 - val_loss: 0.3701 - val_accuracy: 0.9000\n",
      "Epoch 20/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.3652 - accuracy: 0.9167 - val_loss: 0.3704 - val_accuracy: 0.9000\n",
      "Epoch 21/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.3644 - accuracy: 0.9167 - val_loss: 0.3708 - val_accuracy: 0.9000\n",
      "Epoch 22/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.3637 - accuracy: 0.9250 - val_loss: 0.3711 - val_accuracy: 0.9000\n",
      "Epoch 23/205\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.3629 - accuracy: 0.9250 - val_loss: 0.3715 - val_accuracy: 0.9000\n",
      "Epoch 24/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.3621 - accuracy: 0.9250 - val_loss: 0.3718 - val_accuracy: 0.9000\n",
      "Epoch 25/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.3614 - accuracy: 0.9250 - val_loss: 0.3721 - val_accuracy: 0.9000\n",
      "Epoch 26/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.3607 - accuracy: 0.9250 - val_loss: 0.3724 - val_accuracy: 0.9000\n",
      "Epoch 27/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.3599 - accuracy: 0.9250 - val_loss: 0.3727 - val_accuracy: 0.9000\n",
      "Epoch 28/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.3592 - accuracy: 0.9250 - val_loss: 0.3730 - val_accuracy: 0.9000\n",
      "Epoch 29/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.3585 - accuracy: 0.9250 - val_loss: 0.3732 - val_accuracy: 0.9000\n",
      "Epoch 30/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.3577 - accuracy: 0.9250 - val_loss: 0.3735 - val_accuracy: 0.9000\n",
      "Epoch 31/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.3570 - accuracy: 0.9250 - val_loss: 0.3739 - val_accuracy: 0.9000\n",
      "Epoch 32/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.3563 - accuracy: 0.9167 - val_loss: 0.3742 - val_accuracy: 0.9000\n",
      "Epoch 33/205\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.3556 - accuracy: 0.9167 - val_loss: 0.3746 - val_accuracy: 0.9000\n",
      "Epoch 34/205\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.3549 - accuracy: 0.9167 - val_loss: 0.3749 - val_accuracy: 0.8500\n",
      "Epoch 35/205\n",
      "2/2 [==============================] - 0s 109ms/step - loss: 0.3542 - accuracy: 0.9167 - val_loss: 0.3752 - val_accuracy: 0.8500\n",
      "Epoch 36/205\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 0.3536 - accuracy: 0.9167 - val_loss: 0.3756 - val_accuracy: 0.8000\n",
      "Epoch 37/205\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.3529 - accuracy: 0.9167 - val_loss: 0.3759 - val_accuracy: 0.8000\n",
      "Epoch 38/205\n",
      "2/2 [==============================] - 0s 105ms/step - loss: 0.3522 - accuracy: 0.9167 - val_loss: 0.3762 - val_accuracy: 0.8000\n",
      "Epoch 39/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.3515 - accuracy: 0.9167 - val_loss: 0.3765 - val_accuracy: 0.8000\n",
      "Epoch 40/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.3509 - accuracy: 0.9167 - val_loss: 0.3768 - val_accuracy: 0.8000\n",
      "Epoch 41/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.3502 - accuracy: 0.9167 - val_loss: 0.3771 - val_accuracy: 0.8000\n",
      "Epoch 42/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.3496 - accuracy: 0.9167 - val_loss: 0.3774 - val_accuracy: 0.8000\n",
      "Epoch 43/205\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 0.3489 - accuracy: 0.9167 - val_loss: 0.3777 - val_accuracy: 0.8000\n",
      "Epoch 44/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.3483 - accuracy: 0.9167 - val_loss: 0.3780 - val_accuracy: 0.8000\n",
      "Epoch 45/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.3476 - accuracy: 0.9167 - val_loss: 0.3783 - val_accuracy: 0.8000\n",
      "Epoch 46/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.3470 - accuracy: 0.9167 - val_loss: 0.3786 - val_accuracy: 0.8000\n",
      "Epoch 47/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.3463 - accuracy: 0.9167 - val_loss: 0.3789 - val_accuracy: 0.8000\n",
      "Epoch 48/205\n",
      "2/2 [==============================] - 0s 123ms/step - loss: 0.3457 - accuracy: 0.9167 - val_loss: 0.3792 - val_accuracy: 0.8000\n",
      "Epoch 49/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.3451 - accuracy: 0.9167 - val_loss: 0.3795 - val_accuracy: 0.8000\n",
      "Epoch 50/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.3445 - accuracy: 0.9167 - val_loss: 0.3797 - val_accuracy: 0.8000\n",
      "Epoch 51/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.3438 - accuracy: 0.9167 - val_loss: 0.3800 - val_accuracy: 0.8000\n",
      "Epoch 52/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.3432 - accuracy: 0.9167 - val_loss: 0.3803 - val_accuracy: 0.8000\n",
      "Epoch 53/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.3426 - accuracy: 0.9167 - val_loss: 0.3806 - val_accuracy: 0.8000\n",
      "Epoch 54/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.3420 - accuracy: 0.9167 - val_loss: 0.3809 - val_accuracy: 0.8000\n",
      "Epoch 55/205\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.3414 - accuracy: 0.9167 - val_loss: 0.3812 - val_accuracy: 0.8000\n",
      "Epoch 56/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.3408 - accuracy: 0.9167 - val_loss: 0.3815 - val_accuracy: 0.8000\n",
      "Epoch 57/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.3401 - accuracy: 0.9167 - val_loss: 0.3818 - val_accuracy: 0.8000\n",
      "Epoch 58/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.3395 - accuracy: 0.9167 - val_loss: 0.3821 - val_accuracy: 0.8000\n",
      "Epoch 59/205\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.3389 - accuracy: 0.9167 - val_loss: 0.3823 - val_accuracy: 0.8000\n",
      "Epoch 60/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.3383 - accuracy: 0.9167 - val_loss: 0.3826 - val_accuracy: 0.8000\n",
      "Epoch 61/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.3377 - accuracy: 0.9167 - val_loss: 0.3829 - val_accuracy: 0.8000\n",
      "Epoch 62/205\n",
      "2/2 [==============================] - 0s 122ms/step - loss: 0.3371 - accuracy: 0.9167 - val_loss: 0.3831 - val_accuracy: 0.8000\n",
      "Epoch 63/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.3365 - accuracy: 0.9167 - val_loss: 0.3834 - val_accuracy: 0.8000\n",
      "Epoch 64/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.3360 - accuracy: 0.9167 - val_loss: 0.3837 - val_accuracy: 0.8000\n",
      "Epoch 65/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.3354 - accuracy: 0.9167 - val_loss: 0.3840 - val_accuracy: 0.8000\n",
      "Epoch 66/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.3348 - accuracy: 0.9167 - val_loss: 0.3843 - val_accuracy: 0.8000\n",
      "Epoch 67/205\n",
      "2/2 [==============================] - 0s 126ms/step - loss: 0.3342 - accuracy: 0.9167 - val_loss: 0.3846 - val_accuracy: 0.8000\n",
      "Epoch 68/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.3336 - accuracy: 0.9167 - val_loss: 0.3849 - val_accuracy: 0.8000\n",
      "Epoch 69/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.3330 - accuracy: 0.9167 - val_loss: 0.3852 - val_accuracy: 0.8000\n",
      "Epoch 70/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.3325 - accuracy: 0.9167 - val_loss: 0.3855 - val_accuracy: 0.8000\n",
      "Epoch 71/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.3319 - accuracy: 0.9167 - val_loss: 0.3858 - val_accuracy: 0.8000\n",
      "Epoch 72/205\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.3313 - accuracy: 0.9167 - val_loss: 0.3862 - val_accuracy: 0.8000\n",
      "Epoch 73/205\n",
      "2/2 [==============================] - 0s 128ms/step - loss: 0.3308 - accuracy: 0.9167 - val_loss: 0.3865 - val_accuracy: 0.8000\n",
      "Epoch 74/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.3302 - accuracy: 0.9167 - val_loss: 0.3868 - val_accuracy: 0.8000\n",
      "Epoch 75/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.3297 - accuracy: 0.9167 - val_loss: 0.3872 - val_accuracy: 0.8000\n",
      "Epoch 76/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.3291 - accuracy: 0.9167 - val_loss: 0.3875 - val_accuracy: 0.8000\n",
      "Epoch 77/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.3285 - accuracy: 0.9167 - val_loss: 0.3879 - val_accuracy: 0.8000\n",
      "Epoch 78/205\n",
      "2/2 [==============================] - 0s 124ms/step - loss: 0.3280 - accuracy: 0.9167 - val_loss: 0.3882 - val_accuracy: 0.8000\n",
      "Epoch 79/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.3275 - accuracy: 0.9167 - val_loss: 0.3885 - val_accuracy: 0.8000\n",
      "Epoch 80/205\n",
      "2/2 [==============================] - 0s 124ms/step - loss: 0.3269 - accuracy: 0.9250 - val_loss: 0.3887 - val_accuracy: 0.8000\n",
      "Epoch 81/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.3264 - accuracy: 0.9250 - val_loss: 0.3890 - val_accuracy: 0.8000\n",
      "Epoch 82/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.3258 - accuracy: 0.9250 - val_loss: 0.3894 - val_accuracy: 0.8000\n",
      "Epoch 83/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.3253 - accuracy: 0.9250 - val_loss: 0.3897 - val_accuracy: 0.8000\n",
      "Epoch 84/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.3247 - accuracy: 0.9250 - val_loss: 0.3900 - val_accuracy: 0.8000\n",
      "Epoch 85/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.3242 - accuracy: 0.9250 - val_loss: 0.3903 - val_accuracy: 0.8000\n",
      "Epoch 86/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.3237 - accuracy: 0.9250 - val_loss: 0.3906 - val_accuracy: 0.8000\n",
      "Epoch 87/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.3232 - accuracy: 0.9250 - val_loss: 0.3909 - val_accuracy: 0.8000\n",
      "Epoch 88/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.3226 - accuracy: 0.9250 - val_loss: 0.3912 - val_accuracy: 0.8000\n",
      "Epoch 89/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.3221 - accuracy: 0.9250 - val_loss: 0.3915 - val_accuracy: 0.8000\n",
      "Epoch 90/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.3216 - accuracy: 0.9250 - val_loss: 0.3917 - val_accuracy: 0.8000\n",
      "Epoch 91/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.3211 - accuracy: 0.9250 - val_loss: 0.3920 - val_accuracy: 0.8000\n",
      "Epoch 92/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.3205 - accuracy: 0.9250 - val_loss: 0.3923 - val_accuracy: 0.8000\n",
      "Epoch 93/205\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.3200 - accuracy: 0.9250 - val_loss: 0.3926 - val_accuracy: 0.8000\n",
      "Epoch 94/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.3195 - accuracy: 0.9250 - val_loss: 0.3928 - val_accuracy: 0.8000\n",
      "Epoch 95/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.3190 - accuracy: 0.9250 - val_loss: 0.3931 - val_accuracy: 0.8000\n",
      "Epoch 96/205\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.3185 - accuracy: 0.9333 - val_loss: 0.3933 - val_accuracy: 0.8000\n",
      "Epoch 97/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.3180 - accuracy: 0.9333 - val_loss: 0.3936 - val_accuracy: 0.8000\n",
      "Epoch 98/205\n",
      "2/2 [==============================] - 0s 125ms/step - loss: 0.3175 - accuracy: 0.9333 - val_loss: 0.3939 - val_accuracy: 0.8000\n",
      "Epoch 99/205\n",
      "2/2 [==============================] - 0s 121ms/step - loss: 0.3170 - accuracy: 0.9333 - val_loss: 0.3941 - val_accuracy: 0.8000\n",
      "Epoch 100/205\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.3165 - accuracy: 0.9333 - val_loss: 0.3944 - val_accuracy: 0.8000\n",
      "Epoch 101/205\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 0.3160 - accuracy: 0.9333 - val_loss: 0.3947 - val_accuracy: 0.8000\n",
      "Epoch 102/205\n",
      "2/2 [==============================] - 0s 115ms/step - loss: 0.3155 - accuracy: 0.9333 - val_loss: 0.3949 - val_accuracy: 0.8000\n",
      "Epoch 103/205\n",
      "2/2 [==============================] - 0s 113ms/step - loss: 0.3150 - accuracy: 0.9333 - val_loss: 0.3952 - val_accuracy: 0.8000\n",
      "Epoch 104/205\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.3145 - accuracy: 0.9333 - val_loss: 0.3955 - val_accuracy: 0.8000\n",
      "Epoch 105/205\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.3140 - accuracy: 0.9333 - val_loss: 0.3957 - val_accuracy: 0.8000\n",
      "Epoch 106/205\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 0.3135 - accuracy: 0.9333 - val_loss: 0.3960 - val_accuracy: 0.8000\n",
      "Epoch 107/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.3130 - accuracy: 0.9333 - val_loss: 0.3962 - val_accuracy: 0.8000\n",
      "Epoch 108/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.3125 - accuracy: 0.9333 - val_loss: 0.3965 - val_accuracy: 0.8000\n",
      "Epoch 109/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.3120 - accuracy: 0.9333 - val_loss: 0.3968 - val_accuracy: 0.8000\n",
      "Epoch 110/205\n",
      "2/2 [==============================] - 0s 114ms/step - loss: 0.3116 - accuracy: 0.9333 - val_loss: 0.3970 - val_accuracy: 0.8000\n",
      "Epoch 111/205\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.3111 - accuracy: 0.9333 - val_loss: 0.3973 - val_accuracy: 0.8000\n",
      "Epoch 112/205\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 0.3106 - accuracy: 0.9333 - val_loss: 0.3975 - val_accuracy: 0.8000\n",
      "Epoch 113/205\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 0.3101 - accuracy: 0.9333 - val_loss: 0.3978 - val_accuracy: 0.8000\n",
      "Epoch 114/205\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.3097 - accuracy: 0.9333 - val_loss: 0.3980 - val_accuracy: 0.8000\n",
      "Epoch 115/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.3092 - accuracy: 0.9333 - val_loss: 0.3983 - val_accuracy: 0.8000\n",
      "Epoch 116/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.3087 - accuracy: 0.9333 - val_loss: 0.3985 - val_accuracy: 0.8000\n",
      "Epoch 117/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.3082 - accuracy: 0.9333 - val_loss: 0.3987 - val_accuracy: 0.8000\n",
      "Epoch 118/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.3078 - accuracy: 0.9333 - val_loss: 0.3990 - val_accuracy: 0.8000\n",
      "Epoch 119/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.3073 - accuracy: 0.9333 - val_loss: 0.3992 - val_accuracy: 0.8000\n",
      "Epoch 120/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.3068 - accuracy: 0.9333 - val_loss: 0.3995 - val_accuracy: 0.8000\n",
      "Epoch 121/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.3064 - accuracy: 0.9333 - val_loss: 0.3997 - val_accuracy: 0.8000\n",
      "Epoch 122/205\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.3059 - accuracy: 0.9333 - val_loss: 0.4000 - val_accuracy: 0.8000\n",
      "Epoch 123/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.3054 - accuracy: 0.9333 - val_loss: 0.4002 - val_accuracy: 0.8000\n",
      "Epoch 124/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.3050 - accuracy: 0.9333 - val_loss: 0.4004 - val_accuracy: 0.8000\n",
      "Epoch 125/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.3045 - accuracy: 0.9333 - val_loss: 0.4007 - val_accuracy: 0.8000\n",
      "Epoch 126/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.3041 - accuracy: 0.9333 - val_loss: 0.4009 - val_accuracy: 0.8000\n",
      "Epoch 127/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.3036 - accuracy: 0.9333 - val_loss: 0.4012 - val_accuracy: 0.8000\n",
      "Epoch 128/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.3032 - accuracy: 0.9333 - val_loss: 0.4015 - val_accuracy: 0.8000\n",
      "Epoch 129/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.3027 - accuracy: 0.9333 - val_loss: 0.4017 - val_accuracy: 0.8000\n",
      "Epoch 130/205\n",
      "2/2 [==============================] - 0s 124ms/step - loss: 0.3023 - accuracy: 0.9333 - val_loss: 0.4020 - val_accuracy: 0.8000\n",
      "Epoch 131/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.3018 - accuracy: 0.9333 - val_loss: 0.4022 - val_accuracy: 0.8000\n",
      "Epoch 132/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.3014 - accuracy: 0.9333 - val_loss: 0.4024 - val_accuracy: 0.8000\n",
      "Epoch 133/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.3009 - accuracy: 0.9333 - val_loss: 0.4027 - val_accuracy: 0.8000\n",
      "Epoch 134/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.3005 - accuracy: 0.9333 - val_loss: 0.4029 - val_accuracy: 0.8000\n",
      "Epoch 135/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.3000 - accuracy: 0.9333 - val_loss: 0.4032 - val_accuracy: 0.8000\n",
      "Epoch 136/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.2996 - accuracy: 0.9333 - val_loss: 0.4034 - val_accuracy: 0.8000\n",
      "Epoch 137/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.2991 - accuracy: 0.9333 - val_loss: 0.4036 - val_accuracy: 0.8000\n",
      "Epoch 138/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.2987 - accuracy: 0.9333 - val_loss: 0.4039 - val_accuracy: 0.8000\n",
      "Epoch 139/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.2983 - accuracy: 0.9333 - val_loss: 0.4041 - val_accuracy: 0.8000\n",
      "Epoch 140/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.2978 - accuracy: 0.9333 - val_loss: 0.4043 - val_accuracy: 0.8000\n",
      "Epoch 141/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.2974 - accuracy: 0.9333 - val_loss: 0.4046 - val_accuracy: 0.8000\n",
      "Epoch 142/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.2969 - accuracy: 0.9333 - val_loss: 0.4048 - val_accuracy: 0.8000\n",
      "Epoch 143/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.2965 - accuracy: 0.9333 - val_loss: 0.4050 - val_accuracy: 0.8000\n",
      "Epoch 144/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.2961 - accuracy: 0.9333 - val_loss: 0.4052 - val_accuracy: 0.8000\n",
      "Epoch 145/205\n",
      "2/2 [==============================] - 0s 127ms/step - loss: 0.2957 - accuracy: 0.9333 - val_loss: 0.4055 - val_accuracy: 0.8000\n",
      "Epoch 146/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.2952 - accuracy: 0.9333 - val_loss: 0.4057 - val_accuracy: 0.8000\n",
      "Epoch 147/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.2948 - accuracy: 0.9333 - val_loss: 0.4059 - val_accuracy: 0.8000\n",
      "Epoch 148/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.2944 - accuracy: 0.9333 - val_loss: 0.4061 - val_accuracy: 0.8000\n",
      "Epoch 149/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.2939 - accuracy: 0.9333 - val_loss: 0.4064 - val_accuracy: 0.8000\n",
      "Epoch 150/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.2935 - accuracy: 0.9333 - val_loss: 0.4066 - val_accuracy: 0.8500\n",
      "Epoch 151/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.2931 - accuracy: 0.9333 - val_loss: 0.4068 - val_accuracy: 0.8500\n",
      "Epoch 152/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.2927 - accuracy: 0.9333 - val_loss: 0.4071 - val_accuracy: 0.8500\n",
      "Epoch 153/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.2922 - accuracy: 0.9333 - val_loss: 0.4073 - val_accuracy: 0.8500\n",
      "Epoch 154/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.2918 - accuracy: 0.9333 - val_loss: 0.4076 - val_accuracy: 0.8500\n",
      "Epoch 155/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.2914 - accuracy: 0.9333 - val_loss: 0.4078 - val_accuracy: 0.8500\n",
      "Epoch 156/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.2910 - accuracy: 0.9333 - val_loss: 0.4080 - val_accuracy: 0.8500\n",
      "Epoch 157/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.2906 - accuracy: 0.9333 - val_loss: 0.4082 - val_accuracy: 0.8500\n",
      "Epoch 158/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.2902 - accuracy: 0.9333 - val_loss: 0.4084 - val_accuracy: 0.8500\n",
      "Epoch 159/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.2897 - accuracy: 0.9333 - val_loss: 0.4086 - val_accuracy: 0.8500\n",
      "Epoch 160/205\n",
      "2/2 [==============================] - 0s 126ms/step - loss: 0.2893 - accuracy: 0.9333 - val_loss: 0.4089 - val_accuracy: 0.8500\n",
      "Epoch 161/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.2889 - accuracy: 0.9333 - val_loss: 0.4091 - val_accuracy: 0.8500\n",
      "Epoch 162/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.2885 - accuracy: 0.9333 - val_loss: 0.4093 - val_accuracy: 0.8500\n",
      "Epoch 163/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.2881 - accuracy: 0.9333 - val_loss: 0.4095 - val_accuracy: 0.8500\n",
      "Epoch 164/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.2877 - accuracy: 0.9333 - val_loss: 0.4098 - val_accuracy: 0.8500\n",
      "Epoch 165/205\n",
      "2/2 [==============================] - 0s 123ms/step - loss: 0.2873 - accuracy: 0.9333 - val_loss: 0.4100 - val_accuracy: 0.8500\n",
      "Epoch 166/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.2869 - accuracy: 0.9417 - val_loss: 0.4102 - val_accuracy: 0.8500\n",
      "Epoch 167/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.2865 - accuracy: 0.9417 - val_loss: 0.4104 - val_accuracy: 0.8500\n",
      "Epoch 168/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.2861 - accuracy: 0.9417 - val_loss: 0.4106 - val_accuracy: 0.8500\n",
      "Epoch 169/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.2857 - accuracy: 0.9417 - val_loss: 0.4108 - val_accuracy: 0.8500\n",
      "Epoch 170/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.2853 - accuracy: 0.9417 - val_loss: 0.4110 - val_accuracy: 0.8500\n",
      "Epoch 171/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.2849 - accuracy: 0.9417 - val_loss: 0.4112 - val_accuracy: 0.8500\n",
      "Epoch 172/205\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 0.2845 - accuracy: 0.9417 - val_loss: 0.4114 - val_accuracy: 0.8500\n",
      "Epoch 173/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.2841 - accuracy: 0.9417 - val_loss: 0.4116 - val_accuracy: 0.8500\n",
      "Epoch 174/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.2837 - accuracy: 0.9417 - val_loss: 0.4118 - val_accuracy: 0.8500\n",
      "Epoch 175/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.2833 - accuracy: 0.9417 - val_loss: 0.4120 - val_accuracy: 0.8500\n",
      "Epoch 176/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.2829 - accuracy: 0.9417 - val_loss: 0.4122 - val_accuracy: 0.8500\n",
      "Epoch 177/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.2825 - accuracy: 0.9500 - val_loss: 0.4124 - val_accuracy: 0.8500\n",
      "Epoch 178/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.2821 - accuracy: 0.9500 - val_loss: 0.4126 - val_accuracy: 0.8500\n",
      "Epoch 179/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.2817 - accuracy: 0.9500 - val_loss: 0.4129 - val_accuracy: 0.8500\n",
      "Epoch 180/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.2813 - accuracy: 0.9500 - val_loss: 0.4131 - val_accuracy: 0.8500\n",
      "Epoch 181/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.2809 - accuracy: 0.9500 - val_loss: 0.4133 - val_accuracy: 0.8500\n",
      "Epoch 182/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.2805 - accuracy: 0.9500 - val_loss: 0.4135 - val_accuracy: 0.8500\n",
      "Epoch 183/205\n",
      "2/2 [==============================] - 0s 124ms/step - loss: 0.2801 - accuracy: 0.9500 - val_loss: 0.4137 - val_accuracy: 0.8500\n",
      "Epoch 184/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.2798 - accuracy: 0.9500 - val_loss: 0.4139 - val_accuracy: 0.8500\n",
      "Epoch 185/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.2794 - accuracy: 0.9500 - val_loss: 0.4142 - val_accuracy: 0.8500\n",
      "Epoch 186/205\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.2790 - accuracy: 0.9500 - val_loss: 0.4144 - val_accuracy: 0.8500\n",
      "Epoch 187/205\n",
      "2/2 [==============================] - 0s 108ms/step - loss: 0.2786 - accuracy: 0.9500 - val_loss: 0.4146 - val_accuracy: 0.8500\n",
      "Epoch 188/205\n",
      "2/2 [==============================] - 0s 111ms/step - loss: 0.2782 - accuracy: 0.9500 - val_loss: 0.4148 - val_accuracy: 0.8500\n",
      "Epoch 189/205\n",
      "2/2 [==============================] - 0s 111ms/step - loss: 0.2779 - accuracy: 0.9500 - val_loss: 0.4150 - val_accuracy: 0.8500\n",
      "Epoch 190/205\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.2775 - accuracy: 0.9500 - val_loss: 0.4152 - val_accuracy: 0.8500\n",
      "Epoch 191/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.2771 - accuracy: 0.9500 - val_loss: 0.4155 - val_accuracy: 0.8500\n",
      "Epoch 192/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.2767 - accuracy: 0.9500 - val_loss: 0.4157 - val_accuracy: 0.8500\n",
      "Epoch 193/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.2763 - accuracy: 0.9500 - val_loss: 0.4159 - val_accuracy: 0.8500\n",
      "Epoch 194/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.2760 - accuracy: 0.9500 - val_loss: 0.4161 - val_accuracy: 0.8500\n",
      "Epoch 195/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.2756 - accuracy: 0.9500 - val_loss: 0.4163 - val_accuracy: 0.8500\n",
      "Epoch 196/205\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 0.2752 - accuracy: 0.9500 - val_loss: 0.4166 - val_accuracy: 0.8500\n",
      "Epoch 197/205\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 0.2749 - accuracy: 0.9500 - val_loss: 0.4168 - val_accuracy: 0.8500\n",
      "Epoch 198/205\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.2745 - accuracy: 0.9500 - val_loss: 0.4170 - val_accuracy: 0.8500\n",
      "Epoch 199/205\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 0.2741 - accuracy: 0.9500 - val_loss: 0.4172 - val_accuracy: 0.8500\n",
      "Epoch 200/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.2737 - accuracy: 0.9500 - val_loss: 0.4175 - val_accuracy: 0.8500\n",
      "Epoch 201/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.2734 - accuracy: 0.9500 - val_loss: 0.4177 - val_accuracy: 0.8500\n",
      "Epoch 202/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.2730 - accuracy: 0.9500 - val_loss: 0.4179 - val_accuracy: 0.8500\n",
      "Epoch 203/205\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.2726 - accuracy: 0.9500 - val_loss: 0.4181 - val_accuracy: 0.8500\n",
      "Epoch 204/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.2723 - accuracy: 0.9500 - val_loss: 0.4183 - val_accuracy: 0.8500\n",
      "Epoch 205/205\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.2719 - accuracy: 0.9500 - val_loss: 0.4185 - val_accuracy: 0.8500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.4680 - accuracy: 0.8000\n",
      "\n",
      "Fold  2\n",
      "\n",
      "Test accuracy:  0.800000011920929\n",
      "Test loss:  0.46799740195274353\n",
      "\n",
      "Fold  3\n",
      "\n",
      "Training with 120 samples and validating with 20 samples\n",
      "\n",
      "Epoch 1/205\n",
      "2/2 [==============================] - 0s 118ms/step - loss: 0.3069 - accuracy: 0.9333 - val_loss: 0.2733 - val_accuracy: 0.9500\n",
      "Epoch 2/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.3064 - accuracy: 0.9333 - val_loss: 0.2742 - val_accuracy: 0.9500\n",
      "Epoch 3/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.3058 - accuracy: 0.9333 - val_loss: 0.2754 - val_accuracy: 0.9500\n",
      "Epoch 4/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.3052 - accuracy: 0.9333 - val_loss: 0.2769 - val_accuracy: 0.9500\n",
      "Epoch 5/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.3045 - accuracy: 0.9333 - val_loss: 0.2785 - val_accuracy: 0.9500\n",
      "Epoch 6/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.3038 - accuracy: 0.9167 - val_loss: 0.2801 - val_accuracy: 0.9500\n",
      "Epoch 7/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.3031 - accuracy: 0.9167 - val_loss: 0.2815 - val_accuracy: 0.9500\n",
      "Epoch 8/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.3024 - accuracy: 0.9167 - val_loss: 0.2828 - val_accuracy: 0.9500\n",
      "Epoch 9/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.3017 - accuracy: 0.9167 - val_loss: 0.2838 - val_accuracy: 0.9500\n",
      "Epoch 10/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.3011 - accuracy: 0.9167 - val_loss: 0.2846 - val_accuracy: 0.9500\n",
      "Epoch 11/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.3004 - accuracy: 0.9167 - val_loss: 0.2853 - val_accuracy: 0.9500\n",
      "Epoch 12/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.2997 - accuracy: 0.9167 - val_loss: 0.2857 - val_accuracy: 0.9500\n",
      "Epoch 13/205\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 0.2990 - accuracy: 0.9167 - val_loss: 0.2860 - val_accuracy: 0.9500\n",
      "Epoch 14/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.2984 - accuracy: 0.9250 - val_loss: 0.2862 - val_accuracy: 0.9500\n",
      "Epoch 15/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.2977 - accuracy: 0.9250 - val_loss: 0.2863 - val_accuracy: 0.9500\n",
      "Epoch 16/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.2971 - accuracy: 0.9250 - val_loss: 0.2863 - val_accuracy: 0.9500\n",
      "Epoch 17/205\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 0.2964 - accuracy: 0.9250 - val_loss: 0.2864 - val_accuracy: 0.9500\n",
      "Epoch 18/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.2958 - accuracy: 0.9250 - val_loss: 0.2864 - val_accuracy: 0.9500\n",
      "Epoch 19/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.2952 - accuracy: 0.9250 - val_loss: 0.2865 - val_accuracy: 0.9500\n",
      "Epoch 20/205\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.2946 - accuracy: 0.9250 - val_loss: 0.2866 - val_accuracy: 0.9500\n",
      "Epoch 21/205\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.2940 - accuracy: 0.9250 - val_loss: 0.2867 - val_accuracy: 0.9500\n",
      "Epoch 22/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.2934 - accuracy: 0.9250 - val_loss: 0.2867 - val_accuracy: 0.9500\n",
      "Epoch 23/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.2928 - accuracy: 0.9250 - val_loss: 0.2867 - val_accuracy: 0.9500\n",
      "Epoch 24/205\n",
      "2/2 [==============================] - 0s 123ms/step - loss: 0.2922 - accuracy: 0.9333 - val_loss: 0.2868 - val_accuracy: 0.9500\n",
      "Epoch 25/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.2916 - accuracy: 0.9333 - val_loss: 0.2868 - val_accuracy: 0.9500\n",
      "Epoch 26/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.2911 - accuracy: 0.9333 - val_loss: 0.2869 - val_accuracy: 0.9500\n",
      "Epoch 27/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.2905 - accuracy: 0.9333 - val_loss: 0.2870 - val_accuracy: 0.9500\n",
      "Epoch 28/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.2900 - accuracy: 0.9333 - val_loss: 0.2871 - val_accuracy: 0.9500\n",
      "Epoch 29/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.2894 - accuracy: 0.9333 - val_loss: 0.2873 - val_accuracy: 0.9500\n",
      "Epoch 30/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.2889 - accuracy: 0.9333 - val_loss: 0.2875 - val_accuracy: 0.9500\n",
      "Epoch 31/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.2884 - accuracy: 0.9333 - val_loss: 0.2877 - val_accuracy: 0.9500\n",
      "Epoch 32/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.2879 - accuracy: 0.9333 - val_loss: 0.2878 - val_accuracy: 0.9500\n",
      "Epoch 33/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.2874 - accuracy: 0.9333 - val_loss: 0.2880 - val_accuracy: 0.9500\n",
      "Epoch 34/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.2868 - accuracy: 0.9333 - val_loss: 0.2881 - val_accuracy: 0.9500\n",
      "Epoch 35/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.2863 - accuracy: 0.9333 - val_loss: 0.2883 - val_accuracy: 0.9500\n",
      "Epoch 36/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.2858 - accuracy: 0.9333 - val_loss: 0.2884 - val_accuracy: 0.9500\n",
      "Epoch 37/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.2854 - accuracy: 0.9333 - val_loss: 0.2886 - val_accuracy: 0.9500\n",
      "Epoch 38/205\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.2849 - accuracy: 0.9333 - val_loss: 0.2887 - val_accuracy: 0.9500\n",
      "Epoch 39/205\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.2844 - accuracy: 0.9333 - val_loss: 0.2888 - val_accuracy: 0.9500\n",
      "Epoch 40/205\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.2839 - accuracy: 0.9333 - val_loss: 0.2889 - val_accuracy: 0.9500\n",
      "Epoch 41/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.2835 - accuracy: 0.9333 - val_loss: 0.2890 - val_accuracy: 0.9500\n",
      "Epoch 42/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.2830 - accuracy: 0.9333 - val_loss: 0.2892 - val_accuracy: 0.9500\n",
      "Epoch 43/205\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.2825 - accuracy: 0.9333 - val_loss: 0.2894 - val_accuracy: 0.9500\n",
      "Epoch 44/205\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 0.2821 - accuracy: 0.9333 - val_loss: 0.2895 - val_accuracy: 0.9500\n",
      "Epoch 45/205\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.2816 - accuracy: 0.9333 - val_loss: 0.2897 - val_accuracy: 0.9500\n",
      "Epoch 46/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.2812 - accuracy: 0.9333 - val_loss: 0.2899 - val_accuracy: 0.9500\n",
      "Epoch 47/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.2807 - accuracy: 0.9333 - val_loss: 0.2900 - val_accuracy: 0.9500\n",
      "Epoch 48/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.2803 - accuracy: 0.9333 - val_loss: 0.2902 - val_accuracy: 0.9500\n",
      "Epoch 49/205\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.2798 - accuracy: 0.9333 - val_loss: 0.2903 - val_accuracy: 0.9500\n",
      "Epoch 50/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.2794 - accuracy: 0.9333 - val_loss: 0.2905 - val_accuracy: 0.9500\n",
      "Epoch 51/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.2790 - accuracy: 0.9333 - val_loss: 0.2906 - val_accuracy: 0.9500\n",
      "Epoch 52/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.2785 - accuracy: 0.9333 - val_loss: 0.2908 - val_accuracy: 0.9500\n",
      "Epoch 53/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.2781 - accuracy: 0.9333 - val_loss: 0.2909 - val_accuracy: 0.9500\n",
      "Epoch 54/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.2777 - accuracy: 0.9333 - val_loss: 0.2911 - val_accuracy: 0.9500\n",
      "Epoch 55/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.2773 - accuracy: 0.9417 - val_loss: 0.2912 - val_accuracy: 0.9500\n",
      "Epoch 56/205\n",
      "2/2 [==============================] - 0s 114ms/step - loss: 0.2768 - accuracy: 0.9417 - val_loss: 0.2913 - val_accuracy: 0.9500\n",
      "Epoch 57/205\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 0.2764 - accuracy: 0.9417 - val_loss: 0.2915 - val_accuracy: 0.9500\n",
      "Epoch 58/205\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.2760 - accuracy: 0.9417 - val_loss: 0.2917 - val_accuracy: 0.9500\n",
      "Epoch 59/205\n",
      "2/2 [==============================] - 0s 106ms/step - loss: 0.2756 - accuracy: 0.9417 - val_loss: 0.2918 - val_accuracy: 0.9500\n",
      "Epoch 60/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.2752 - accuracy: 0.9417 - val_loss: 0.2920 - val_accuracy: 0.9500\n",
      "Epoch 61/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.2748 - accuracy: 0.9417 - val_loss: 0.2921 - val_accuracy: 0.9500\n",
      "Epoch 62/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.2744 - accuracy: 0.9417 - val_loss: 0.2923 - val_accuracy: 0.9500\n",
      "Epoch 63/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.2740 - accuracy: 0.9417 - val_loss: 0.2924 - val_accuracy: 0.9500\n",
      "Epoch 64/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.2736 - accuracy: 0.9417 - val_loss: 0.2925 - val_accuracy: 0.9500\n",
      "Epoch 65/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.2732 - accuracy: 0.9417 - val_loss: 0.2927 - val_accuracy: 0.9500\n",
      "Epoch 66/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.2728 - accuracy: 0.9417 - val_loss: 0.2928 - val_accuracy: 0.9500\n",
      "Epoch 67/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.2724 - accuracy: 0.9417 - val_loss: 0.2930 - val_accuracy: 0.9500\n",
      "Epoch 68/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.2720 - accuracy: 0.9417 - val_loss: 0.2931 - val_accuracy: 0.9500\n",
      "Epoch 69/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.2716 - accuracy: 0.9417 - val_loss: 0.2933 - val_accuracy: 0.9500\n",
      "Epoch 70/205\n",
      "2/2 [==============================] - 0s 128ms/step - loss: 0.2712 - accuracy: 0.9417 - val_loss: 0.2934 - val_accuracy: 0.9500\n",
      "Epoch 71/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.2708 - accuracy: 0.9417 - val_loss: 0.2935 - val_accuracy: 0.9500\n",
      "Epoch 72/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.2704 - accuracy: 0.9417 - val_loss: 0.2937 - val_accuracy: 0.9500\n",
      "Epoch 73/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.2700 - accuracy: 0.9417 - val_loss: 0.2938 - val_accuracy: 0.9500\n",
      "Epoch 74/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.2697 - accuracy: 0.9417 - val_loss: 0.2939 - val_accuracy: 0.9500\n",
      "Epoch 75/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.2693 - accuracy: 0.9500 - val_loss: 0.2940 - val_accuracy: 0.9500\n",
      "Epoch 76/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.2689 - accuracy: 0.9500 - val_loss: 0.2942 - val_accuracy: 0.9500\n",
      "Epoch 77/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.2685 - accuracy: 0.9500 - val_loss: 0.2944 - val_accuracy: 0.9500\n",
      "Epoch 78/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.2681 - accuracy: 0.9500 - val_loss: 0.2945 - val_accuracy: 0.9500\n",
      "Epoch 79/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.2678 - accuracy: 0.9500 - val_loss: 0.2946 - val_accuracy: 0.9500\n",
      "Epoch 80/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.2674 - accuracy: 0.9500 - val_loss: 0.2947 - val_accuracy: 0.9500\n",
      "Epoch 81/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.2670 - accuracy: 0.9500 - val_loss: 0.2948 - val_accuracy: 0.9500\n",
      "Epoch 82/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.2667 - accuracy: 0.9500 - val_loss: 0.2949 - val_accuracy: 0.9500\n",
      "Epoch 83/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.2663 - accuracy: 0.9417 - val_loss: 0.2950 - val_accuracy: 0.9500\n",
      "Epoch 84/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.2659 - accuracy: 0.9417 - val_loss: 0.2951 - val_accuracy: 0.9500\n",
      "Epoch 85/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.2656 - accuracy: 0.9417 - val_loss: 0.2952 - val_accuracy: 0.9500\n",
      "Epoch 86/205\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 0.2652 - accuracy: 0.9417 - val_loss: 0.2954 - val_accuracy: 0.9500\n",
      "Epoch 87/205\n",
      "2/2 [==============================] - 0s 128ms/step - loss: 0.2648 - accuracy: 0.9417 - val_loss: 0.2955 - val_accuracy: 0.9500\n",
      "Epoch 88/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.2645 - accuracy: 0.9417 - val_loss: 0.2957 - val_accuracy: 0.9500\n",
      "Epoch 89/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.2641 - accuracy: 0.9417 - val_loss: 0.2958 - val_accuracy: 0.9500\n",
      "Epoch 90/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.2638 - accuracy: 0.9417 - val_loss: 0.2959 - val_accuracy: 0.9500\n",
      "Epoch 91/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.2634 - accuracy: 0.9417 - val_loss: 0.2960 - val_accuracy: 0.9500\n",
      "Epoch 92/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.2630 - accuracy: 0.9417 - val_loss: 0.2962 - val_accuracy: 0.9500\n",
      "Epoch 93/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.2627 - accuracy: 0.9417 - val_loss: 0.2963 - val_accuracy: 0.9500\n",
      "Epoch 94/205\n",
      "2/2 [==============================] - 0s 123ms/step - loss: 0.2623 - accuracy: 0.9417 - val_loss: 0.2964 - val_accuracy: 0.9500\n",
      "Epoch 95/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.2620 - accuracy: 0.9500 - val_loss: 0.2965 - val_accuracy: 0.9500\n",
      "Epoch 96/205\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.2616 - accuracy: 0.9500 - val_loss: 0.2966 - val_accuracy: 0.9500\n",
      "Epoch 97/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.2613 - accuracy: 0.9500 - val_loss: 0.2968 - val_accuracy: 0.9500\n",
      "Epoch 98/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.2609 - accuracy: 0.9500 - val_loss: 0.2969 - val_accuracy: 0.9500\n",
      "Epoch 99/205\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.2606 - accuracy: 0.9500 - val_loss: 0.2970 - val_accuracy: 0.9500\n",
      "Epoch 100/205\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.2602 - accuracy: 0.9500 - val_loss: 0.2971 - val_accuracy: 0.9500\n",
      "Epoch 101/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.2599 - accuracy: 0.9500 - val_loss: 0.2972 - val_accuracy: 0.9500\n",
      "Epoch 102/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.2595 - accuracy: 0.9500 - val_loss: 0.2974 - val_accuracy: 0.9500\n",
      "Epoch 103/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.2592 - accuracy: 0.9500 - val_loss: 0.2975 - val_accuracy: 0.9500\n",
      "Epoch 104/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.2589 - accuracy: 0.9500 - val_loss: 0.2977 - val_accuracy: 0.9500\n",
      "Epoch 105/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.2585 - accuracy: 0.9500 - val_loss: 0.2978 - val_accuracy: 0.9500\n",
      "Epoch 106/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.2582 - accuracy: 0.9500 - val_loss: 0.2979 - val_accuracy: 0.9500\n",
      "Epoch 107/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.2578 - accuracy: 0.9500 - val_loss: 0.2980 - val_accuracy: 0.9500\n",
      "Epoch 108/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.2575 - accuracy: 0.9500 - val_loss: 0.2982 - val_accuracy: 0.9500\n",
      "Epoch 109/205\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 0.2572 - accuracy: 0.9500 - val_loss: 0.2983 - val_accuracy: 0.9500\n",
      "Epoch 110/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.2568 - accuracy: 0.9500 - val_loss: 0.2984 - val_accuracy: 0.9500\n",
      "Epoch 111/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.2565 - accuracy: 0.9500 - val_loss: 0.2985 - val_accuracy: 0.9500\n",
      "Epoch 112/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.2561 - accuracy: 0.9500 - val_loss: 0.2986 - val_accuracy: 0.9500\n",
      "Epoch 113/205\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 0.2558 - accuracy: 0.9500 - val_loss: 0.2988 - val_accuracy: 0.9500\n",
      "Epoch 114/205\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 0.2555 - accuracy: 0.9500 - val_loss: 0.2989 - val_accuracy: 0.9500\n",
      "Epoch 115/205\n",
      "2/2 [==============================] - 0s 108ms/step - loss: 0.2552 - accuracy: 0.9500 - val_loss: 0.2990 - val_accuracy: 0.9500\n",
      "Epoch 116/205\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 0.2548 - accuracy: 0.9500 - val_loss: 0.2991 - val_accuracy: 0.9500\n",
      "Epoch 117/205\n",
      "2/2 [==============================] - 0s 112ms/step - loss: 0.2545 - accuracy: 0.9500 - val_loss: 0.2993 - val_accuracy: 0.9500\n",
      "Epoch 118/205\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 0.2542 - accuracy: 0.9500 - val_loss: 0.2994 - val_accuracy: 0.9500\n",
      "Epoch 119/205\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.2538 - accuracy: 0.9500 - val_loss: 0.2995 - val_accuracy: 0.9500\n",
      "Epoch 120/205\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.2535 - accuracy: 0.9500 - val_loss: 0.2996 - val_accuracy: 0.9500\n",
      "Epoch 121/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.2532 - accuracy: 0.9500 - val_loss: 0.2997 - val_accuracy: 0.9500\n",
      "Epoch 122/205\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.2529 - accuracy: 0.9500 - val_loss: 0.2999 - val_accuracy: 0.9500\n",
      "Epoch 123/205\n",
      "2/2 [==============================] - 0s 123ms/step - loss: 0.2525 - accuracy: 0.9500 - val_loss: 0.3000 - val_accuracy: 0.9500\n",
      "Epoch 124/205\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 0.2522 - accuracy: 0.9500 - val_loss: 0.3001 - val_accuracy: 0.9500\n",
      "Epoch 125/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.2519 - accuracy: 0.9500 - val_loss: 0.3003 - val_accuracy: 0.9500\n",
      "Epoch 126/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.2516 - accuracy: 0.9500 - val_loss: 0.3004 - val_accuracy: 0.9500\n",
      "Epoch 127/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.2512 - accuracy: 0.9500 - val_loss: 0.3005 - val_accuracy: 0.9500\n",
      "Epoch 128/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.2509 - accuracy: 0.9500 - val_loss: 0.3007 - val_accuracy: 0.9500\n",
      "Epoch 129/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.2506 - accuracy: 0.9500 - val_loss: 0.3008 - val_accuracy: 0.9500\n",
      "Epoch 130/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.2503 - accuracy: 0.9500 - val_loss: 0.3009 - val_accuracy: 0.9500\n",
      "Epoch 131/205\n",
      "2/2 [==============================] - 0s 118ms/step - loss: 0.2500 - accuracy: 0.9500 - val_loss: 0.3010 - val_accuracy: 0.9500\n",
      "Epoch 132/205\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 0.2497 - accuracy: 0.9500 - val_loss: 0.3012 - val_accuracy: 0.9500\n",
      "Epoch 133/205\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 0.2493 - accuracy: 0.9500 - val_loss: 0.3013 - val_accuracy: 0.9500\n",
      "Epoch 134/205\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 0.2490 - accuracy: 0.9500 - val_loss: 0.3014 - val_accuracy: 0.9500\n",
      "Epoch 135/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.2487 - accuracy: 0.9500 - val_loss: 0.3016 - val_accuracy: 0.9500\n",
      "Epoch 136/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.2484 - accuracy: 0.9500 - val_loss: 0.3017 - val_accuracy: 0.9500\n",
      "Epoch 137/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.2481 - accuracy: 0.9500 - val_loss: 0.3018 - val_accuracy: 0.9500\n",
      "Epoch 138/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.2478 - accuracy: 0.9500 - val_loss: 0.3019 - val_accuracy: 0.9500\n",
      "Epoch 139/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.2475 - accuracy: 0.9500 - val_loss: 0.3020 - val_accuracy: 0.9500\n",
      "Epoch 140/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.2472 - accuracy: 0.9500 - val_loss: 0.3021 - val_accuracy: 0.9500\n",
      "Epoch 141/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.2468 - accuracy: 0.9500 - val_loss: 0.3022 - val_accuracy: 0.9500\n",
      "Epoch 142/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.2465 - accuracy: 0.9500 - val_loss: 0.3024 - val_accuracy: 0.9500\n",
      "Epoch 143/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.2462 - accuracy: 0.9500 - val_loss: 0.3025 - val_accuracy: 0.9500\n",
      "Epoch 144/205\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.2459 - accuracy: 0.9500 - val_loss: 0.3026 - val_accuracy: 0.9500\n",
      "Epoch 145/205\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.2456 - accuracy: 0.9500 - val_loss: 0.3028 - val_accuracy: 0.9500\n",
      "Epoch 146/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.2453 - accuracy: 0.9500 - val_loss: 0.3029 - val_accuracy: 0.9500\n",
      "Epoch 147/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.2450 - accuracy: 0.9500 - val_loss: 0.3030 - val_accuracy: 0.9500\n",
      "Epoch 148/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.2447 - accuracy: 0.9500 - val_loss: 0.3031 - val_accuracy: 0.9500\n",
      "Epoch 149/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.2444 - accuracy: 0.9500 - val_loss: 0.3032 - val_accuracy: 0.9500\n",
      "Epoch 150/205\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.2441 - accuracy: 0.9500 - val_loss: 0.3033 - val_accuracy: 0.9500\n",
      "Epoch 151/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.2438 - accuracy: 0.9500 - val_loss: 0.3035 - val_accuracy: 0.9500\n",
      "Epoch 152/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.2435 - accuracy: 0.9500 - val_loss: 0.3036 - val_accuracy: 0.9500\n",
      "Epoch 153/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.2432 - accuracy: 0.9500 - val_loss: 0.3037 - val_accuracy: 0.9500\n",
      "Epoch 154/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.2429 - accuracy: 0.9500 - val_loss: 0.3039 - val_accuracy: 0.9500\n",
      "Epoch 155/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.2426 - accuracy: 0.9500 - val_loss: 0.3040 - val_accuracy: 0.9500\n",
      "Epoch 156/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.2423 - accuracy: 0.9500 - val_loss: 0.3041 - val_accuracy: 0.9500\n",
      "Epoch 157/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.2420 - accuracy: 0.9500 - val_loss: 0.3042 - val_accuracy: 0.9500\n",
      "Epoch 158/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.2417 - accuracy: 0.9500 - val_loss: 0.3044 - val_accuracy: 0.9500\n",
      "Epoch 159/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.2414 - accuracy: 0.9500 - val_loss: 0.3045 - val_accuracy: 0.9500\n",
      "Epoch 160/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.2411 - accuracy: 0.9500 - val_loss: 0.3046 - val_accuracy: 0.9500\n",
      "Epoch 161/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.2408 - accuracy: 0.9500 - val_loss: 0.3048 - val_accuracy: 0.9500\n",
      "Epoch 162/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.2405 - accuracy: 0.9500 - val_loss: 0.3049 - val_accuracy: 0.9500\n",
      "Epoch 163/205\n",
      "2/2 [==============================] - 0s 127ms/step - loss: 0.2402 - accuracy: 0.9500 - val_loss: 0.3050 - val_accuracy: 0.9500\n",
      "Epoch 164/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.2399 - accuracy: 0.9500 - val_loss: 0.3051 - val_accuracy: 0.9500\n",
      "Epoch 165/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.2397 - accuracy: 0.9500 - val_loss: 0.3052 - val_accuracy: 0.9500\n",
      "Epoch 166/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.2394 - accuracy: 0.9500 - val_loss: 0.3054 - val_accuracy: 0.9500\n",
      "Epoch 167/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.2391 - accuracy: 0.9500 - val_loss: 0.3055 - val_accuracy: 0.9500\n",
      "Epoch 168/205\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.2388 - accuracy: 0.9500 - val_loss: 0.3056 - val_accuracy: 0.9500\n",
      "Epoch 169/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.2385 - accuracy: 0.9500 - val_loss: 0.3057 - val_accuracy: 0.9500\n",
      "Epoch 170/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.2382 - accuracy: 0.9500 - val_loss: 0.3059 - val_accuracy: 0.9500\n",
      "Epoch 171/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.2379 - accuracy: 0.9583 - val_loss: 0.3060 - val_accuracy: 0.9500\n",
      "Epoch 172/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.2376 - accuracy: 0.9583 - val_loss: 0.3061 - val_accuracy: 0.9500\n",
      "Epoch 173/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.2373 - accuracy: 0.9583 - val_loss: 0.3062 - val_accuracy: 0.9500\n",
      "Epoch 174/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.2371 - accuracy: 0.9583 - val_loss: 0.3064 - val_accuracy: 0.9500\n",
      "Epoch 175/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.2368 - accuracy: 0.9583 - val_loss: 0.3065 - val_accuracy: 0.9500\n",
      "Epoch 176/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.2365 - accuracy: 0.9583 - val_loss: 0.3066 - val_accuracy: 0.9500\n",
      "Epoch 177/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.2362 - accuracy: 0.9583 - val_loss: 0.3067 - val_accuracy: 0.9500\n",
      "Epoch 178/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.2359 - accuracy: 0.9583 - val_loss: 0.3068 - val_accuracy: 0.9500\n",
      "Epoch 179/205\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.2356 - accuracy: 0.9583 - val_loss: 0.3070 - val_accuracy: 0.9500\n",
      "Epoch 180/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.2354 - accuracy: 0.9583 - val_loss: 0.3071 - val_accuracy: 0.9500\n",
      "Epoch 181/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.2351 - accuracy: 0.9583 - val_loss: 0.3072 - val_accuracy: 0.9500\n",
      "Epoch 182/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.2348 - accuracy: 0.9583 - val_loss: 0.3073 - val_accuracy: 0.9500\n",
      "Epoch 183/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.2345 - accuracy: 0.9583 - val_loss: 0.3074 - val_accuracy: 0.9500\n",
      "Epoch 184/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.2342 - accuracy: 0.9583 - val_loss: 0.3076 - val_accuracy: 0.9500\n",
      "Epoch 185/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.2340 - accuracy: 0.9583 - val_loss: 0.3077 - val_accuracy: 0.9500\n",
      "Epoch 186/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.2337 - accuracy: 0.9583 - val_loss: 0.3078 - val_accuracy: 0.9500\n",
      "Epoch 187/205\n",
      "2/2 [==============================] - 0s 123ms/step - loss: 0.2334 - accuracy: 0.9583 - val_loss: 0.3079 - val_accuracy: 0.9500\n",
      "Epoch 188/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.2331 - accuracy: 0.9583 - val_loss: 0.3081 - val_accuracy: 0.9500\n",
      "Epoch 189/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.2329 - accuracy: 0.9583 - val_loss: 0.3082 - val_accuracy: 0.9500\n",
      "Epoch 190/205\n",
      "2/2 [==============================] - 0s 122ms/step - loss: 0.2326 - accuracy: 0.9583 - val_loss: 0.3083 - val_accuracy: 0.9500\n",
      "Epoch 191/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.2323 - accuracy: 0.9583 - val_loss: 0.3084 - val_accuracy: 0.9500\n",
      "Epoch 192/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.2320 - accuracy: 0.9583 - val_loss: 0.3086 - val_accuracy: 0.9500\n",
      "Epoch 193/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.2318 - accuracy: 0.9583 - val_loss: 0.3087 - val_accuracy: 0.9500\n",
      "Epoch 194/205\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.2315 - accuracy: 0.9583 - val_loss: 0.3088 - val_accuracy: 0.9500\n",
      "Epoch 195/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.2312 - accuracy: 0.9583 - val_loss: 0.3090 - val_accuracy: 0.9500\n",
      "Epoch 196/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.2310 - accuracy: 0.9583 - val_loss: 0.3091 - val_accuracy: 0.9500\n",
      "Epoch 197/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.2307 - accuracy: 0.9583 - val_loss: 0.3092 - val_accuracy: 0.9500\n",
      "Epoch 198/205\n",
      "2/2 [==============================] - 0s 122ms/step - loss: 0.2304 - accuracy: 0.9583 - val_loss: 0.3093 - val_accuracy: 0.9500\n",
      "Epoch 199/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.2301 - accuracy: 0.9583 - val_loss: 0.3095 - val_accuracy: 0.9500\n",
      "Epoch 200/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.2299 - accuracy: 0.9583 - val_loss: 0.3096 - val_accuracy: 0.9500\n",
      "Epoch 201/205\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 0.2296 - accuracy: 0.9583 - val_loss: 0.3097 - val_accuracy: 0.9500\n",
      "Epoch 202/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.2293 - accuracy: 0.9583 - val_loss: 0.3098 - val_accuracy: 0.9500\n",
      "Epoch 203/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.2291 - accuracy: 0.9583 - val_loss: 0.3100 - val_accuracy: 0.9500\n",
      "Epoch 204/205\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.2288 - accuracy: 0.9583 - val_loss: 0.3101 - val_accuracy: 0.9500\n",
      "Epoch 205/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.2285 - accuracy: 0.9583 - val_loss: 0.3102 - val_accuracy: 0.9500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.4375 - accuracy: 0.8500\n",
      "\n",
      "Fold  3\n",
      "\n",
      "Test accuracy:  0.8500000238418579\n",
      "Test loss:  0.43748825788497925\n",
      "\n",
      "Fold  4\n",
      "\n",
      "Training with 120 samples and validating with 20 samples\n",
      "\n",
      "Epoch 1/205\n",
      "2/2 [==============================] - 0s 154ms/step - loss: 0.2487 - accuracy: 0.9583 - val_loss: 0.2256 - val_accuracy: 0.9500\n",
      "Epoch 2/205\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 0.2484 - accuracy: 0.9583 - val_loss: 0.2259 - val_accuracy: 0.9500\n",
      "Epoch 3/205\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 0.2480 - accuracy: 0.9583 - val_loss: 0.2265 - val_accuracy: 0.9500\n",
      "Epoch 4/205\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 0.2477 - accuracy: 0.9583 - val_loss: 0.2273 - val_accuracy: 0.9500\n",
      "Epoch 5/205\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 0.2473 - accuracy: 0.9583 - val_loss: 0.2282 - val_accuracy: 0.9500\n",
      "Epoch 6/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.2470 - accuracy: 0.9583 - val_loss: 0.2291 - val_accuracy: 0.9000\n",
      "Epoch 7/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.2466 - accuracy: 0.9583 - val_loss: 0.2301 - val_accuracy: 0.9000\n",
      "Epoch 8/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.2463 - accuracy: 0.9583 - val_loss: 0.2310 - val_accuracy: 0.9000\n",
      "Epoch 9/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.2460 - accuracy: 0.9583 - val_loss: 0.2318 - val_accuracy: 0.9000\n",
      "Epoch 10/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.2457 - accuracy: 0.9583 - val_loss: 0.2324 - val_accuracy: 0.9000\n",
      "Epoch 11/205\n",
      "2/2 [==============================] - 0s 129ms/step - loss: 0.2453 - accuracy: 0.9583 - val_loss: 0.2328 - val_accuracy: 0.9000\n",
      "Epoch 12/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.2450 - accuracy: 0.9583 - val_loss: 0.2332 - val_accuracy: 0.9000\n",
      "Epoch 13/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.2447 - accuracy: 0.9583 - val_loss: 0.2334 - val_accuracy: 0.9000\n",
      "Epoch 14/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.2444 - accuracy: 0.9583 - val_loss: 0.2335 - val_accuracy: 0.9000\n",
      "Epoch 15/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.2440 - accuracy: 0.9583 - val_loss: 0.2336 - val_accuracy: 0.9000\n",
      "Epoch 16/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.2437 - accuracy: 0.9583 - val_loss: 0.2336 - val_accuracy: 0.9000\n",
      "Epoch 17/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.2433 - accuracy: 0.9583 - val_loss: 0.2337 - val_accuracy: 0.9000\n",
      "Epoch 18/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.2430 - accuracy: 0.9583 - val_loss: 0.2338 - val_accuracy: 0.9000\n",
      "Epoch 19/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.2427 - accuracy: 0.9583 - val_loss: 0.2339 - val_accuracy: 0.9000\n",
      "Epoch 20/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.2423 - accuracy: 0.9583 - val_loss: 0.2341 - val_accuracy: 0.9000\n",
      "Epoch 21/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.2420 - accuracy: 0.9583 - val_loss: 0.2342 - val_accuracy: 0.9000\n",
      "Epoch 22/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.2417 - accuracy: 0.9583 - val_loss: 0.2344 - val_accuracy: 0.9000\n",
      "Epoch 23/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.2413 - accuracy: 0.9583 - val_loss: 0.2345 - val_accuracy: 0.9000\n",
      "Epoch 24/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.2410 - accuracy: 0.9583 - val_loss: 0.2345 - val_accuracy: 0.9000\n",
      "Epoch 25/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.2407 - accuracy: 0.9583 - val_loss: 0.2346 - val_accuracy: 0.9000\n",
      "Epoch 26/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.2403 - accuracy: 0.9583 - val_loss: 0.2346 - val_accuracy: 0.9000\n",
      "Epoch 27/205\n",
      "2/2 [==============================] - 0s 106ms/step - loss: 0.2400 - accuracy: 0.9583 - val_loss: 0.2347 - val_accuracy: 0.9000\n",
      "Epoch 28/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.2397 - accuracy: 0.9583 - val_loss: 0.2347 - val_accuracy: 0.9000\n",
      "Epoch 29/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.2394 - accuracy: 0.9583 - val_loss: 0.2348 - val_accuracy: 0.9000\n",
      "Epoch 30/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.2390 - accuracy: 0.9583 - val_loss: 0.2349 - val_accuracy: 0.9000\n",
      "Epoch 31/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.2387 - accuracy: 0.9583 - val_loss: 0.2350 - val_accuracy: 0.9000\n",
      "Epoch 32/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.2384 - accuracy: 0.9583 - val_loss: 0.2351 - val_accuracy: 0.9000\n",
      "Epoch 33/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.2381 - accuracy: 0.9583 - val_loss: 0.2353 - val_accuracy: 0.9000\n",
      "Epoch 34/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.2378 - accuracy: 0.9583 - val_loss: 0.2354 - val_accuracy: 0.9000\n",
      "Epoch 35/205\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 0.2374 - accuracy: 0.9583 - val_loss: 0.2356 - val_accuracy: 0.9000\n",
      "Epoch 36/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.2371 - accuracy: 0.9583 - val_loss: 0.2356 - val_accuracy: 0.9000\n",
      "Epoch 37/205\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.2368 - accuracy: 0.9583 - val_loss: 0.2357 - val_accuracy: 0.9000\n",
      "Epoch 38/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.2365 - accuracy: 0.9583 - val_loss: 0.2358 - val_accuracy: 0.9000\n",
      "Epoch 39/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.2362 - accuracy: 0.9583 - val_loss: 0.2359 - val_accuracy: 0.9000\n",
      "Epoch 40/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.2359 - accuracy: 0.9583 - val_loss: 0.2360 - val_accuracy: 0.9000\n",
      "Epoch 41/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.2356 - accuracy: 0.9583 - val_loss: 0.2361 - val_accuracy: 0.9000\n",
      "Epoch 42/205\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.2353 - accuracy: 0.9583 - val_loss: 0.2363 - val_accuracy: 0.9000\n",
      "Epoch 43/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.2350 - accuracy: 0.9583 - val_loss: 0.2364 - val_accuracy: 0.9000\n",
      "Epoch 44/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.2347 - accuracy: 0.9583 - val_loss: 0.2365 - val_accuracy: 0.9000\n",
      "Epoch 45/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.2344 - accuracy: 0.9583 - val_loss: 0.2366 - val_accuracy: 0.9000\n",
      "Epoch 46/205\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 0.2341 - accuracy: 0.9583 - val_loss: 0.2367 - val_accuracy: 0.9000\n",
      "Epoch 47/205\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 0.2338 - accuracy: 0.9583 - val_loss: 0.2368 - val_accuracy: 0.9000\n",
      "Epoch 48/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.2335 - accuracy: 0.9583 - val_loss: 0.2369 - val_accuracy: 0.9000\n",
      "Epoch 49/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.2332 - accuracy: 0.9583 - val_loss: 0.2370 - val_accuracy: 0.9000\n",
      "Epoch 50/205\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.2329 - accuracy: 0.9583 - val_loss: 0.2371 - val_accuracy: 0.9000\n",
      "Epoch 51/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.2326 - accuracy: 0.9583 - val_loss: 0.2372 - val_accuracy: 0.9000\n",
      "Epoch 52/205\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.2323 - accuracy: 0.9583 - val_loss: 0.2373 - val_accuracy: 0.9000\n",
      "Epoch 53/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.2320 - accuracy: 0.9583 - val_loss: 0.2374 - val_accuracy: 0.9000\n",
      "Epoch 54/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.2317 - accuracy: 0.9583 - val_loss: 0.2375 - val_accuracy: 0.9000\n",
      "Epoch 55/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.2314 - accuracy: 0.9583 - val_loss: 0.2376 - val_accuracy: 0.9000\n",
      "Epoch 56/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.2311 - accuracy: 0.9583 - val_loss: 0.2377 - val_accuracy: 0.9000\n",
      "Epoch 57/205\n",
      "2/2 [==============================] - 0s 125ms/step - loss: 0.2308 - accuracy: 0.9583 - val_loss: 0.2377 - val_accuracy: 0.9000\n",
      "Epoch 58/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.2305 - accuracy: 0.9583 - val_loss: 0.2378 - val_accuracy: 0.9000\n",
      "Epoch 59/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.2302 - accuracy: 0.9583 - val_loss: 0.2379 - val_accuracy: 0.9000\n",
      "Epoch 60/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.2299 - accuracy: 0.9583 - val_loss: 0.2380 - val_accuracy: 0.9000\n",
      "Epoch 61/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.2297 - accuracy: 0.9583 - val_loss: 0.2381 - val_accuracy: 0.9000\n",
      "Epoch 62/205\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 0.2294 - accuracy: 0.9583 - val_loss: 0.2382 - val_accuracy: 0.9000\n",
      "Epoch 63/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.2291 - accuracy: 0.9583 - val_loss: 0.2383 - val_accuracy: 0.9000\n",
      "Epoch 64/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.2288 - accuracy: 0.9583 - val_loss: 0.2383 - val_accuracy: 0.9000\n",
      "Epoch 65/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.2285 - accuracy: 0.9583 - val_loss: 0.2384 - val_accuracy: 0.9000\n",
      "Epoch 66/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.2282 - accuracy: 0.9583 - val_loss: 0.2385 - val_accuracy: 0.9000\n",
      "Epoch 67/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.2280 - accuracy: 0.9583 - val_loss: 0.2386 - val_accuracy: 0.9000\n",
      "Epoch 68/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.2277 - accuracy: 0.9667 - val_loss: 0.2387 - val_accuracy: 0.9000\n",
      "Epoch 69/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.2274 - accuracy: 0.9667 - val_loss: 0.2387 - val_accuracy: 0.9000\n",
      "Epoch 70/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.2271 - accuracy: 0.9667 - val_loss: 0.2389 - val_accuracy: 0.9000\n",
      "Epoch 71/205\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.2268 - accuracy: 0.9667 - val_loss: 0.2390 - val_accuracy: 0.9000\n",
      "Epoch 72/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.2266 - accuracy: 0.9667 - val_loss: 0.2391 - val_accuracy: 0.9000\n",
      "Epoch 73/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.2263 - accuracy: 0.9667 - val_loss: 0.2391 - val_accuracy: 0.9000\n",
      "Epoch 74/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.2260 - accuracy: 0.9667 - val_loss: 0.2392 - val_accuracy: 0.9000\n",
      "Epoch 75/205\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.2258 - accuracy: 0.9667 - val_loss: 0.2392 - val_accuracy: 0.9000\n",
      "Epoch 76/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.2255 - accuracy: 0.9667 - val_loss: 0.2393 - val_accuracy: 0.9000\n",
      "Epoch 77/205\n",
      "2/2 [==============================] - 0s 111ms/step - loss: 0.2252 - accuracy: 0.9667 - val_loss: 0.2395 - val_accuracy: 0.9000\n",
      "Epoch 78/205\n",
      "2/2 [==============================] - 0s 105ms/step - loss: 0.2249 - accuracy: 0.9667 - val_loss: 0.2396 - val_accuracy: 0.9000\n",
      "Epoch 79/205\n",
      "2/2 [==============================] - 0s 115ms/step - loss: 0.2247 - accuracy: 0.9667 - val_loss: 0.2397 - val_accuracy: 0.9000\n",
      "Epoch 80/205\n",
      "2/2 [==============================] - 0s 111ms/step - loss: 0.2244 - accuracy: 0.9667 - val_loss: 0.2397 - val_accuracy: 0.9000\n",
      "Epoch 81/205\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 0.2241 - accuracy: 0.9667 - val_loss: 0.2398 - val_accuracy: 0.9000\n",
      "Epoch 82/205\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.2239 - accuracy: 0.9667 - val_loss: 0.2399 - val_accuracy: 0.9000\n",
      "Epoch 83/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.2236 - accuracy: 0.9667 - val_loss: 0.2400 - val_accuracy: 0.9000\n",
      "Epoch 84/205\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 0.2233 - accuracy: 0.9667 - val_loss: 0.2401 - val_accuracy: 0.9000\n",
      "Epoch 85/205\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 0.2231 - accuracy: 0.9667 - val_loss: 0.2402 - val_accuracy: 0.9000\n",
      "Epoch 86/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.2228 - accuracy: 0.9667 - val_loss: 0.2403 - val_accuracy: 0.9000\n",
      "Epoch 87/205\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 0.2225 - accuracy: 0.9667 - val_loss: 0.2404 - val_accuracy: 0.9000\n",
      "Epoch 88/205\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 0.2223 - accuracy: 0.9667 - val_loss: 0.2405 - val_accuracy: 0.9000\n",
      "Epoch 89/205\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 0.2220 - accuracy: 0.9667 - val_loss: 0.2406 - val_accuracy: 0.9000\n",
      "Epoch 90/205\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 0.2218 - accuracy: 0.9750 - val_loss: 0.2406 - val_accuracy: 0.9000\n",
      "Epoch 91/205\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 0.2215 - accuracy: 0.9750 - val_loss: 0.2408 - val_accuracy: 0.9000\n",
      "Epoch 92/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.2212 - accuracy: 0.9750 - val_loss: 0.2409 - val_accuracy: 0.9000\n",
      "Epoch 93/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.2210 - accuracy: 0.9750 - val_loss: 0.2410 - val_accuracy: 0.9000\n",
      "Epoch 94/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.2207 - accuracy: 0.9750 - val_loss: 0.2410 - val_accuracy: 0.9000\n",
      "Epoch 95/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.2205 - accuracy: 0.9750 - val_loss: 0.2411 - val_accuracy: 0.9000\n",
      "Epoch 96/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.2202 - accuracy: 0.9750 - val_loss: 0.2412 - val_accuracy: 0.9000\n",
      "Epoch 97/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.2200 - accuracy: 0.9750 - val_loss: 0.2413 - val_accuracy: 0.9000\n",
      "Epoch 98/205\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.2197 - accuracy: 0.9750 - val_loss: 0.2414 - val_accuracy: 0.9000\n",
      "Epoch 99/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.2194 - accuracy: 0.9750 - val_loss: 0.2415 - val_accuracy: 0.9000\n",
      "Epoch 100/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.2192 - accuracy: 0.9750 - val_loss: 0.2416 - val_accuracy: 0.9000\n",
      "Epoch 101/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.2189 - accuracy: 0.9750 - val_loss: 0.2416 - val_accuracy: 0.9000\n",
      "Epoch 102/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.2187 - accuracy: 0.9750 - val_loss: 0.2417 - val_accuracy: 0.9000\n",
      "Epoch 103/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.2184 - accuracy: 0.9750 - val_loss: 0.2419 - val_accuracy: 0.9000\n",
      "Epoch 104/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.2182 - accuracy: 0.9750 - val_loss: 0.2419 - val_accuracy: 0.9000\n",
      "Epoch 105/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.2179 - accuracy: 0.9750 - val_loss: 0.2420 - val_accuracy: 0.9000\n",
      "Epoch 106/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.2177 - accuracy: 0.9750 - val_loss: 0.2420 - val_accuracy: 0.9000\n",
      "Epoch 107/205\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.2174 - accuracy: 0.9750 - val_loss: 0.2422 - val_accuracy: 0.9000\n",
      "Epoch 108/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.2172 - accuracy: 0.9750 - val_loss: 0.2423 - val_accuracy: 0.9000\n",
      "Epoch 109/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.2169 - accuracy: 0.9750 - val_loss: 0.2424 - val_accuracy: 0.9000\n",
      "Epoch 110/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.2167 - accuracy: 0.9750 - val_loss: 0.2424 - val_accuracy: 0.9000\n",
      "Epoch 111/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.2165 - accuracy: 0.9750 - val_loss: 0.2425 - val_accuracy: 0.9000\n",
      "Epoch 112/205\n",
      "2/2 [==============================] - 0s 129ms/step - loss: 0.2162 - accuracy: 0.9750 - val_loss: 0.2426 - val_accuracy: 0.9000\n",
      "Epoch 113/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.2160 - accuracy: 0.9750 - val_loss: 0.2427 - val_accuracy: 0.9000\n",
      "Epoch 114/205\n",
      "2/2 [==============================] - 0s 128ms/step - loss: 0.2157 - accuracy: 0.9750 - val_loss: 0.2428 - val_accuracy: 0.9000\n",
      "Epoch 115/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.2155 - accuracy: 0.9750 - val_loss: 0.2429 - val_accuracy: 0.9000\n",
      "Epoch 116/205\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.2152 - accuracy: 0.9750 - val_loss: 0.2430 - val_accuracy: 0.9000\n",
      "Epoch 117/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.2150 - accuracy: 0.9750 - val_loss: 0.2430 - val_accuracy: 0.9000\n",
      "Epoch 118/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.2147 - accuracy: 0.9750 - val_loss: 0.2432 - val_accuracy: 0.9000\n",
      "Epoch 119/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.2145 - accuracy: 0.9750 - val_loss: 0.2433 - val_accuracy: 0.9000\n",
      "Epoch 120/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.2143 - accuracy: 0.9750 - val_loss: 0.2433 - val_accuracy: 0.9000\n",
      "Epoch 121/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.2140 - accuracy: 0.9750 - val_loss: 0.2434 - val_accuracy: 0.9000\n",
      "Epoch 122/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.2138 - accuracy: 0.9750 - val_loss: 0.2435 - val_accuracy: 0.9000\n",
      "Epoch 123/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.2136 - accuracy: 0.9750 - val_loss: 0.2436 - val_accuracy: 0.9000\n",
      "Epoch 124/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.2133 - accuracy: 0.9750 - val_loss: 0.2437 - val_accuracy: 0.9000\n",
      "Epoch 125/205\n",
      "2/2 [==============================] - 0s 123ms/step - loss: 0.2131 - accuracy: 0.9750 - val_loss: 0.2438 - val_accuracy: 0.9000\n",
      "Epoch 126/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.2128 - accuracy: 0.9750 - val_loss: 0.2439 - val_accuracy: 0.9000\n",
      "Epoch 127/205\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 0.2126 - accuracy: 0.9750 - val_loss: 0.2440 - val_accuracy: 0.9000\n",
      "Epoch 128/205\n",
      "2/2 [==============================] - 0s 108ms/step - loss: 0.2124 - accuracy: 0.9750 - val_loss: 0.2441 - val_accuracy: 0.9000\n",
      "Epoch 129/205\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.2121 - accuracy: 0.9750 - val_loss: 0.2441 - val_accuracy: 0.9000\n",
      "Epoch 130/205\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.2119 - accuracy: 0.9750 - val_loss: 0.2442 - val_accuracy: 0.9000\n",
      "Epoch 131/205\n",
      "2/2 [==============================] - 0s 109ms/step - loss: 0.2117 - accuracy: 0.9750 - val_loss: 0.2443 - val_accuracy: 0.9000\n",
      "Epoch 132/205\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.2114 - accuracy: 0.9750 - val_loss: 0.2444 - val_accuracy: 0.9000\n",
      "Epoch 133/205\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 0.2112 - accuracy: 0.9750 - val_loss: 0.2445 - val_accuracy: 0.9000\n",
      "Epoch 134/205\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.2110 - accuracy: 0.9750 - val_loss: 0.2446 - val_accuracy: 0.9000\n",
      "Epoch 135/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.2108 - accuracy: 0.9750 - val_loss: 0.2447 - val_accuracy: 0.9000\n",
      "Epoch 136/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.2105 - accuracy: 0.9750 - val_loss: 0.2447 - val_accuracy: 0.9000\n",
      "Epoch 137/205\n",
      "2/2 [==============================] - 0s 125ms/step - loss: 0.2103 - accuracy: 0.9750 - val_loss: 0.2448 - val_accuracy: 0.9000\n",
      "Epoch 138/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.2101 - accuracy: 0.9750 - val_loss: 0.2450 - val_accuracy: 0.9000\n",
      "Epoch 139/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.2098 - accuracy: 0.9750 - val_loss: 0.2451 - val_accuracy: 0.9000\n",
      "Epoch 140/205\n",
      "2/2 [==============================] - 0s 128ms/step - loss: 0.2096 - accuracy: 0.9750 - val_loss: 0.2451 - val_accuracy: 0.9000\n",
      "Epoch 141/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.2094 - accuracy: 0.9750 - val_loss: 0.2452 - val_accuracy: 0.9000\n",
      "Epoch 142/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.2092 - accuracy: 0.9750 - val_loss: 0.2453 - val_accuracy: 0.9000\n",
      "Epoch 143/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.2089 - accuracy: 0.9750 - val_loss: 0.2454 - val_accuracy: 0.9000\n",
      "Epoch 144/205\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.2087 - accuracy: 0.9750 - val_loss: 0.2454 - val_accuracy: 0.9000\n",
      "Epoch 145/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.2085 - accuracy: 0.9750 - val_loss: 0.2456 - val_accuracy: 0.9000\n",
      "Epoch 146/205\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 0.2083 - accuracy: 0.9750 - val_loss: 0.2456 - val_accuracy: 0.9000\n",
      "Epoch 147/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.2080 - accuracy: 0.9750 - val_loss: 0.2457 - val_accuracy: 0.9000\n",
      "Epoch 148/205\n",
      "2/2 [==============================] - 0s 129ms/step - loss: 0.2078 - accuracy: 0.9750 - val_loss: 0.2459 - val_accuracy: 0.9000\n",
      "Epoch 149/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.2076 - accuracy: 0.9750 - val_loss: 0.2459 - val_accuracy: 0.9000\n",
      "Epoch 150/205\n",
      "2/2 [==============================] - 0s 127ms/step - loss: 0.2074 - accuracy: 0.9750 - val_loss: 0.2460 - val_accuracy: 0.9000\n",
      "Epoch 151/205\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.2071 - accuracy: 0.9750 - val_loss: 0.2461 - val_accuracy: 0.9000\n",
      "Epoch 152/205\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 0.2069 - accuracy: 0.9750 - val_loss: 0.2462 - val_accuracy: 0.9000\n",
      "Epoch 153/205\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.2067 - accuracy: 0.9750 - val_loss: 0.2463 - val_accuracy: 0.9000\n",
      "Epoch 154/205\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.2065 - accuracy: 0.9750 - val_loss: 0.2463 - val_accuracy: 0.9000\n",
      "Epoch 155/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.2063 - accuracy: 0.9750 - val_loss: 0.2464 - val_accuracy: 0.9000\n",
      "Epoch 156/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.2060 - accuracy: 0.9750 - val_loss: 0.2465 - val_accuracy: 0.9000\n",
      "Epoch 157/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.2058 - accuracy: 0.9750 - val_loss: 0.2467 - val_accuracy: 0.9000\n",
      "Epoch 158/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.2056 - accuracy: 0.9750 - val_loss: 0.2467 - val_accuracy: 0.9000\n",
      "Epoch 159/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.2054 - accuracy: 0.9750 - val_loss: 0.2468 - val_accuracy: 0.9000\n",
      "Epoch 160/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.2052 - accuracy: 0.9750 - val_loss: 0.2468 - val_accuracy: 0.9000\n",
      "Epoch 161/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.2049 - accuracy: 0.9750 - val_loss: 0.2470 - val_accuracy: 0.9000\n",
      "Epoch 162/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.2047 - accuracy: 0.9750 - val_loss: 0.2471 - val_accuracy: 0.9000\n",
      "Epoch 163/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.2045 - accuracy: 0.9750 - val_loss: 0.2472 - val_accuracy: 0.9000\n",
      "Epoch 164/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.2043 - accuracy: 0.9750 - val_loss: 0.2472 - val_accuracy: 0.9000\n",
      "Epoch 165/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.2041 - accuracy: 0.9750 - val_loss: 0.2473 - val_accuracy: 0.9000\n",
      "Epoch 166/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.2039 - accuracy: 0.9750 - val_loss: 0.2474 - val_accuracy: 0.9000\n",
      "Epoch 167/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.2036 - accuracy: 0.9750 - val_loss: 0.2475 - val_accuracy: 0.9000\n",
      "Epoch 168/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.2034 - accuracy: 0.9750 - val_loss: 0.2476 - val_accuracy: 0.9000\n",
      "Epoch 169/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.2032 - accuracy: 0.9750 - val_loss: 0.2477 - val_accuracy: 0.9000\n",
      "Epoch 170/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.2030 - accuracy: 0.9750 - val_loss: 0.2477 - val_accuracy: 0.9000\n",
      "Epoch 171/205\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 0.2028 - accuracy: 0.9750 - val_loss: 0.2478 - val_accuracy: 0.9000\n",
      "Epoch 172/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.2026 - accuracy: 0.9750 - val_loss: 0.2479 - val_accuracy: 0.9000\n",
      "Epoch 173/205\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.2024 - accuracy: 0.9750 - val_loss: 0.2480 - val_accuracy: 0.9000\n",
      "Epoch 174/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.2022 - accuracy: 0.9750 - val_loss: 0.2481 - val_accuracy: 0.9000\n",
      "Epoch 175/205\n",
      "2/2 [==============================] - 0s 127ms/step - loss: 0.2019 - accuracy: 0.9750 - val_loss: 0.2482 - val_accuracy: 0.9000\n",
      "Epoch 176/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.2017 - accuracy: 0.9750 - val_loss: 0.2483 - val_accuracy: 0.9000\n",
      "Epoch 177/205\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.2015 - accuracy: 0.9750 - val_loss: 0.2483 - val_accuracy: 0.9000\n",
      "Epoch 178/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.2013 - accuracy: 0.9750 - val_loss: 0.2484 - val_accuracy: 0.9000\n",
      "Epoch 179/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.2011 - accuracy: 0.9750 - val_loss: 0.2485 - val_accuracy: 0.9000\n",
      "Epoch 180/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.2009 - accuracy: 0.9750 - val_loss: 0.2486 - val_accuracy: 0.9000\n",
      "Epoch 181/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.2007 - accuracy: 0.9750 - val_loss: 0.2487 - val_accuracy: 0.9000\n",
      "Epoch 182/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.2005 - accuracy: 0.9750 - val_loss: 0.2488 - val_accuracy: 0.9000\n",
      "Epoch 183/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.2003 - accuracy: 0.9750 - val_loss: 0.2489 - val_accuracy: 0.9000\n",
      "Epoch 184/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.2001 - accuracy: 0.9750 - val_loss: 0.2489 - val_accuracy: 0.9000\n",
      "Epoch 185/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.1999 - accuracy: 0.9750 - val_loss: 0.2490 - val_accuracy: 0.9000\n",
      "Epoch 186/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.1996 - accuracy: 0.9750 - val_loss: 0.2491 - val_accuracy: 0.9000\n",
      "Epoch 187/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.1994 - accuracy: 0.9750 - val_loss: 0.2492 - val_accuracy: 0.9000\n",
      "Epoch 188/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.1992 - accuracy: 0.9750 - val_loss: 0.2493 - val_accuracy: 0.9000\n",
      "Epoch 189/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.1990 - accuracy: 0.9750 - val_loss: 0.2493 - val_accuracy: 0.9000\n",
      "Epoch 190/205\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 0.1988 - accuracy: 0.9750 - val_loss: 0.2494 - val_accuracy: 0.9000\n",
      "Epoch 191/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.1986 - accuracy: 0.9750 - val_loss: 0.2495 - val_accuracy: 0.9000\n",
      "Epoch 192/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.1984 - accuracy: 0.9750 - val_loss: 0.2496 - val_accuracy: 0.9000\n",
      "Epoch 193/205\n",
      "2/2 [==============================] - 0s 122ms/step - loss: 0.1982 - accuracy: 0.9833 - val_loss: 0.2497 - val_accuracy: 0.9000\n",
      "Epoch 194/205\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.1980 - accuracy: 0.9833 - val_loss: 0.2497 - val_accuracy: 0.9000\n",
      "Epoch 195/205\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 0.1978 - accuracy: 0.9833 - val_loss: 0.2498 - val_accuracy: 0.9000\n",
      "Epoch 196/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.1976 - accuracy: 0.9833 - val_loss: 0.2500 - val_accuracy: 0.9000\n",
      "Epoch 197/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.1974 - accuracy: 0.9833 - val_loss: 0.2502 - val_accuracy: 0.9000\n",
      "Epoch 198/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.1972 - accuracy: 0.9833 - val_loss: 0.2502 - val_accuracy: 0.9000\n",
      "Epoch 199/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.1970 - accuracy: 0.9833 - val_loss: 0.2502 - val_accuracy: 0.9000\n",
      "Epoch 200/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.1968 - accuracy: 0.9833 - val_loss: 0.2503 - val_accuracy: 0.9000\n",
      "Epoch 201/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.1966 - accuracy: 0.9833 - val_loss: 0.2504 - val_accuracy: 0.9000\n",
      "Epoch 202/205\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.1964 - accuracy: 0.9833 - val_loss: 0.2505 - val_accuracy: 0.9000\n",
      "Epoch 203/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.1962 - accuracy: 0.9833 - val_loss: 0.2506 - val_accuracy: 0.9000\n",
      "Epoch 204/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.1960 - accuracy: 0.9833 - val_loss: 0.2507 - val_accuracy: 0.9000\n",
      "Epoch 205/205\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.1958 - accuracy: 0.9833 - val_loss: 0.2508 - val_accuracy: 0.9000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.4190 - accuracy: 0.9000\n",
      "\n",
      "Fold  4\n",
      "\n",
      "Test accuracy:  0.8999999761581421\n",
      "Test loss:  0.4189908504486084\n",
      "\n",
      "Fold  5\n",
      "\n",
      "Training with 120 samples and validating with 20 samples\n",
      "\n",
      "Epoch 1/205\n",
      "2/2 [==============================] - 0s 153ms/step - loss: 0.2054 - accuracy: 0.9667 - val_loss: 0.1818 - val_accuracy: 1.0000\n",
      "Epoch 2/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.2053 - accuracy: 0.9667 - val_loss: 0.1817 - val_accuracy: 1.0000\n",
      "Epoch 3/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.2051 - accuracy: 0.9667 - val_loss: 0.1817 - val_accuracy: 1.0000\n",
      "Epoch 4/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.2049 - accuracy: 0.9667 - val_loss: 0.1817 - val_accuracy: 1.0000\n",
      "Epoch 5/205\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.2047 - accuracy: 0.9667 - val_loss: 0.1818 - val_accuracy: 1.0000\n",
      "Epoch 6/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.2045 - accuracy: 0.9667 - val_loss: 0.1818 - val_accuracy: 1.0000\n",
      "Epoch 7/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.2043 - accuracy: 0.9667 - val_loss: 0.1819 - val_accuracy: 1.0000\n",
      "Epoch 8/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.2041 - accuracy: 0.9667 - val_loss: 0.1820 - val_accuracy: 1.0000\n",
      "Epoch 9/205\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.2038 - accuracy: 0.9667 - val_loss: 0.1821 - val_accuracy: 1.0000\n",
      "Epoch 10/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.2036 - accuracy: 0.9667 - val_loss: 0.1822 - val_accuracy: 1.0000\n",
      "Epoch 11/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.2034 - accuracy: 0.9667 - val_loss: 0.1823 - val_accuracy: 1.0000\n",
      "Epoch 12/205\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.2032 - accuracy: 0.9667 - val_loss: 0.1825 - val_accuracy: 1.0000\n",
      "Epoch 13/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.2030 - accuracy: 0.9667 - val_loss: 0.1825 - val_accuracy: 1.0000\n",
      "Epoch 14/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.2028 - accuracy: 0.9667 - val_loss: 0.1826 - val_accuracy: 1.0000\n",
      "Epoch 15/205\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.2026 - accuracy: 0.9667 - val_loss: 0.1827 - val_accuracy: 1.0000\n",
      "Epoch 16/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.2023 - accuracy: 0.9667 - val_loss: 0.1828 - val_accuracy: 1.0000\n",
      "Epoch 17/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.2021 - accuracy: 0.9667 - val_loss: 0.1828 - val_accuracy: 1.0000\n",
      "Epoch 18/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.2019 - accuracy: 0.9667 - val_loss: 0.1829 - val_accuracy: 1.0000\n",
      "Epoch 19/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.2017 - accuracy: 0.9667 - val_loss: 0.1829 - val_accuracy: 1.0000\n",
      "Epoch 20/205\n",
      "2/2 [==============================] - 0s 116ms/step - loss: 0.2015 - accuracy: 0.9667 - val_loss: 0.1829 - val_accuracy: 1.0000\n",
      "Epoch 21/205\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 0.2013 - accuracy: 0.9667 - val_loss: 0.1830 - val_accuracy: 1.0000\n",
      "Epoch 22/205\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 0.2010 - accuracy: 0.9667 - val_loss: 0.1830 - val_accuracy: 1.0000\n",
      "Epoch 23/205\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 0.2008 - accuracy: 0.9667 - val_loss: 0.1831 - val_accuracy: 1.0000\n",
      "Epoch 24/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.2006 - accuracy: 0.9667 - val_loss: 0.1831 - val_accuracy: 1.0000\n",
      "Epoch 25/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.2004 - accuracy: 0.9667 - val_loss: 0.1831 - val_accuracy: 1.0000\n",
      "Epoch 26/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.2002 - accuracy: 0.9667 - val_loss: 0.1832 - val_accuracy: 1.0000\n",
      "Epoch 27/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.2000 - accuracy: 0.9667 - val_loss: 0.1832 - val_accuracy: 1.0000\n",
      "Epoch 28/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.1998 - accuracy: 0.9667 - val_loss: 0.1832 - val_accuracy: 1.0000\n",
      "Epoch 29/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.1996 - accuracy: 0.9667 - val_loss: 0.1833 - val_accuracy: 1.0000\n",
      "Epoch 30/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.1993 - accuracy: 0.9667 - val_loss: 0.1833 - val_accuracy: 1.0000\n",
      "Epoch 31/205\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 0.1991 - accuracy: 0.9667 - val_loss: 0.1834 - val_accuracy: 1.0000\n",
      "Epoch 32/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.1989 - accuracy: 0.9667 - val_loss: 0.1834 - val_accuracy: 1.0000\n",
      "Epoch 33/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.1987 - accuracy: 0.9667 - val_loss: 0.1834 - val_accuracy: 1.0000\n",
      "Epoch 34/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.1985 - accuracy: 0.9667 - val_loss: 0.1835 - val_accuracy: 1.0000\n",
      "Epoch 35/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.1983 - accuracy: 0.9667 - val_loss: 0.1835 - val_accuracy: 1.0000\n",
      "Epoch 36/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.1981 - accuracy: 0.9667 - val_loss: 0.1836 - val_accuracy: 1.0000\n",
      "Epoch 37/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.1979 - accuracy: 0.9667 - val_loss: 0.1836 - val_accuracy: 1.0000\n",
      "Epoch 38/205\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.1977 - accuracy: 0.9667 - val_loss: 0.1837 - val_accuracy: 1.0000\n",
      "Epoch 39/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.1975 - accuracy: 0.9667 - val_loss: 0.1837 - val_accuracy: 1.0000\n",
      "Epoch 40/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.1973 - accuracy: 0.9667 - val_loss: 0.1838 - val_accuracy: 1.0000\n",
      "Epoch 41/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.1971 - accuracy: 0.9667 - val_loss: 0.1838 - val_accuracy: 1.0000\n",
      "Epoch 42/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.1969 - accuracy: 0.9667 - val_loss: 0.1839 - val_accuracy: 1.0000\n",
      "Epoch 43/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.1967 - accuracy: 0.9667 - val_loss: 0.1839 - val_accuracy: 1.0000\n",
      "Epoch 44/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.1965 - accuracy: 0.9667 - val_loss: 0.1840 - val_accuracy: 1.0000\n",
      "Epoch 45/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.1963 - accuracy: 0.9667 - val_loss: 0.1840 - val_accuracy: 1.0000\n",
      "Epoch 46/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.1961 - accuracy: 0.9667 - val_loss: 0.1841 - val_accuracy: 1.0000\n",
      "Epoch 47/205\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 0.1959 - accuracy: 0.9667 - val_loss: 0.1841 - val_accuracy: 1.0000\n",
      "Epoch 48/205\n",
      "2/2 [==============================] - 0s 109ms/step - loss: 0.1957 - accuracy: 0.9667 - val_loss: 0.1841 - val_accuracy: 1.0000\n",
      "Epoch 49/205\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.1955 - accuracy: 0.9667 - val_loss: 0.1842 - val_accuracy: 1.0000\n",
      "Epoch 50/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.1953 - accuracy: 0.9667 - val_loss: 0.1842 - val_accuracy: 1.0000\n",
      "Epoch 51/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.1951 - accuracy: 0.9667 - val_loss: 0.1843 - val_accuracy: 1.0000\n",
      "Epoch 52/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.1949 - accuracy: 0.9667 - val_loss: 0.1844 - val_accuracy: 1.0000\n",
      "Epoch 53/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.1947 - accuracy: 0.9667 - val_loss: 0.1844 - val_accuracy: 1.0000\n",
      "Epoch 54/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.1945 - accuracy: 0.9667 - val_loss: 0.1844 - val_accuracy: 1.0000\n",
      "Epoch 55/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.1943 - accuracy: 0.9667 - val_loss: 0.1845 - val_accuracy: 1.0000\n",
      "Epoch 56/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.1941 - accuracy: 0.9667 - val_loss: 0.1845 - val_accuracy: 1.0000\n",
      "Epoch 57/205\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.1939 - accuracy: 0.9667 - val_loss: 0.1846 - val_accuracy: 1.0000\n",
      "Epoch 58/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.1937 - accuracy: 0.9667 - val_loss: 0.1847 - val_accuracy: 1.0000\n",
      "Epoch 59/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.1935 - accuracy: 0.9667 - val_loss: 0.1847 - val_accuracy: 1.0000\n",
      "Epoch 60/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.1933 - accuracy: 0.9667 - val_loss: 0.1847 - val_accuracy: 1.0000\n",
      "Epoch 61/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.1931 - accuracy: 0.9667 - val_loss: 0.1847 - val_accuracy: 1.0000\n",
      "Epoch 62/205\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.1929 - accuracy: 0.9667 - val_loss: 0.1848 - val_accuracy: 1.0000\n",
      "Epoch 63/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.1927 - accuracy: 0.9667 - val_loss: 0.1849 - val_accuracy: 1.0000\n",
      "Epoch 64/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.1926 - accuracy: 0.9667 - val_loss: 0.1849 - val_accuracy: 1.0000\n",
      "Epoch 65/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.1924 - accuracy: 0.9667 - val_loss: 0.1849 - val_accuracy: 1.0000\n",
      "Epoch 66/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.1922 - accuracy: 0.9667 - val_loss: 0.1850 - val_accuracy: 1.0000\n",
      "Epoch 67/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.1920 - accuracy: 0.9667 - val_loss: 0.1850 - val_accuracy: 1.0000\n",
      "Epoch 68/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.1918 - accuracy: 0.9667 - val_loss: 0.1851 - val_accuracy: 1.0000\n",
      "Epoch 69/205\n",
      "2/2 [==============================] - 0s 123ms/step - loss: 0.1916 - accuracy: 0.9667 - val_loss: 0.1851 - val_accuracy: 1.0000\n",
      "Epoch 70/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.1914 - accuracy: 0.9750 - val_loss: 0.1851 - val_accuracy: 1.0000\n",
      "Epoch 71/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.1912 - accuracy: 0.9750 - val_loss: 0.1852 - val_accuracy: 1.0000\n",
      "Epoch 72/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.1910 - accuracy: 0.9750 - val_loss: 0.1852 - val_accuracy: 1.0000\n",
      "Epoch 73/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.1909 - accuracy: 0.9750 - val_loss: 0.1853 - val_accuracy: 1.0000\n",
      "Epoch 74/205\n",
      "2/2 [==============================] - 0s 126ms/step - loss: 0.1907 - accuracy: 0.9750 - val_loss: 0.1853 - val_accuracy: 1.0000\n",
      "Epoch 75/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.1905 - accuracy: 0.9750 - val_loss: 0.1853 - val_accuracy: 1.0000\n",
      "Epoch 76/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.1903 - accuracy: 0.9750 - val_loss: 0.1853 - val_accuracy: 1.0000\n",
      "Epoch 77/205\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 0.1901 - accuracy: 0.9750 - val_loss: 0.1854 - val_accuracy: 1.0000\n",
      "Epoch 78/205\n",
      "2/2 [==============================] - 0s 126ms/step - loss: 0.1899 - accuracy: 0.9750 - val_loss: 0.1855 - val_accuracy: 1.0000\n",
      "Epoch 79/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.1897 - accuracy: 0.9750 - val_loss: 0.1855 - val_accuracy: 1.0000\n",
      "Epoch 80/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.1896 - accuracy: 0.9750 - val_loss: 0.1856 - val_accuracy: 1.0000\n",
      "Epoch 81/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.1894 - accuracy: 0.9750 - val_loss: 0.1856 - val_accuracy: 1.0000\n",
      "Epoch 82/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.1892 - accuracy: 0.9750 - val_loss: 0.1856 - val_accuracy: 1.0000\n",
      "Epoch 83/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.1890 - accuracy: 0.9750 - val_loss: 0.1857 - val_accuracy: 1.0000\n",
      "Epoch 84/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.1888 - accuracy: 0.9750 - val_loss: 0.1858 - val_accuracy: 1.0000\n",
      "Epoch 85/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.1887 - accuracy: 0.9750 - val_loss: 0.1858 - val_accuracy: 1.0000\n",
      "Epoch 86/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.1885 - accuracy: 0.9750 - val_loss: 0.1858 - val_accuracy: 1.0000\n",
      "Epoch 87/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.1883 - accuracy: 0.9750 - val_loss: 0.1858 - val_accuracy: 1.0000\n",
      "Epoch 88/205\n",
      "2/2 [==============================] - 0s 126ms/step - loss: 0.1881 - accuracy: 0.9750 - val_loss: 0.1859 - val_accuracy: 1.0000\n",
      "Epoch 89/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.1879 - accuracy: 0.9750 - val_loss: 0.1860 - val_accuracy: 1.0000\n",
      "Epoch 90/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.1877 - accuracy: 0.9750 - val_loss: 0.1860 - val_accuracy: 1.0000\n",
      "Epoch 91/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.1876 - accuracy: 0.9750 - val_loss: 0.1860 - val_accuracy: 1.0000\n",
      "Epoch 92/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.1874 - accuracy: 0.9750 - val_loss: 0.1860 - val_accuracy: 1.0000\n",
      "Epoch 93/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.1872 - accuracy: 0.9750 - val_loss: 0.1861 - val_accuracy: 1.0000\n",
      "Epoch 94/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.1870 - accuracy: 0.9750 - val_loss: 0.1862 - val_accuracy: 1.0000\n",
      "Epoch 95/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.1869 - accuracy: 0.9750 - val_loss: 0.1862 - val_accuracy: 1.0000\n",
      "Epoch 96/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.1867 - accuracy: 0.9750 - val_loss: 0.1862 - val_accuracy: 1.0000\n",
      "Epoch 97/205\n",
      "2/2 [==============================] - 0s 116ms/step - loss: 0.1865 - accuracy: 0.9750 - val_loss: 0.1862 - val_accuracy: 1.0000\n",
      "Epoch 98/205\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.1863 - accuracy: 0.9750 - val_loss: 0.1863 - val_accuracy: 1.0000\n",
      "Epoch 99/205\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 0.1862 - accuracy: 0.9750 - val_loss: 0.1863 - val_accuracy: 1.0000\n",
      "Epoch 100/205\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.1860 - accuracy: 0.9750 - val_loss: 0.1864 - val_accuracy: 1.0000\n",
      "Epoch 101/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.1858 - accuracy: 0.9750 - val_loss: 0.1864 - val_accuracy: 1.0000\n",
      "Epoch 102/205\n",
      "2/2 [==============================] - 0s 126ms/step - loss: 0.1856 - accuracy: 0.9750 - val_loss: 0.1864 - val_accuracy: 1.0000\n",
      "Epoch 103/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.1855 - accuracy: 0.9750 - val_loss: 0.1865 - val_accuracy: 1.0000\n",
      "Epoch 104/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.1853 - accuracy: 0.9750 - val_loss: 0.1865 - val_accuracy: 1.0000\n",
      "Epoch 105/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.1851 - accuracy: 0.9750 - val_loss: 0.1866 - val_accuracy: 1.0000\n",
      "Epoch 106/205\n",
      "2/2 [==============================] - 0s 124ms/step - loss: 0.1849 - accuracy: 0.9750 - val_loss: 0.1866 - val_accuracy: 1.0000\n",
      "Epoch 107/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.1848 - accuracy: 0.9750 - val_loss: 0.1866 - val_accuracy: 1.0000\n",
      "Epoch 108/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.1846 - accuracy: 0.9750 - val_loss: 0.1867 - val_accuracy: 1.0000\n",
      "Epoch 109/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.1844 - accuracy: 0.9750 - val_loss: 0.1867 - val_accuracy: 1.0000\n",
      "Epoch 110/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.1842 - accuracy: 0.9750 - val_loss: 0.1867 - val_accuracy: 1.0000\n",
      "Epoch 111/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.1841 - accuracy: 0.9750 - val_loss: 0.1868 - val_accuracy: 1.0000\n",
      "Epoch 112/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.1839 - accuracy: 0.9750 - val_loss: 0.1868 - val_accuracy: 1.0000\n",
      "Epoch 113/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.1837 - accuracy: 0.9750 - val_loss: 0.1869 - val_accuracy: 1.0000\n",
      "Epoch 114/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.1836 - accuracy: 0.9750 - val_loss: 0.1869 - val_accuracy: 1.0000\n",
      "Epoch 115/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.1834 - accuracy: 0.9750 - val_loss: 0.1870 - val_accuracy: 1.0000\n",
      "Epoch 116/205\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.1832 - accuracy: 0.9750 - val_loss: 0.1870 - val_accuracy: 1.0000\n",
      "Epoch 117/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.1831 - accuracy: 0.9750 - val_loss: 0.1870 - val_accuracy: 1.0000\n",
      "Epoch 118/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.1829 - accuracy: 0.9750 - val_loss: 0.1871 - val_accuracy: 1.0000\n",
      "Epoch 119/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.1827 - accuracy: 0.9750 - val_loss: 0.1871 - val_accuracy: 1.0000\n",
      "Epoch 120/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.1825 - accuracy: 0.9750 - val_loss: 0.1871 - val_accuracy: 1.0000\n",
      "Epoch 121/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.1824 - accuracy: 0.9750 - val_loss: 0.1871 - val_accuracy: 1.0000\n",
      "Epoch 122/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.1822 - accuracy: 0.9750 - val_loss: 0.1872 - val_accuracy: 1.0000\n",
      "Epoch 123/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.1820 - accuracy: 0.9750 - val_loss: 0.1872 - val_accuracy: 1.0000\n",
      "Epoch 124/205\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.1819 - accuracy: 0.9750 - val_loss: 0.1873 - val_accuracy: 1.0000\n",
      "Epoch 125/205\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.1817 - accuracy: 0.9750 - val_loss: 0.1873 - val_accuracy: 1.0000\n",
      "Epoch 126/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.1815 - accuracy: 0.9750 - val_loss: 0.1874 - val_accuracy: 1.0000\n",
      "Epoch 127/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.1814 - accuracy: 0.9750 - val_loss: 0.1874 - val_accuracy: 1.0000\n",
      "Epoch 128/205\n",
      "2/2 [==============================] - 0s 124ms/step - loss: 0.1812 - accuracy: 0.9750 - val_loss: 0.1874 - val_accuracy: 1.0000\n",
      "Epoch 129/205\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.1810 - accuracy: 0.9750 - val_loss: 0.1874 - val_accuracy: 1.0000\n",
      "Epoch 130/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.1809 - accuracy: 0.9750 - val_loss: 0.1875 - val_accuracy: 1.0000\n",
      "Epoch 131/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.1807 - accuracy: 0.9750 - val_loss: 0.1875 - val_accuracy: 1.0000\n",
      "Epoch 132/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.1806 - accuracy: 0.9750 - val_loss: 0.1875 - val_accuracy: 1.0000\n",
      "Epoch 133/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.1804 - accuracy: 0.9750 - val_loss: 0.1875 - val_accuracy: 1.0000\n",
      "Epoch 134/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.1802 - accuracy: 0.9750 - val_loss: 0.1876 - val_accuracy: 1.0000\n",
      "Epoch 135/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.1801 - accuracy: 0.9750 - val_loss: 0.1876 - val_accuracy: 1.0000\n",
      "Epoch 136/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.1799 - accuracy: 0.9750 - val_loss: 0.1877 - val_accuracy: 1.0000\n",
      "Epoch 137/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.1797 - accuracy: 0.9750 - val_loss: 0.1877 - val_accuracy: 1.0000\n",
      "Epoch 138/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.1796 - accuracy: 0.9750 - val_loss: 0.1878 - val_accuracy: 1.0000\n",
      "Epoch 139/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.1794 - accuracy: 0.9750 - val_loss: 0.1878 - val_accuracy: 1.0000\n",
      "Epoch 140/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.1793 - accuracy: 0.9750 - val_loss: 0.1878 - val_accuracy: 1.0000\n",
      "Epoch 141/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.1791 - accuracy: 0.9750 - val_loss: 0.1878 - val_accuracy: 1.0000\n",
      "Epoch 142/205\n",
      "2/2 [==============================] - 0s 127ms/step - loss: 0.1789 - accuracy: 0.9750 - val_loss: 0.1879 - val_accuracy: 1.0000\n",
      "Epoch 143/205\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.1788 - accuracy: 0.9750 - val_loss: 0.1879 - val_accuracy: 1.0000\n",
      "Epoch 144/205\n",
      "2/2 [==============================] - 0s 106ms/step - loss: 0.1786 - accuracy: 0.9750 - val_loss: 0.1879 - val_accuracy: 1.0000\n",
      "Epoch 145/205\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.1784 - accuracy: 0.9750 - val_loss: 0.1880 - val_accuracy: 1.0000\n",
      "Epoch 146/205\n",
      "2/2 [==============================] - 0s 114ms/step - loss: 0.1783 - accuracy: 0.9750 - val_loss: 0.1880 - val_accuracy: 1.0000\n",
      "Epoch 147/205\n",
      "2/2 [==============================] - 0s 106ms/step - loss: 0.1781 - accuracy: 0.9750 - val_loss: 0.1881 - val_accuracy: 1.0000\n",
      "Epoch 148/205\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 0.1780 - accuracy: 0.9750 - val_loss: 0.1881 - val_accuracy: 1.0000\n",
      "Epoch 149/205\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.1778 - accuracy: 0.9750 - val_loss: 0.1881 - val_accuracy: 1.0000\n",
      "Epoch 150/205\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 0.1776 - accuracy: 0.9750 - val_loss: 0.1882 - val_accuracy: 1.0000\n",
      "Epoch 151/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.1775 - accuracy: 0.9750 - val_loss: 0.1882 - val_accuracy: 1.0000\n",
      "Epoch 152/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.1773 - accuracy: 0.9750 - val_loss: 0.1882 - val_accuracy: 1.0000\n",
      "Epoch 153/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.1772 - accuracy: 0.9750 - val_loss: 0.1883 - val_accuracy: 1.0000\n",
      "Epoch 154/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.1770 - accuracy: 0.9750 - val_loss: 0.1883 - val_accuracy: 1.0000\n",
      "Epoch 155/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.1769 - accuracy: 0.9750 - val_loss: 0.1883 - val_accuracy: 1.0000\n",
      "Epoch 156/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.1767 - accuracy: 0.9750 - val_loss: 0.1884 - val_accuracy: 1.0000\n",
      "Epoch 157/205\n",
      "2/2 [==============================] - 0s 129ms/step - loss: 0.1765 - accuracy: 0.9750 - val_loss: 0.1884 - val_accuracy: 1.0000\n",
      "Epoch 158/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.1764 - accuracy: 0.9750 - val_loss: 0.1884 - val_accuracy: 1.0000\n",
      "Epoch 159/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.1762 - accuracy: 0.9750 - val_loss: 0.1884 - val_accuracy: 1.0000\n",
      "Epoch 160/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.1761 - accuracy: 0.9750 - val_loss: 0.1885 - val_accuracy: 1.0000\n",
      "Epoch 161/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.1759 - accuracy: 0.9750 - val_loss: 0.1885 - val_accuracy: 1.0000\n",
      "Epoch 162/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.1758 - accuracy: 0.9750 - val_loss: 0.1885 - val_accuracy: 1.0000\n",
      "Epoch 163/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.1756 - accuracy: 0.9750 - val_loss: 0.1885 - val_accuracy: 1.0000\n",
      "Epoch 164/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.1754 - accuracy: 0.9750 - val_loss: 0.1886 - val_accuracy: 1.0000\n",
      "Epoch 165/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.1753 - accuracy: 0.9750 - val_loss: 0.1886 - val_accuracy: 1.0000\n",
      "Epoch 166/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.1752 - accuracy: 0.9750 - val_loss: 0.1886 - val_accuracy: 1.0000\n",
      "Epoch 167/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.1750 - accuracy: 0.9750 - val_loss: 0.1887 - val_accuracy: 1.0000\n",
      "Epoch 168/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.1748 - accuracy: 0.9750 - val_loss: 0.1887 - val_accuracy: 1.0000\n",
      "Epoch 169/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.1747 - accuracy: 0.9750 - val_loss: 0.1887 - val_accuracy: 1.0000\n",
      "Epoch 170/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.1745 - accuracy: 0.9750 - val_loss: 0.1888 - val_accuracy: 1.0000\n",
      "Epoch 171/205\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.1744 - accuracy: 0.9750 - val_loss: 0.1888 - val_accuracy: 1.0000\n",
      "Epoch 172/205\n",
      "2/2 [==============================] - 0s 107ms/step - loss: 0.1742 - accuracy: 0.9750 - val_loss: 0.1888 - val_accuracy: 1.0000\n",
      "Epoch 173/205\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 0.1741 - accuracy: 0.9750 - val_loss: 0.1889 - val_accuracy: 1.0000\n",
      "Epoch 174/205\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.1739 - accuracy: 0.9750 - val_loss: 0.1889 - val_accuracy: 1.0000\n",
      "Epoch 175/205\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.1738 - accuracy: 0.9750 - val_loss: 0.1889 - val_accuracy: 1.0000\n",
      "Epoch 176/205\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 0.1736 - accuracy: 0.9750 - val_loss: 0.1890 - val_accuracy: 1.0000\n",
      "Epoch 177/205\n",
      "2/2 [==============================] - 0s 123ms/step - loss: 0.1735 - accuracy: 0.9750 - val_loss: 0.1890 - val_accuracy: 1.0000\n",
      "Epoch 178/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.1733 - accuracy: 0.9750 - val_loss: 0.1890 - val_accuracy: 1.0000\n",
      "Epoch 179/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.1732 - accuracy: 0.9750 - val_loss: 0.1891 - val_accuracy: 1.0000\n",
      "Epoch 180/205\n",
      "2/2 [==============================] - 0s 125ms/step - loss: 0.1730 - accuracy: 0.9750 - val_loss: 0.1891 - val_accuracy: 1.0000\n",
      "Epoch 181/205\n",
      "2/2 [==============================] - 0s 128ms/step - loss: 0.1729 - accuracy: 0.9750 - val_loss: 0.1891 - val_accuracy: 1.0000\n",
      "Epoch 182/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.1727 - accuracy: 0.9750 - val_loss: 0.1892 - val_accuracy: 1.0000\n",
      "Epoch 183/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.1726 - accuracy: 0.9750 - val_loss: 0.1892 - val_accuracy: 1.0000\n",
      "Epoch 184/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.1724 - accuracy: 0.9750 - val_loss: 0.1892 - val_accuracy: 1.0000\n",
      "Epoch 185/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.1723 - accuracy: 0.9750 - val_loss: 0.1892 - val_accuracy: 1.0000\n",
      "Epoch 186/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.1721 - accuracy: 0.9750 - val_loss: 0.1893 - val_accuracy: 1.0000\n",
      "Epoch 187/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.1720 - accuracy: 0.9750 - val_loss: 0.1893 - val_accuracy: 1.0000\n",
      "Epoch 188/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.1718 - accuracy: 0.9750 - val_loss: 0.1893 - val_accuracy: 1.0000\n",
      "Epoch 189/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.1717 - accuracy: 0.9750 - val_loss: 0.1893 - val_accuracy: 1.0000\n",
      "Epoch 190/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.1715 - accuracy: 0.9750 - val_loss: 0.1894 - val_accuracy: 1.0000\n",
      "Epoch 191/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.1714 - accuracy: 0.9750 - val_loss: 0.1895 - val_accuracy: 1.0000\n",
      "Epoch 192/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.1712 - accuracy: 0.9750 - val_loss: 0.1895 - val_accuracy: 1.0000\n",
      "Epoch 193/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.1711 - accuracy: 0.9750 - val_loss: 0.1895 - val_accuracy: 1.0000\n",
      "Epoch 194/205\n",
      "2/2 [==============================] - 0s 125ms/step - loss: 0.1709 - accuracy: 0.9750 - val_loss: 0.1895 - val_accuracy: 1.0000\n",
      "Epoch 195/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.1708 - accuracy: 0.9750 - val_loss: 0.1896 - val_accuracy: 1.0000\n",
      "Epoch 196/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.1706 - accuracy: 0.9750 - val_loss: 0.1896 - val_accuracy: 1.0000\n",
      "Epoch 197/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.1705 - accuracy: 0.9750 - val_loss: 0.1896 - val_accuracy: 1.0000\n",
      "Epoch 198/205\n",
      "2/2 [==============================] - 0s 124ms/step - loss: 0.1703 - accuracy: 0.9750 - val_loss: 0.1896 - val_accuracy: 1.0000\n",
      "Epoch 199/205\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.1702 - accuracy: 0.9750 - val_loss: 0.1896 - val_accuracy: 1.0000\n",
      "Epoch 200/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.1700 - accuracy: 0.9750 - val_loss: 0.1897 - val_accuracy: 1.0000\n",
      "Epoch 201/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.1699 - accuracy: 0.9750 - val_loss: 0.1897 - val_accuracy: 1.0000\n",
      "Epoch 202/205\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.1698 - accuracy: 0.9750 - val_loss: 0.1897 - val_accuracy: 1.0000\n",
      "Epoch 203/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.1696 - accuracy: 0.9750 - val_loss: 0.1897 - val_accuracy: 1.0000\n",
      "Epoch 204/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.1695 - accuracy: 0.9750 - val_loss: 0.1898 - val_accuracy: 1.0000\n",
      "Epoch 205/205\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.1693 - accuracy: 0.9750 - val_loss: 0.1898 - val_accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.4332 - accuracy: 0.8500\n",
      "\n",
      "Fold  5\n",
      "\n",
      "Test accuracy:  0.8500000238418579\n",
      "Test loss:  0.4332309663295746\n",
      "\n",
      "Fold  6\n",
      "\n",
      "Training with 120 samples and validating with 20 samples\n",
      "\n",
      "Epoch 1/205\n",
      "2/2 [==============================] - 0s 129ms/step - loss: 0.1759 - accuracy: 0.9750 - val_loss: 0.1326 - val_accuracy: 1.0000\n",
      "Epoch 2/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.1758 - accuracy: 0.9750 - val_loss: 0.1325 - val_accuracy: 1.0000\n",
      "Epoch 3/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.1756 - accuracy: 0.9750 - val_loss: 0.1324 - val_accuracy: 1.0000\n",
      "Epoch 4/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.1754 - accuracy: 0.9750 - val_loss: 0.1323 - val_accuracy: 1.0000\n",
      "Epoch 5/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.1752 - accuracy: 0.9750 - val_loss: 0.1322 - val_accuracy: 1.0000\n",
      "Epoch 6/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.1750 - accuracy: 0.9750 - val_loss: 0.1322 - val_accuracy: 1.0000\n",
      "Epoch 7/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.1748 - accuracy: 0.9750 - val_loss: 0.1322 - val_accuracy: 1.0000\n",
      "Epoch 8/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.1746 - accuracy: 0.9750 - val_loss: 0.1322 - val_accuracy: 1.0000\n",
      "Epoch 9/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.1744 - accuracy: 0.9750 - val_loss: 0.1322 - val_accuracy: 1.0000\n",
      "Epoch 10/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.1742 - accuracy: 0.9750 - val_loss: 0.1323 - val_accuracy: 1.0000\n",
      "Epoch 11/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.1740 - accuracy: 0.9750 - val_loss: 0.1323 - val_accuracy: 1.0000\n",
      "Epoch 12/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.1738 - accuracy: 0.9750 - val_loss: 0.1324 - val_accuracy: 1.0000\n",
      "Epoch 13/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.1735 - accuracy: 0.9750 - val_loss: 0.1326 - val_accuracy: 1.0000\n",
      "Epoch 14/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.1733 - accuracy: 0.9750 - val_loss: 0.1327 - val_accuracy: 1.0000\n",
      "Epoch 15/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.1731 - accuracy: 0.9750 - val_loss: 0.1329 - val_accuracy: 1.0000\n",
      "Epoch 16/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.1729 - accuracy: 0.9750 - val_loss: 0.1330 - val_accuracy: 1.0000\n",
      "Epoch 17/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.1727 - accuracy: 0.9750 - val_loss: 0.1332 - val_accuracy: 1.0000\n",
      "Epoch 18/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.1725 - accuracy: 0.9750 - val_loss: 0.1334 - val_accuracy: 1.0000\n",
      "Epoch 19/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.1723 - accuracy: 0.9750 - val_loss: 0.1336 - val_accuracy: 1.0000\n",
      "Epoch 20/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.1721 - accuracy: 0.9750 - val_loss: 0.1338 - val_accuracy: 1.0000\n",
      "Epoch 21/205\n",
      "2/2 [==============================] - 0s 123ms/step - loss: 0.1719 - accuracy: 0.9750 - val_loss: 0.1340 - val_accuracy: 1.0000\n",
      "Epoch 22/205\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 0.1717 - accuracy: 0.9750 - val_loss: 0.1343 - val_accuracy: 1.0000\n",
      "Epoch 23/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.1715 - accuracy: 0.9750 - val_loss: 0.1345 - val_accuracy: 1.0000\n",
      "Epoch 24/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.1713 - accuracy: 0.9750 - val_loss: 0.1347 - val_accuracy: 1.0000\n",
      "Epoch 25/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.1711 - accuracy: 0.9750 - val_loss: 0.1349 - val_accuracy: 1.0000\n",
      "Epoch 26/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.1709 - accuracy: 0.9750 - val_loss: 0.1351 - val_accuracy: 1.0000\n",
      "Epoch 27/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.1707 - accuracy: 0.9750 - val_loss: 0.1353 - val_accuracy: 1.0000\n",
      "Epoch 28/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.1705 - accuracy: 0.9750 - val_loss: 0.1355 - val_accuracy: 1.0000\n",
      "Epoch 29/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.1703 - accuracy: 0.9750 - val_loss: 0.1357 - val_accuracy: 1.0000\n",
      "Epoch 30/205\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.1701 - accuracy: 0.9750 - val_loss: 0.1359 - val_accuracy: 1.0000\n",
      "Epoch 31/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.1699 - accuracy: 0.9750 - val_loss: 0.1361 - val_accuracy: 1.0000\n",
      "Epoch 32/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.1697 - accuracy: 0.9750 - val_loss: 0.1363 - val_accuracy: 1.0000\n",
      "Epoch 33/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.1695 - accuracy: 0.9750 - val_loss: 0.1365 - val_accuracy: 1.0000\n",
      "Epoch 34/205\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.1693 - accuracy: 0.9750 - val_loss: 0.1367 - val_accuracy: 1.0000\n",
      "Epoch 35/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.1691 - accuracy: 0.9750 - val_loss: 0.1369 - val_accuracy: 1.0000\n",
      "Epoch 36/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.1689 - accuracy: 0.9750 - val_loss: 0.1370 - val_accuracy: 1.0000\n",
      "Epoch 37/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.1688 - accuracy: 0.9750 - val_loss: 0.1372 - val_accuracy: 1.0000\n",
      "Epoch 38/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.1686 - accuracy: 0.9750 - val_loss: 0.1374 - val_accuracy: 1.0000\n",
      "Epoch 39/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.1684 - accuracy: 0.9750 - val_loss: 0.1375 - val_accuracy: 1.0000\n",
      "Epoch 40/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.1682 - accuracy: 0.9750 - val_loss: 0.1377 - val_accuracy: 1.0000\n",
      "Epoch 41/205\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 0.1680 - accuracy: 0.9750 - val_loss: 0.1378 - val_accuracy: 1.0000\n",
      "Epoch 42/205\n",
      "2/2 [==============================] - 0s 110ms/step - loss: 0.1678 - accuracy: 0.9750 - val_loss: 0.1380 - val_accuracy: 1.0000\n",
      "Epoch 43/205\n",
      "2/2 [==============================] - 0s 112ms/step - loss: 0.1677 - accuracy: 0.9750 - val_loss: 0.1382 - val_accuracy: 1.0000\n",
      "Epoch 44/205\n",
      "2/2 [==============================] - 0s 107ms/step - loss: 0.1675 - accuracy: 0.9750 - val_loss: 0.1383 - val_accuracy: 1.0000\n",
      "Epoch 45/205\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 0.1673 - accuracy: 0.9750 - val_loss: 0.1385 - val_accuracy: 1.0000\n",
      "Epoch 46/205\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 0.1671 - accuracy: 0.9750 - val_loss: 0.1386 - val_accuracy: 1.0000\n",
      "Epoch 47/205\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 0.1669 - accuracy: 0.9750 - val_loss: 0.1388 - val_accuracy: 1.0000\n",
      "Epoch 48/205\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 0.1668 - accuracy: 0.9750 - val_loss: 0.1389 - val_accuracy: 1.0000\n",
      "Epoch 49/205\n",
      "2/2 [==============================] - 0s 105ms/step - loss: 0.1666 - accuracy: 0.9750 - val_loss: 0.1391 - val_accuracy: 1.0000\n",
      "Epoch 50/205\n",
      "2/2 [==============================] - 0s 110ms/step - loss: 0.1664 - accuracy: 0.9750 - val_loss: 0.1392 - val_accuracy: 1.0000\n",
      "Epoch 51/205\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 0.1662 - accuracy: 0.9750 - val_loss: 0.1393 - val_accuracy: 1.0000\n",
      "Epoch 52/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.1661 - accuracy: 0.9750 - val_loss: 0.1395 - val_accuracy: 1.0000\n",
      "Epoch 53/205\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 0.1659 - accuracy: 0.9750 - val_loss: 0.1396 - val_accuracy: 1.0000\n",
      "Epoch 54/205\n",
      "2/2 [==============================] - 0s 127ms/step - loss: 0.1657 - accuracy: 0.9750 - val_loss: 0.1398 - val_accuracy: 1.0000\n",
      "Epoch 55/205\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 0.1656 - accuracy: 0.9750 - val_loss: 0.1399 - val_accuracy: 1.0000\n",
      "Epoch 56/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.1654 - accuracy: 0.9750 - val_loss: 0.1401 - val_accuracy: 1.0000\n",
      "Epoch 57/205\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 0.1652 - accuracy: 0.9750 - val_loss: 0.1402 - val_accuracy: 1.0000\n",
      "Epoch 58/205\n",
      "2/2 [==============================] - 0s 128ms/step - loss: 0.1650 - accuracy: 0.9750 - val_loss: 0.1403 - val_accuracy: 1.0000\n",
      "Epoch 59/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.1649 - accuracy: 0.9750 - val_loss: 0.1405 - val_accuracy: 1.0000\n",
      "Epoch 60/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.1647 - accuracy: 0.9750 - val_loss: 0.1406 - val_accuracy: 1.0000\n",
      "Epoch 61/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.1645 - accuracy: 0.9750 - val_loss: 0.1407 - val_accuracy: 1.0000\n",
      "Epoch 62/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.1644 - accuracy: 0.9750 - val_loss: 0.1409 - val_accuracy: 1.0000\n",
      "Epoch 63/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.1642 - accuracy: 0.9750 - val_loss: 0.1410 - val_accuracy: 1.0000\n",
      "Epoch 64/205\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 0.1640 - accuracy: 0.9750 - val_loss: 0.1411 - val_accuracy: 1.0000\n",
      "Epoch 65/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.1639 - accuracy: 0.9750 - val_loss: 0.1413 - val_accuracy: 1.0000\n",
      "Epoch 66/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.1637 - accuracy: 0.9750 - val_loss: 0.1414 - val_accuracy: 1.0000\n",
      "Epoch 67/205\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.1636 - accuracy: 0.9750 - val_loss: 0.1415 - val_accuracy: 1.0000\n",
      "Epoch 68/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.1634 - accuracy: 0.9750 - val_loss: 0.1416 - val_accuracy: 1.0000\n",
      "Epoch 69/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.1632 - accuracy: 0.9750 - val_loss: 0.1417 - val_accuracy: 1.0000\n",
      "Epoch 70/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.1631 - accuracy: 0.9750 - val_loss: 0.1419 - val_accuracy: 1.0000\n",
      "Epoch 71/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.1629 - accuracy: 0.9750 - val_loss: 0.1420 - val_accuracy: 1.0000\n",
      "Epoch 72/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.1627 - accuracy: 0.9750 - val_loss: 0.1421 - val_accuracy: 1.0000\n",
      "Epoch 73/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.1626 - accuracy: 0.9750 - val_loss: 0.1422 - val_accuracy: 1.0000\n",
      "Epoch 74/205\n",
      "2/2 [==============================] - 0s 125ms/step - loss: 0.1624 - accuracy: 0.9750 - val_loss: 0.1423 - val_accuracy: 1.0000\n",
      "Epoch 75/205\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 0.1623 - accuracy: 0.9750 - val_loss: 0.1425 - val_accuracy: 1.0000\n",
      "Epoch 76/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.1621 - accuracy: 0.9750 - val_loss: 0.1426 - val_accuracy: 1.0000\n",
      "Epoch 77/205\n",
      "2/2 [==============================] - 0s 128ms/step - loss: 0.1620 - accuracy: 0.9750 - val_loss: 0.1427 - val_accuracy: 1.0000\n",
      "Epoch 78/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.1618 - accuracy: 0.9750 - val_loss: 0.1428 - val_accuracy: 1.0000\n",
      "Epoch 79/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.1616 - accuracy: 0.9750 - val_loss: 0.1429 - val_accuracy: 1.0000\n",
      "Epoch 80/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.1615 - accuracy: 0.9750 - val_loss: 0.1430 - val_accuracy: 1.0000\n",
      "Epoch 81/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.1613 - accuracy: 0.9750 - val_loss: 0.1432 - val_accuracy: 1.0000\n",
      "Epoch 82/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.1612 - accuracy: 0.9750 - val_loss: 0.1433 - val_accuracy: 1.0000\n",
      "Epoch 83/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.1610 - accuracy: 0.9750 - val_loss: 0.1434 - val_accuracy: 1.0000\n",
      "Epoch 84/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.1609 - accuracy: 0.9750 - val_loss: 0.1435 - val_accuracy: 1.0000\n",
      "Epoch 85/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.1607 - accuracy: 0.9750 - val_loss: 0.1436 - val_accuracy: 1.0000\n",
      "Epoch 86/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.1606 - accuracy: 0.9750 - val_loss: 0.1437 - val_accuracy: 1.0000\n",
      "Epoch 87/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.1604 - accuracy: 0.9750 - val_loss: 0.1438 - val_accuracy: 1.0000\n",
      "Epoch 88/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.1602 - accuracy: 0.9750 - val_loss: 0.1439 - val_accuracy: 1.0000\n",
      "Epoch 89/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.1601 - accuracy: 0.9750 - val_loss: 0.1440 - val_accuracy: 1.0000\n",
      "Epoch 90/205\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.1599 - accuracy: 0.9750 - val_loss: 0.1442 - val_accuracy: 1.0000\n",
      "Epoch 91/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.1598 - accuracy: 0.9750 - val_loss: 0.1443 - val_accuracy: 1.0000\n",
      "Epoch 92/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.1596 - accuracy: 0.9750 - val_loss: 0.1444 - val_accuracy: 1.0000\n",
      "Epoch 93/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.1595 - accuracy: 0.9750 - val_loss: 0.1445 - val_accuracy: 1.0000\n",
      "Epoch 94/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.1593 - accuracy: 0.9750 - val_loss: 0.1446 - val_accuracy: 1.0000\n",
      "Epoch 95/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.1592 - accuracy: 0.9750 - val_loss: 0.1447 - val_accuracy: 1.0000\n",
      "Epoch 96/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.1590 - accuracy: 0.9750 - val_loss: 0.1448 - val_accuracy: 1.0000\n",
      "Epoch 97/205\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 0.1589 - accuracy: 0.9750 - val_loss: 0.1449 - val_accuracy: 1.0000\n",
      "Epoch 98/205\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 0.1587 - accuracy: 0.9750 - val_loss: 0.1450 - val_accuracy: 1.0000\n",
      "Epoch 99/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.1586 - accuracy: 0.9750 - val_loss: 0.1451 - val_accuracy: 1.0000\n",
      "Epoch 100/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.1584 - accuracy: 0.9750 - val_loss: 0.1452 - val_accuracy: 1.0000\n",
      "Epoch 101/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.1583 - accuracy: 0.9750 - val_loss: 0.1453 - val_accuracy: 1.0000\n",
      "Epoch 102/205\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 0.1581 - accuracy: 0.9750 - val_loss: 0.1454 - val_accuracy: 1.0000\n",
      "Epoch 103/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.1580 - accuracy: 0.9750 - val_loss: 0.1455 - val_accuracy: 1.0000\n",
      "Epoch 104/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.1578 - accuracy: 0.9750 - val_loss: 0.1456 - val_accuracy: 1.0000\n",
      "Epoch 105/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.1577 - accuracy: 0.9750 - val_loss: 0.1457 - val_accuracy: 1.0000\n",
      "Epoch 106/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.1576 - accuracy: 0.9750 - val_loss: 0.1458 - val_accuracy: 1.0000\n",
      "Epoch 107/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.1574 - accuracy: 0.9750 - val_loss: 0.1458 - val_accuracy: 1.0000\n",
      "Epoch 108/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.1573 - accuracy: 0.9750 - val_loss: 0.1459 - val_accuracy: 1.0000\n",
      "Epoch 109/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.1571 - accuracy: 0.9750 - val_loss: 0.1460 - val_accuracy: 1.0000\n",
      "Epoch 110/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.1570 - accuracy: 0.9750 - val_loss: 0.1461 - val_accuracy: 1.0000\n",
      "Epoch 111/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.1568 - accuracy: 0.9750 - val_loss: 0.1462 - val_accuracy: 1.0000\n",
      "Epoch 112/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.1567 - accuracy: 0.9750 - val_loss: 0.1463 - val_accuracy: 1.0000\n",
      "Epoch 113/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.1565 - accuracy: 0.9750 - val_loss: 0.1464 - val_accuracy: 1.0000\n",
      "Epoch 114/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.1564 - accuracy: 0.9750 - val_loss: 0.1465 - val_accuracy: 1.0000\n",
      "Epoch 115/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.1563 - accuracy: 0.9750 - val_loss: 0.1466 - val_accuracy: 1.0000\n",
      "Epoch 116/205\n",
      "2/2 [==============================] - 0s 108ms/step - loss: 0.1561 - accuracy: 0.9750 - val_loss: 0.1467 - val_accuracy: 1.0000\n",
      "Epoch 117/205\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.1560 - accuracy: 0.9750 - val_loss: 0.1468 - val_accuracy: 1.0000\n",
      "Epoch 118/205\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.1558 - accuracy: 0.9750 - val_loss: 0.1468 - val_accuracy: 1.0000\n",
      "Epoch 119/205\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.1557 - accuracy: 0.9750 - val_loss: 0.1469 - val_accuracy: 1.0000\n",
      "Epoch 120/205\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 0.1555 - accuracy: 0.9750 - val_loss: 0.1470 - val_accuracy: 1.0000\n",
      "Epoch 121/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.1554 - accuracy: 0.9750 - val_loss: 0.1471 - val_accuracy: 1.0000\n",
      "Epoch 122/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.1553 - accuracy: 0.9750 - val_loss: 0.1472 - val_accuracy: 1.0000\n",
      "Epoch 123/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.1551 - accuracy: 0.9750 - val_loss: 0.1473 - val_accuracy: 1.0000\n",
      "Epoch 124/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.1550 - accuracy: 0.9750 - val_loss: 0.1474 - val_accuracy: 1.0000\n",
      "Epoch 125/205\n",
      "2/2 [==============================] - 0s 126ms/step - loss: 0.1548 - accuracy: 0.9750 - val_loss: 0.1475 - val_accuracy: 1.0000\n",
      "Epoch 126/205\n",
      "2/2 [==============================] - 0s 126ms/step - loss: 0.1547 - accuracy: 0.9750 - val_loss: 0.1475 - val_accuracy: 1.0000\n",
      "Epoch 127/205\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.1546 - accuracy: 0.9750 - val_loss: 0.1476 - val_accuracy: 1.0000\n",
      "Epoch 128/205\n",
      "2/2 [==============================] - 0s 126ms/step - loss: 0.1544 - accuracy: 0.9750 - val_loss: 0.1477 - val_accuracy: 1.0000\n",
      "Epoch 129/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.1543 - accuracy: 0.9750 - val_loss: 0.1478 - val_accuracy: 1.0000\n",
      "Epoch 130/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.1541 - accuracy: 0.9750 - val_loss: 0.1479 - val_accuracy: 1.0000\n",
      "Epoch 131/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.1540 - accuracy: 0.9750 - val_loss: 0.1480 - val_accuracy: 1.0000\n",
      "Epoch 132/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.1539 - accuracy: 0.9750 - val_loss: 0.1480 - val_accuracy: 1.0000\n",
      "Epoch 133/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.1537 - accuracy: 0.9750 - val_loss: 0.1481 - val_accuracy: 1.0000\n",
      "Epoch 134/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.1536 - accuracy: 0.9750 - val_loss: 0.1482 - val_accuracy: 1.0000\n",
      "Epoch 135/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.1534 - accuracy: 0.9750 - val_loss: 0.1483 - val_accuracy: 1.0000\n",
      "Epoch 136/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.1533 - accuracy: 0.9750 - val_loss: 0.1484 - val_accuracy: 1.0000\n",
      "Epoch 137/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.1532 - accuracy: 0.9750 - val_loss: 0.1485 - val_accuracy: 1.0000\n",
      "Epoch 138/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.1530 - accuracy: 0.9750 - val_loss: 0.1486 - val_accuracy: 1.0000\n",
      "Epoch 139/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.1529 - accuracy: 0.9750 - val_loss: 0.1486 - val_accuracy: 1.0000\n",
      "Epoch 140/205\n",
      "2/2 [==============================] - 0s 125ms/step - loss: 0.1528 - accuracy: 0.9750 - val_loss: 0.1487 - val_accuracy: 1.0000\n",
      "Epoch 141/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.1526 - accuracy: 0.9750 - val_loss: 0.1488 - val_accuracy: 1.0000\n",
      "Epoch 142/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.1525 - accuracy: 0.9750 - val_loss: 0.1489 - val_accuracy: 1.0000\n",
      "Epoch 143/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.1524 - accuracy: 0.9750 - val_loss: 0.1490 - val_accuracy: 1.0000\n",
      "Epoch 144/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.1522 - accuracy: 0.9750 - val_loss: 0.1490 - val_accuracy: 1.0000\n",
      "Epoch 145/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.1521 - accuracy: 0.9750 - val_loss: 0.1491 - val_accuracy: 1.0000\n",
      "Epoch 146/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.1519 - accuracy: 0.9750 - val_loss: 0.1492 - val_accuracy: 1.0000\n",
      "Epoch 147/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.1518 - accuracy: 0.9750 - val_loss: 0.1493 - val_accuracy: 1.0000\n",
      "Epoch 148/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.1517 - accuracy: 0.9750 - val_loss: 0.1493 - val_accuracy: 1.0000\n",
      "Epoch 149/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.1516 - accuracy: 0.9750 - val_loss: 0.1494 - val_accuracy: 1.0000\n",
      "Epoch 150/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.1514 - accuracy: 0.9750 - val_loss: 0.1495 - val_accuracy: 1.0000\n",
      "Epoch 151/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.1513 - accuracy: 0.9750 - val_loss: 0.1496 - val_accuracy: 1.0000\n",
      "Epoch 152/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.1512 - accuracy: 0.9750 - val_loss: 0.1497 - val_accuracy: 1.0000\n",
      "Epoch 153/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.1510 - accuracy: 0.9750 - val_loss: 0.1498 - val_accuracy: 1.0000\n",
      "Epoch 154/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.1509 - accuracy: 0.9750 - val_loss: 0.1498 - val_accuracy: 1.0000\n",
      "Epoch 155/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.1508 - accuracy: 0.9750 - val_loss: 0.1499 - val_accuracy: 1.0000\n",
      "Epoch 156/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.1506 - accuracy: 0.9750 - val_loss: 0.1500 - val_accuracy: 1.0000\n",
      "Epoch 157/205\n",
      "2/2 [==============================] - 0s 125ms/step - loss: 0.1505 - accuracy: 0.9750 - val_loss: 0.1501 - val_accuracy: 1.0000\n",
      "Epoch 158/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.1504 - accuracy: 0.9750 - val_loss: 0.1501 - val_accuracy: 1.0000\n",
      "Epoch 159/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.1502 - accuracy: 0.9750 - val_loss: 0.1502 - val_accuracy: 1.0000\n",
      "Epoch 160/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.1501 - accuracy: 0.9750 - val_loss: 0.1503 - val_accuracy: 1.0000\n",
      "Epoch 161/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.1500 - accuracy: 0.9750 - val_loss: 0.1504 - val_accuracy: 1.0000\n",
      "Epoch 162/205\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.1498 - accuracy: 0.9750 - val_loss: 0.1504 - val_accuracy: 1.0000\n",
      "Epoch 163/205\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 0.1497 - accuracy: 0.9750 - val_loss: 0.1505 - val_accuracy: 1.0000\n",
      "Epoch 164/205\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.1496 - accuracy: 0.9750 - val_loss: 0.1506 - val_accuracy: 1.0000\n",
      "Epoch 165/205\n",
      "2/2 [==============================] - 0s 105ms/step - loss: 0.1494 - accuracy: 0.9750 - val_loss: 0.1507 - val_accuracy: 1.0000\n",
      "Epoch 166/205\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.1493 - accuracy: 0.9750 - val_loss: 0.1507 - val_accuracy: 1.0000\n",
      "Epoch 167/205\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 0.1492 - accuracy: 0.9750 - val_loss: 0.1508 - val_accuracy: 1.0000\n",
      "Epoch 168/205\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.1491 - accuracy: 0.9750 - val_loss: 0.1509 - val_accuracy: 1.0000\n",
      "Epoch 169/205\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.1489 - accuracy: 0.9750 - val_loss: 0.1510 - val_accuracy: 1.0000\n",
      "Epoch 170/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.1488 - accuracy: 0.9750 - val_loss: 0.1510 - val_accuracy: 1.0000\n",
      "Epoch 171/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.1487 - accuracy: 0.9750 - val_loss: 0.1511 - val_accuracy: 1.0000\n",
      "Epoch 172/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.1485 - accuracy: 0.9750 - val_loss: 0.1512 - val_accuracy: 1.0000\n",
      "Epoch 173/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.1484 - accuracy: 0.9750 - val_loss: 0.1512 - val_accuracy: 1.0000\n",
      "Epoch 174/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.1483 - accuracy: 0.9750 - val_loss: 0.1513 - val_accuracy: 1.0000\n",
      "Epoch 175/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.1482 - accuracy: 0.9750 - val_loss: 0.1514 - val_accuracy: 1.0000\n",
      "Epoch 176/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.1480 - accuracy: 0.9750 - val_loss: 0.1515 - val_accuracy: 1.0000\n",
      "Epoch 177/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.1479 - accuracy: 0.9750 - val_loss: 0.1515 - val_accuracy: 1.0000\n",
      "Epoch 178/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.1478 - accuracy: 0.9750 - val_loss: 0.1516 - val_accuracy: 1.0000\n",
      "Epoch 179/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.1477 - accuracy: 0.9750 - val_loss: 0.1517 - val_accuracy: 1.0000\n",
      "Epoch 180/205\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.1475 - accuracy: 0.9750 - val_loss: 0.1518 - val_accuracy: 1.0000\n",
      "Epoch 181/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.1474 - accuracy: 0.9750 - val_loss: 0.1518 - val_accuracy: 1.0000\n",
      "Epoch 182/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.1473 - accuracy: 0.9750 - val_loss: 0.1519 - val_accuracy: 1.0000\n",
      "Epoch 183/205\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.1472 - accuracy: 0.9750 - val_loss: 0.1520 - val_accuracy: 1.0000\n",
      "Epoch 184/205\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.1470 - accuracy: 0.9750 - val_loss: 0.1520 - val_accuracy: 1.0000\n",
      "Epoch 185/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.1469 - accuracy: 0.9750 - val_loss: 0.1521 - val_accuracy: 1.0000\n",
      "Epoch 186/205\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.1468 - accuracy: 0.9750 - val_loss: 0.1522 - val_accuracy: 1.0000\n",
      "Epoch 187/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.1467 - accuracy: 0.9750 - val_loss: 0.1523 - val_accuracy: 1.0000\n",
      "Epoch 188/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.1465 - accuracy: 0.9750 - val_loss: 0.1523 - val_accuracy: 1.0000\n",
      "Epoch 189/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.1464 - accuracy: 0.9750 - val_loss: 0.1524 - val_accuracy: 1.0000\n",
      "Epoch 190/205\n",
      "2/2 [==============================] - 0s 126ms/step - loss: 0.1463 - accuracy: 0.9750 - val_loss: 0.1525 - val_accuracy: 1.0000\n",
      "Epoch 191/205\n",
      "2/2 [==============================] - 0s 111ms/step - loss: 0.1462 - accuracy: 0.9750 - val_loss: 0.1525 - val_accuracy: 1.0000\n",
      "Epoch 192/205\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.1460 - accuracy: 0.9750 - val_loss: 0.1526 - val_accuracy: 1.0000\n",
      "Epoch 193/205\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 0.1459 - accuracy: 0.9750 - val_loss: 0.1527 - val_accuracy: 1.0000\n",
      "Epoch 194/205\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 0.1458 - accuracy: 0.9750 - val_loss: 0.1528 - val_accuracy: 0.9500\n",
      "Epoch 195/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.1457 - accuracy: 0.9750 - val_loss: 0.1528 - val_accuracy: 0.9500\n",
      "Epoch 196/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.1455 - accuracy: 0.9750 - val_loss: 0.1529 - val_accuracy: 0.9500\n",
      "Epoch 197/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.1454 - accuracy: 0.9750 - val_loss: 0.1530 - val_accuracy: 0.9500\n",
      "Epoch 198/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.1453 - accuracy: 0.9750 - val_loss: 0.1530 - val_accuracy: 0.9500\n",
      "Epoch 199/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.1452 - accuracy: 0.9750 - val_loss: 0.1531 - val_accuracy: 0.9500\n",
      "Epoch 200/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.1451 - accuracy: 0.9750 - val_loss: 0.1532 - val_accuracy: 0.9500\n",
      "Epoch 201/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.1449 - accuracy: 0.9750 - val_loss: 0.1532 - val_accuracy: 0.9500\n",
      "Epoch 202/205\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.1448 - accuracy: 0.9750 - val_loss: 0.1533 - val_accuracy: 0.9500\n",
      "Epoch 203/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.1447 - accuracy: 0.9750 - val_loss: 0.1534 - val_accuracy: 0.9500\n",
      "Epoch 204/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.1446 - accuracy: 0.9750 - val_loss: 0.1534 - val_accuracy: 0.9500\n",
      "Epoch 205/205\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.1444 - accuracy: 0.9750 - val_loss: 0.1535 - val_accuracy: 0.9500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.4067 - accuracy: 0.9500\n",
      "\n",
      "Fold  6\n",
      "\n",
      "Test accuracy:  0.949999988079071\n",
      "Test loss:  0.4067228436470032\n",
      "\n",
      "K-Fold accuracy:  [0.800000011920929, 0.800000011920929, 0.800000011920929, 0.8500000238418579, 0.8999999761581421, 0.8500000238418579, 0.949999988079071]\n",
      "\n",
      "Average accuracy:  0.8500000068119594\n",
      "K-Fold Standard deviation:  0.05345223723147834\n",
      "\n",
      "Classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       sober       1.00      0.80      0.89         5\n",
      "       drunk       0.94      1.00      0.97        15\n",
      "\n",
      "    accuracy                           0.95        20\n",
      "   macro avg       0.97      0.90      0.93        20\n",
      "weighted avg       0.95      0.95      0.95        20\n",
      "\n",
      "\n",
      "AUC ROC: \n",
      "0.8666666666666667\n",
      "\n",
      "Confusion Matrix: \n",
      "[[ 4  1]\n",
      " [ 0 15]]\n",
      "\n",
      "True Negatives:  4\n",
      "False Positives:  1\n",
      "False Negatives:  0\n",
      "True Positives:  15\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAA6k0lEQVR4nO3de5yUdfn4/9c1sydYTrssKLDAgqKCiqKkomaYxzyRaSZpSVamn+ykZVrmR61+3+xjpZaVZmamSWSp5LGDouYRVEQBEUSQ5SSwwHLY48z1++N+z+7s7Jz2cM/Mcl/Px2MfO3Pf99xzzb0793W/D/f7LaqKMcaY4ArlOwBjjDH5ZYnAGGMCzhKBMcYEnCUCY4wJOEsExhgTcJYIjDEm4CwR9FEiUiMiKiJFWWw7S0T+m4u4Cp2IPCAin8x3HL1JRK4Xkfvc4zEislNEwpm27eZ7LRaR6d19fZr9zhORL6VYl/Yz5YqITBaRF/MZg18sEeSAiKwSkWYRqUpY/oY7mdfkKbRAEZHJwCHAI/mOxS+q+oGqDlDVSE/3JSL3iMiPEvZ/oKrO6+m+uyLbz+T3BY+qLgK2iciZfr1HvlgiyJ33gZmxJyJyMNA/f+EUhmxKNL3oK8D92sW7KHMco8mTLP/O9+P9H+1RLBHkzp+Az8c9vwi4N34DERksIveKyCYRWS0i14pIyK0Li8jNIrJZRFYCpyd57e9FZL2IrBWRH2VblBaRv4rIBhHZLiLPiciBcev6icjPXDzbReS/ItLPrTtWRF4UkW0iskZEZrnlHYr5iVdqrhT0VRFZDix3y251+6gXkddE5KNx24dF5Hsi8p6I7HDrR4vI7SLys4TPMldEvpXio34CeDZh+y+LyFK33yUicphbvkpEvisii4BdIlIkIme5qpFt7jNOjNvPd91x3yEiy0TkBLf8CBFZ4D7XRhH5eYq/wRMicnnCsjdF5FOZjk/CazpUGYrIOBF51sX1LyCxVJr0by8ilwAXAFeJVy3zj7jjcqJ7XCoit4jIOvdzi4iUunXTRaRWRK4UkQ/d/+UXUvxdYsaKyAsu1n+KK0En+UyzRGSl2+59EbnA/S1+C0xz8W5z26b7Ts1y7/cLEdkC3CgideJdpMWOz3AR2S0iw9yiecAJsc+5x1BV+/H5B1gFnAgsAyYCYaAWGAsoUOO2uxev2mIgUAO8C3zRrbsUeAcYDVQCz7jXFrn1DwF3AOXAcOBV4Ctu3Szgv2niu9i9ZylwC7Awbt3teP/8o1zcR7vtxgI78Eo5xcBQ4FD3mnnAl+L20eH9Xdz/cp+jn1t2odtHEXAlsAEoc+u+A7wF7A8IXvXOUOAIYB0QcttVAbuBvZJ8xnL3vsPiln0aWAt8xO13X2Bs3N9soTve/YD9gF3ASe7zXgWsAEpcXGuAke61NcA+7vFLwOfc4wHAUSn+Bp8HXoh7PgnYBpRmcXyuB+6Le+/4/4uXgJ+7v9lx7m92X5Z/+3uAHyX7X3aPbwRexvt/Gwa8CPzQrZsOtLptioHT3N+mIsXnnwe8545zP/f8J4mfyf0d64H93boRwIGp/s9J/52a5WL8mtt3P+DXwE1xr/8G8I+EfdYDk/N9XunVc1S+AwjCD+2J4Frg/wGn4p0Ii9w/eA3eSbYZmBT3uq8A89zjp4FL49adHPfl2Atowp1U3fqZwDPucacvSJpYh7j9DsYrMTYAhyTZ7hrgoRT7mEfmRPDxDHFsjb0vXgKdkWK7pcBJ7vHlwOMpthvl3rcsbtlTwDfS/M0ujnv+A2BO3PMQXhKZjpdAPnR/4+KE/TwH3ABUZfi8A/ESzVj3/MfA3Vken+tJkgiAMe5EVx73uj8TlwhS/e3d83tInwjeA06LW3cKsMo9nu7+d4ri1n9I6kQ4D7g27vn/AE8m+UzleAnyHOL+31P8n2X6Ts0CPkjYx5HAB4C45wuA8xK2WQscl833qa/8WNVQbv0J+CzeP+C9Ceuq8K6cVsctW413AgMYiXfVGb8uZqx77XpXbbENr3QwPFNArtrlJ67apR7vix6Lpwoow/vCJxqdYnm24j8LIvJtV0Wz3cU/mPZqjHTv9Ue8q2Xc7z+l2G6b+z0wblmmzxAf40jijrmqRt36Uaq6Avgm3gn5QxGZLSIj3aZfxLvKfUdE5ovIGQAi8ltXhbFTRL6nqjuAx4Dz3etm4tVH47ZPd3xSGQlsVdVdccvaPkOGv302OhwT93hk3PMtqtoa93w3XqkolQ2ZtnWf5TN4JeT1IvKYiByQYn+ZvlOQ8H+oqq+4957u9rsvMDdhvwNp/3/aI1giyCFVXY3XaHwa8PeE1ZuBFryTeswYvKsPgPV4J674dTFr8EoEVao6xP0MUtUDyeyzwAy8q9nBeFdf4FWVbAYagX2SvG5NiuXgXdnGN4TvnWSbtgZbV999FXAeXtXBEGC7iyHTe90HzBCRQ/Cq3R5OtpE7gcSqHrL5DB1ixKuCavvbiIjg/T3Wuv3/WVWPpb267ya3fLmqzsRLyjcBD4pIuapeql5PmAGq+v+53T4AzBSRaXgJ+Bn3XpmOTyrrgQoRKY9bFv9/k+5vn/j5k+lwTNy+12V4TY+p6lOqehJetdA7wO9iqxI2zfSdSvYaaL+4+BzwoKo2xlaIyCi86sBlPfkMhcYSQe59Ea9aJP4qDfW6xs0BfiwiA0VkLHAF3okOt+7rIlItIhXA1XGvXQ/8E/iZiAwSkZCI7CMiH8sinoF4SWQL3sk7dlKKXfXeDfxcREa6K8hprqHsfuBEETlPvIbUoSJyqHvpQuBTItJfRPZ1nzlTDK3AJqBIRK4DBsWtvwv4oYhMEM9kERnqYqwF5uOVBP6mqg1p3udxIP6Y3AV8W0QOd/vd1x33ZOYAp4vICSJSjFdP3wS8KCL7i8jH3XFpxKsSiQKIyIUiMswdy21uX9E08Y3Fq1f/i3tNNscnKXfhsQC4QURKRORYIL7rY8q/vbMRGJ/mLR4ArhWRYa5h9zra/199ISJ7icgMl9yagJ20H8+NQLWIlEBW36lU7gPOxksGiSX3jwFPq2pTr3ygAmGJIMdU9T1VXZBi9dfwrqZXAv/Fq8+92637HV6d9pvA63QuUXwe70plCV798YN4V0yZ3ItXXF7rXvtywvpv4zXUzgfq8K5qQ6r6AV7J5kq3fCFeIy7AL/DqZjfiXV3dT3pPAU/iNeStxjuZxhfZf473hf4nXkPd7/Ea9mL+CBxM6mqhmDuBC9zVPKr6V7y6+D/jNaI+jNeA3YmqLsM7MfwS70rzTOBMVW3Ga2j9iVu+Ae/q/xr30lOBxSKyE7gVOD9VsnInl7/jXaH/OW5VpuOTzmfx6r3rgP+l44kt09/+98AkV934cJJ9/wgv0SzC+x953S3zUwjvZL4O7zN9DLjMrXsaWAxsEJHNblm671RSqroG77Mo8HzC6gvweiftUWINIsb0WSJyHN5V3FjN8A8tIn/Ga/R9OBexmb5JRO4G1qnqtXHLJgN3qOq0/EXmD0sEpk9z1TSzgTdV9cZ8x2P6PvHu9F8ITFHV9/MbTW5Y1ZDps9xNRNvwqsBuyWswZo8gIj8E3gb+LyhJAKxEYIwxgWclAmOMCbg+N5hWVVWV1tTU5DsMY4zpU1577bXNqjos2bo+lwhqampYsCBV70tjjDHJiMjqVOusasgYYwLOEoExxgScJQJjjAm4PtdGYIwxXdXS0kJtbS2NjY2ZN+7jysrKqK6upri4OOvXWCIwxuzxamtrGThwIDU1NbihpvZIqsqWLVuora1l3LhxWb/Ot6ohEblbvCnq3k6xXkTkNhFZISKLxE0RaIwxva2xsZGhQ4fu0UkAQEQYOnRol0s+frYR3IM38mIqnwAmuJ9LgN/4GIsxJuD29CQQ053P6VvVkKo+5wZvSmUGcK8bLfJlERkiIiPc2PrGmL7oncdg3cJ8R9FZxQlQ30dPLSXlUJZx+okeyWcbwSg6jqle65Z1+muJyCV4pQbGjBmTuNoYUygeuxJ2rCfz5Gk5dspU2Lkh83Y+2VK3jRM+cykAGzZtIRwOMayyAoBXH/sTJSWpG3YXvLWcex/7L7fddptv8fWJxmJVvRNvUhGmTp1qo+QZU6giLTD1YjjjF/mOpKOlS2HkxLy9/dCRsHCxN7vl9ddfz4ABA/j2t7/dtr61tZWioiSn462rmXrogUw95Txf48vnfQRr6TgHbzUd5xI1xvQ5CmK3J2Vj1qxZXHrppRx55JFcddVVvPrqq0ybNo0pU6Zw9NFHs2zZMhCY98KrnHHGGYCXRC6++GKmT5/O+PHje62UkM8SwVzgchGZjTeV3nZrHzCmj9MoBVctlOCGfyxmybr6Xt3npJGD+N8zD+zy62pra3nxxRcJh8PU19fz/PPPU1RUxL///W++973v8bff/wJvxsx277zzDs888ww7duxg//3357LLLuvSPQPJ+JYIROQBYDpQJSK1ePOlFgOo6m/xJuo+DVgB7Aa+4Fcsxhh/tUa8+ePDGkWsRJC1T3/604TDYQC2b9/ORRddxPLlyxERWlpaSJZUTz/9dEpLSyktLWX48OFs3LiR6urqHsXhZ6+hmRnWK/BVv97fGJMbP33yHX497z0AFpW2sLO+iZF5jimd7ly5+6W8vLzt8Q9+8AOOP/54HnroIVatWsX06dO9PJAweVhpaWnb43A4TGtra4/jsNRtjOmRt9ZuZ9SQflw2fR8EZXtDz09MQbR9+3ZGjRoFwD333OOW5qaarU/0GjLG9FxDc4TXP9hKtJenp127tYH99x7IV4/fF3kpSku06/tQVV7/YBu7m/1JIgNaIuxobPFl313V1BqhuCVCSyRKQ3NrW1xf/cYVXPrli7nxhz/k5FM/gSo0RxRVpTWq7GhsIRL1p9Nkn5uzeOrUqWoT0xjTdbf+ezm/+Pe7vuz7wqPGcP2ZB9J8494srT6Pw7/8qy69/rXVdZzzm5d8iQ3gd2eNYK8x433bv1/2ljqqqOdtrQFg1JB+DB1Qmv5FwNKlS5k4sWN3WRF5TVWnJtveSgTGBMT67Q1Ulpdw5+cO7/V9Txo5iKJwiFaitHTjqnXdNm9snFvPP5RRQ/r1dngU7VjPPsMG9Pp+/Va8eyfSAPtUebGXFPlTm2+JwJg9UGskyl3/fZ/6hvbqkPmr6hg+sJSpNZW+vW9IlNptjfz0yXe69LrFrjvnMftWUZXFFW9XLV26kfLSPni6awoB6nvsffDIGGMyeWvtdn7yxDuEQ0Iorr3x01NHp35RLwgBm3a28LvnV3b5tfsMK6eif0nvB9WnuT+eKvg4aJ4lAmP2QHW7mgH422VHc+joITl73yJRLpu+L5edcFrO3nOPlqMRUy0RGJNnv/jXuzy3fFOv7nOrSwSVub7C1qgNMeEHKxEYs2ebs8AbhHff4b3XmDmgtIipNZWMquj9htf0bKyh3hU7+fvbu9MSgekdrU3e1WCoCMI9G/ekt7RGorQm6cESQimh+33KWyJRIlGltCiEiBCJKi2RbnSed3bt2smFR43hqlMOSL1RuARC4W6/R060dUUv7LGG8uH444/n6quv5pRTTmlbdsstt7Bs2TJ+85vOc3JNnz6dm2++makHuDYdjUJUvFKBDyUDSwSm5955DGZfACgUl8PXX4eBe+c1pIbmCMfe9DRbXBVJvEdKfsAhofe6ve9i9xMTdj/dtagIWOB+UhlxKHzl2R68Sw7EEoGVCDqZOXMms2fP7pAIZs+ezU9/+tP0L4wdy41uxt/B1VA+rNfjs0Rgem7rKkDhwLNh8UOwY0PeE8GG+ka27GrmrENGMnFE++xOO5taqHlxPZsqDmXY4Z/s8n7f27STvy6opaw4zICyIi6aVsPP/rmMfYaVU11ZnnkHSYQFDhw5KHUXwXefgo2Lu7XvnFJXKgrIlJBdce6553LttdfS3NxMSUkJq1atYt26dTzwwANcccUVNDQ0cO6553LDDTd0fGFZhZdgY0m2uHv/Y5lYIjA9F/sn3f90LxFEI3kLpak1wqvv17Fsww4Azj5sFMfvP7xt/bbdzciLyocDJzHs2G9l3F9jS4RX3q8j6qqYXq7fwh2RlRw1tpKl63dQOXASv428yU+OPphpR/g0e97uLbD+TX/23Zv6SiJ44mrY8Fbv7nPvg+ETP0m5urKykiOOOIInnniCGTNmMHv2bM477zy+973vUVlZSSQS4YQTTmDRokVMnjy5/YXhIhgwPOV+e4slAtMLXCIIu38nzV8iePiNtXz3b+1f8uqEu1TLisO0AC3R7E5W97/yAT98dEmHZSXhEFPGVPDyyjqumOOdoH1tlJVwXpNr9qxqKJ1Y9VAsEfz+979nzpw53HnnnbS2trJ+/XqWLFnSMRHkiCUC03OxEkHI/Tvl8aS1frs3VMHf/+doBpUVd+qJU1oUIkKU1kh2vTA2bG+grDjE7EumtS0bWl7C3oPL+MRBexNVKCsOsf9eA3vvQyQKhfOaXLMWKxEUemNxmit3P82YMYNvfetbvP766+zevZvKykpuvvlm5s+fT0VFBbNmzaKxsTEvsVkiML0gMRH0bATJ3c2t3PHsShpaun7ye+m9LQzuV8xhYyqSrhcRQiivrdnOmPpGhg8qA+BPL61izdaGTts/v3wzlf1Lkt6UNbm68zJfSLjHxzQn2qqGrESQzIABAzj++OO5+OKLmTlzJvX19ZSXlzN48GA2btzIE0884c1BkAeWCEzPtZUIXF+aHl69vrxyC7f+ZzmlRSFC3ahvPm6/qrTrQwJKiEcXrefiY8exfXcLP3hkMcVhoSjU+SR26kH5bfhuT7BRSBJfwbBeQxnNnDmTs88+m9mzZ3PAAQcwZcoUDjjgAEaPHs0xxxyTt7gsEZhekNBG0MOqoS07vS6f/77iY4yu7N+jfSVTEgZtpa3EUbfbe7+fnjuZs6f0bMo/X8RO/hqhoOeS6iuNxXn0yU9+kvih/9snoOlo3rx5uQnIsURgMlq3rYFvzl6Ysqrm3Ib3uQg6tBFcOedN3t24g3BIuOGsAzkkw3g3f12whntfWg3Alp1NAAzp79ONaaoowpadzXzu96+0tSsU7IBn4u5SiLYWzM16SVnVUJ9lfzGT0esfbOXVVXWUl4YZNrC008/uJneXrksELa0t/O31WnY3t7JwzTb+u2Jzxvd4dNF6Vm/ZxbCBpRwwYhAXHzOOAT4NvSsaJRwOs6h2G88v30x5aRFnTB7BlNHJ2xXyrgAa4bNjVUN9lZUIAkxV2dWc+eSywV0x3zZzCsMHlnVa/+SvZ8OHtLUR7GpsAkq5+Nhx/PixpWysb2RnU/rGzi27mpgypoK7Z32ky5+jyzRKOBRi3Tavcfj6MycxJUXjckGIDS1R6D2HCnyICVVFAlBt1Z1ZJy0RBNgN/1jCPS+uymrbcEhSVp20TZrkTlhLarcCezOkXwlVA0q596XVbdU+6ZxzWK7q55WioiLWuQTnx0QovaqtaqjQE0HhVg2VlZWxZcsWhg4dukcnA1Vly5YtlJV1vmBLxxJBgC1ZX0/N0P5ccOTYjNvWVJVTHE7+BQ/HZj5xVRg7G706/mP2Hcr/nTuZRbXbs4rnpEl7ZbVdj7irpRMn7kVL1USGDijxpUG6V4X6SiKIVQ0V3om2urqa2tpaNm3q3eG+C1FZWRnV1V27qLJEEBDbG1qY/35dh8Fs125tYHL1YL58XM8m9Q677/2qbc3UAGs276RfcZgh/Us4cvxQjhw/tEf771XuqrW6ckCPP3fO9JmqocLtNVRcXMy4cePyHUbB8jURiMipwK14gzPepao/SVg/FrgbGAbUAReqaq2fMQXVr55ezu+ef7/T8tMO7nkf+VjV0Ndmv8U/gLdr66iuyvU4+Fnqi33d+0rVkDUW91m+JQIRCQO3AycBtcB8EZmrqvEDt9wM3KuqfxSRjwP/D/icXzEF2frtjYwa0o87Pnd4h+X79cLQCEXuAnBrk0IpXHniPvQ/clr6F+VL21VrfsPokl66Y9t3BdxGYNLzs0RwBLBCVVcCiMhsYAYQnwgmAVe4x88AD/sYT9/yxn2w4W2oqIGjLu3WLlZ8uIMHXl1DVJU3PtjG3oPLOGjU4N6NE4g1HUTUe1C95h/QFDfe/9hpMGlGr79vt/TFk1WsaujZm6B0UPpt86l5h3vQl7KsAX8TwShgTdzzWuDIhG3eBD6FV310NjBQRIaq6pb4jUTkEuASgDFjfBrqt9A8eQ001XuPD/s8lHS9QfO+lz/gnhdXMbDM+zOfPWVUb0bYZtiAEqIIofKhNA7cl7J1C2HdQm9lyy5Y/lThJIK+WH1RtR+UD4elj+Y7ksz6V8GwNDOtmYKU78bibwO/EpFZwHPAWqBTRaiq3gncCTB16lR/J+8sFBrFu7LSblcJ1O1qpmZof+Z95/heDS3RmIp+IMILPzgdOL3jyocuhdUv+Pr+XdJXRsiMN+ow+M7yfEdh9mB+JoK1wOi459VuWRtVXYdXIkBEBgDnqOo2H2PqO1S9uuFoS7d6i+xobGHum+uSjprZ+5SUJ9ZCG0u/L1YNGeMzP78N84EJIjJOREqA84G58RuISJVI2zfyGrweRAYA7dHQAu9u9OprDxqVgzpl1dRdBkOFlgj6YNWQMT7z7dugqq3A5cBTwFJgjqouFpEbReQst9l0YJmIvAvsBfzYr3j6kkhUvZrsHiSC2EBqn5maizaVNCWCQptUpYD7uhuTL762Eajq48DjCcuui3v8IPCgnzH0RbP+8Cp3NrdSXFrk/YG6eCJ9ZOFavjF7IQBVA3Mwoma6EkGhTapiVUPGdGLfhgL0/HJvtM4IccMPd0Fs+OdffXYKIwbn4sYuTX1iDRV5E6oUGksExrTJd68h42xvaOGl9za3V2GjaJZ3lO5qauW/Kzazz7AB1Aztz+NvbaCifzFnTB7pc9SO9sGqob7Ua8gYn1kiKBC/nreCO55d2WFZNJYINP0V9b0vreamJ99hfFU51505ifc372JcVblfoSaRrmooVGBVQ4U7MJox+WKJoEBs3N7IyMFl3P2Fj/BhfRNyP3ElgvQn0o31XsPw+u2NbY9/e+Hh6V7SuzKVCAqq15C1ERiTyBJBnkSiyq3/fpdtDd7sXvNXbWXYoDIO2HsQlf29k3k0y6qh/7yzEfDm4J2zwBuzb0wuh1ZO2320qDCrhqxEYEwbSwR5snR9Pbc9vYIBpUUUu3Gcj93Xq9MPhbw7iqPi/jwZTqSxyd6HDSxl5aadHDGukn4lYd9i7yzDDWUaTZ8scsruIzAmkSWCPNm62zt5/+ELH+EjNZUd1oVFEJRorFNXmhJBSyTK7uYIV5y0H18/YYJv8aaV6YYy8D5DuAD+3ayx2JhO7LIoT34zzxuds6J/cad1YVdCyKZq6OanlqXcT+5kaCOAwqkesjYCYzqxb0OexO78rRnauXePVyKI7zWU+iQau+fg+AOG93qMWVNNfYGdZYN3ztgQE8Z0UgBl9T1bJKrU7WrutLxuVzOfO2osRUnmAY7NAdx+Q1lcIthd1+Gk2lq/gVmHVFE9qAd3ECfss5P+Q9uv7JPKokTgZ8+hSCs01GW37S4vcRZGe4UxhcESgc+umLOQRxauS7quakBp0uXhkLgbymJtBO4kveQRmPP5Dtv+E2AZ8JdT4bN/6XqASx+Fv1yQfptDL4BP/jr1ek1zZ3EsAXy4BMYc1fX4sjH7s96cB10RzsHQG8b0EZYIfLZ8404mjhjEZ4/sOPhbWIRTD0o+X3BYhCgQIaHX0HY3nfPJP4biMlZu2sXdL7zPd4a+yODYuq6Kve6kHyaf/ObFX8L2NZ2Xd5CmsXjEZO934/buxZeN7Wtgr4Nh6qzstg+Xwv6f8C8eY/oYSwQ+amyJsGR9PZ8+vJrPHTU269eFQoKi7Gh29dnRCDsaW6hdu5WJwGOlpxAqGcCaAbu5L/IO3x26AXau6l6QsSRz+EVQlmQay7f/nnmsoLZJdJLoV9H2GXwTjcDw/eAjX/LvPYzZg1ki8NFji9YDUDUweRVQJpt2tTIxDGiUe19azY431jCxGK7469s00V61UVpS0v1eObETtKRoA8hmiIhMo4+Cv72GNJKhDcMYk451nfDRxh1ez6CvfXzfLr9WgNa40Uc31jcS6yH6j68f17bd2KH9KSku6v4Vd+wkn+pEGspmGOl0jcWxORV87DUUbU2dyIwxGVmJoBs27Wji1/NWMKayP+9u3ElI4MKjxjJxRPtsYK++X8ejb66nrDhE/5KuH+aQaFuvoZ89uZR710e5bqBAC+y3dwUDy4rY0dhKdUU/N9RzN0+0sSv1UIoYQ1kkmWxvKPNLNJo6fmNMRvbt6YZn3vmQP7ywCoCikBBRpTgc4vqzDmzb5q7nV/Luxh2ccmDyBuFstLoC2/KNXkPr+KH9YAMQCnHCAcN54b0tfHTCMKgLZxyhNKVY/X/KqqFshpHOMMQEdD++bGgEQla4Naa7LBF0Q93u9vsCPlJTybrtDW1DRsRs3d3M1JoKbr/gsG6/T8QlgiK8E/H0CZWw0Tux3nL+lPYNHwl1/4q7rUSQamKZcBaNxfkuEUSsasiYHrBEkIVFtdv49l/fpF9JEfdefARb424QqywvoaElQt2uZuYsWNM2dMTarQ2cOKlnd/vGqoZCuBNxtDV5FUhPRvhMtc+2ffe0jSAHdxZn+gzGmLSsPJ2FV9+v492NO3lzzTZWfLiz7U7hsw4ZyQVHjaGyvIStu5t5eumHbNnZxMGjBnPqQXtz4ZHZdxntwA2DMHX8MADOnLwXv73wsNS9Y3oyL3Cmq+lsqoaU1DeUWa8hYwqeXUZlYcWHO9ser93WwIb6RiaNGMRtM73qmb+9tpYl6+opDoc4IG55T42qGAir4aT9q+CgEVAbTX7S7snkL5lOolntO99VQymOizEmK1YiyODvr9cye377nbVff+ANnl++ucO9AcMGlrKhvpE3PtjGsG7eM9BBbGC0xGqVaGvyunzpSWNxFiWCjPcRpLmhrK37qJ+JoNVKBMb0gJUIMnhvk1ca+Ntl0/jG7IXUbm3gqPGVXH/mpLZtvvTRcYyr6k9U4dh9q3rvzUMJQ0xoJEUbQQ9KBNFMJYKizEkm7eijLnFZ1ZAxBcsSQQZbd7dQNaCEw8dWcvKkvbn7hfeZOraS8cMGtG1TNaCUz3xkTJq9dFViiSDS/jtl1VAP7iNImwiy6ZGU59FHrdeQMT3ia9WQiJwqIstEZIWIXJ1k/RgReUZE3hCRRSJymp/xdMfWXc1U9PeGcxg71BuUbczQHM0HnFitkqoKJKu+/ilkuis3q6qhLIaY8L3XkCUCY7rLtxKBiISB24GTgFpgvojMVdUlcZtdC8xR1d+IyCTgcaDGr5i6o25XMxXlXiL4/LSxnHzgXuw9qMzfN01sI2irGkpxB202d/+mEk1R3RS/757cUJZYvdXbVL33t+6jxnSbn9+eI4AVqroSQERmAzOA+ESgQGxchsFA8oH78+SPL67ilffrOH5/rxuniDBicL8cvHMsEbg/z1t/hY1LYPWLybtphsLeax7+H7o8F+8HL2buNdS0Ax7+aupt1ryafqwigMUPw5aVXYstK7EZx6xEYEx3+ZkIRgHxA9nXAkcmbHM98E8R+RpQDpyYbEcicglwCcCYMb1ZF5+aqvK/cxcDULu1ISfv2Ulxf6j5KNS9DzvnecsmJDlEo6bCkDGw8tnuvc8+x6deN/pIWPYErJyXfh/7npB8eagIxh8Pm5dn3kd3DRkDo7p/B7cxQZfv8vRM4B5V/ZmITAP+JCIHqXbspqKqdwJ3AkydOlVzEdju5jxOth4/r+6sRzNvP+FE+OZb/sRy8LneT3eJwOcf7rVwjDG9z8/G4rXA6Ljn1W5ZvC8CcwBU9SWgDOjF/pfdFz920JDY+M/GGLMH8jMRzAcmiMg4ESkBzgfmJmzzAXACgIhMxEsEm3yMKWtbd7UAMGXMkF67Uzh7sRKBTbBujPGfb4lAVVuBy4GngKV4vYMWi8iNInKW2+xK4Msi8ibwADBLVXNS9ZNJ7Eay7582MUcNxMYYkx++thGo6uN4XULjl10X93gJcIyfMXTH6i27+OZfFgL0zpARXdWWC61EYIzxn401lMSG7d4Uk188dhxjh5bnORpjjPGXJYIkGlq8HkOnTx6RpwisjcAYkzuWCJJodImgX3GeblKyqiFjTA5ZIkiiId+JwBhjcsgSQRINzd79bP1K8pUIrGrIGJM7lgiS2Nnk3UNQXprvG6+NMcZ/lgiSqNvVQkk4RHm+SgTWRmCMyaFAJYJ7X1rFV/60ION2jyxcy6B+RYhVzRhjAiBQdR/XPbI4q+1CInlsHwBrIzDG5FJgSgTb6zZRI+sZxlaaW9PPwdvUGuG4CcNyFJkxxuRXYEoECx++lXmlvwRg29IqSg4+NeW2Dc2R/HYdtTYCY0wOBaZE0DAubkKXuvdSbqeqNLRE8lw15FjVkDEmBwKTCMZNPLztsaapGWpoiRBVKMvrzWQFMQCrMSYgApMI4qt6omlGul6waisARaE8Xo1b1ZAxJocCkwjKits/ajTNBXdsispjJxTERGnGGOO7wCSCkqK4RJCi6mVR7TYuve81IN/jDFn3UWNM7gQmEQwqa593WFMUCR56o31K5YJoLDbGmBwITCIIxdX5p6oaig0/DXkuEVgbgTEmhwKTCOJFU3QbamxpX57fXkPGGJM7WSUCESkXkZB7vJ+InCUixZleV6g0mjwRNLiG4u+fNrEwEoG1ERhjciDbEsFzQJmIjAL+CXwOuMevoHwXjSRdvGlnE4dUD+bLx43PcUDGGJM/2SYCUdXdwKeAX6vqp4ED/QvLX9FIa6dlqsprq7cWxlW4tREYY3Io60QgItOAC4DH3LICqDvppiQlgtj0lIdUD851NElY91FjTO5kmwi+CVwDPKSqi0VkPPCMb1H5TLVzIqjb1QzAgSMH5TocY4zJq6xGH1XVZ4FnAVyj8WZV/Xqm14nIqcCteKWHu1T1JwnrfwEc7572B4ar6pCso+8mjXauGtq225uesqJ/id9vn1maITCMMaa3Zdtr6M8iMkhEyoG3gSUi8p0MrwkDtwOfACYBM0VkUvw2qvotVT1UVQ8Ffgn8vRufoesiqUsEleUFkAiMMSaHsq0amqSq9cAngSeAcXg9h9I5AlihqitVtRmYDcxIs/1M4IEs4+kRTdJG8MKKzQBUFEQisDYCY0zuZJsIit19A58E5qpqC5nHSh4FrIl7XuuWdSIiY/GSy9Mp1l8iIgtEZMGmTZuyDLmz9cffAkA0SdVQrERQXdGv2/s3xpi+KNtEcAewCigHnnMn7vpejON84EFN1ooLqOqdqjpVVacOG9b9KSRbD/4MjVpMa6TzDWVNrVHGVZVTWlQAnaGs+6gxJoeySgSqepuqjlLV09SzmvZG3lTWAqPjnle7ZcmcTw6qhfqVhFGE1tbO+aapNUJpUSBH3DDGBFy2jcWDReTnseoZEfkZXukgnfnABBEZJyIleCf7uUn2fQBQAbzUxdi7rH9JmCiSskRQOInA2giMMbmT7ZnvbmAHcJ77qQf+kO4FqtoKXA48BSwF5rh7EG4UkbPiNj0fmK3qf5/JsqIwUUJEktxZ3NQSLYxqoQ4sERhj/JfVfQTAPqp6TtzzG0RkYaYXqerjwOMJy65LeH59ljH0WGwo6tYk3UebWiOUl2Z7OHxm9xEYY3Io2xJBg4gcG3siIscADf6E5C+VEJEko49a1ZAxJqiyvQS+FLhXRGID8WwFLvInJH8pQiRpiaAQq4aMMcZ/2Q4x8SZwiIgMcs/rReSbwCIfY/OHpEoEkQ7zGueVdR81xuRQl858qlrv7jAGuMKHeHynhJL3GmoppKohY4zJnZ6c+frm5aqEiFobgTHGtOnJma9vdm1JUzVUWgjTUxpjTI6lbSMQkR0kP+EL0EcH5enca0hVaS6kEoG1ERhjcihtIlDVgbkKJGdEiCaMPtoSUaIKJeECSQTGGJNDgTvzSZI2gvpGb1KaQf2K8xFSEtZGYIzJnQK5lTaHJEQkodfQVjcEdUV5CWz7AHbXta8sGwSV473Hu7ZAQx0MHg3FZd6yLe9B04727curYHB18veuWwmNGQZtHbi3VQ0ZY3IqeIkgFAKitESiFLuqoNhcBMPDu+DWI0ATehV97XUIl8AtB3nPJ54Jn7kPNi6B30xL2H8xXLXSSyDx6lbCbVMyx1c6CL7yXDc+mDHGdE/gEoGIEELZ3RxhcD8vEWx18xVXFjV4SeDIy2DccbB+ITx7EzRshfjJbGpf8343uJLDx6+F4QfCe/+B+XdB867OiWD3Vu/3cVfByBQJYfFD8Nac9kRkVUPGmBwIYCIIISgNzREGuzaB+19ZDcDgUtd9dNRhcMBpUOSmrYxGEgaC0/blAGOOhppjYJebPS3Z/DqxZaOPhAknJg9u01K3386joxpjjF8C11iMhFyJwDvZbtrRxPPL3XzF/aRtG++3SwypTsyx5aFwx9/Jtm/bNs0h7/R+ViIwxvgvcIlAQu1VQwC7mryT7k/PmUxJ7LwbKur4WyMJ1TTucawKJ3H7aJISQWxZKE0hrO31ViIwxuRO4BJBSEKA0tDinZhjvweWFbVX33S6wk+sGqJ9OXQuQSQ2NkP7viXN3cuJJQprIzDG5EDgEoGEwh1KBI0uEZSVhONOwOGOv5Nd4UOSqqFQx+Xptk0aXIb3M8YYHwQvEbheQw2ujSBWIuhXHIZoiqqeTlVDtC9Ptn3SqqGEfScTSkwEViIwxvgvcIkgFAojKLuaXNVQc1wiaDuxhzr+TlkiSKjuaasaStNrSNIccqsaMsbkQfASQdjrNbS9wbt3IPZ7UL/i9Cf2ZN1H2xqLk7QpJIomtD8kk6mXkjHG+CBwiSAcChEiytbd3t3EbTeT9S+Jq8dPrOppTX6V36mNIF3VUMK+k+n0eisRGGP8F7hEIBKiOCztiWBXM+GQZO411OHkLu3LIa4E4Q5n2qqhLvQaMsaYHAhcIkBCFIWgscWr1qnb3cyQfsWEQpKmaiia/uSeVdVQQjVSiti8ba2NwBiTOwFMBEKReBPRgFciqCiPG0oCkncHTVfd06m7aZruo11pLDbGmBzwNRGIyKkiskxEVojI1Sm2OU9ElojIYhH5s5/xeG8YIize1JQA23a3MCQ2D0GnK/y4Ovts7haO726aKLGraTLpXm+MMT7xbdA5EQkDtwMnAbXAfBGZq6pL4raZAFwDHKOqW0VkuF/xtAcWIixKkysR7GxqZeiAhBJB0l5DyU7uqXoNJbmzuEu9hiwRGGNyx8/RR48AVqjqSgARmQ3MAJbEbfNl4HZV3Qqgqh/6GI8jHNqykGhzI+CNNXR0+Tq4//+gfr23SeKJ/aVfe/MRxOxYBzfvD6VuJs/EISb+cwO8fHvHt92+tuM2ycTe77+/cNtaG4Exxn9+Vg2NAtbEPa91y+LtB+wnIi+IyMsicmqyHYnIJSKyQEQWbNq0qWdR9a8EoKKpFvBKBIc0vQbL/wnhYphwMlSM87YtHwaTZnhzC8RmJIvZucFbdtA53mQyAFUTYN8ToajUm7Us/qdsEEw8y9tnKntPhnEfg5JyqPlo6nkLjDGmF+V7PoIiYAIwHagGnhORg1V1W/xGqnoncCfA1KlTk4z+1gWHnA/LHqeltX300bKw2+XFT3on8ZhQGM67t+Pr598Fj13pPf7M/VAxtn1d2SC48G/dj23QCLhobvdfb4wx3eBniWAtMDruebVbFq8WmKuqLar6PvAuXmLwj6vGaY1EiEaVXc0RSmPpMF21TTLp6vuNMaaP8DMRzAcmiMg4ESkBzgcSL3cfxisNICJVeFVFK32MKS4RtLI7NvJoyJUIsjmxxw810dXEYYwxBci3RKCqrcDlwFPAUmCOqi4WkRtF5Cy32VPAFhFZAjwDfEdVt/gVE9CeCFojbNnZBEBpWL3lXW2ctRKBMWYP4Gsbgao+DjyesOy6uMcKXOF+csMlgkhrK3c85xU++hdL9lf38cki3T0BxhjTRwTwzmLvhN8aiVDvRh6tqSjr3tV9uruEjTGmjwjemcxd0be0Rnh00XoOGzOEkEa6V99vVUPGmD1AABOB95FDeHf/Du5X7N013J1qHqsaMsbsAQKbCMJucpkrT97fG9Ih1I1DYb2GjDF7gMAmgpB4JYKKcjchTbYn9fjuo1Y1ZIzZAwQvEbiTt7gSwd6DyrpfNWSNxcaYPUDwzmRtbQTK+KpywiHxRgvN9uo+vvuoDQpnjNkDBDgRRL3pKcErEVh9vzEmoAKeCNyENNFWq+83xgRWgBOBMqA0bgYySwTGmIAKdCLoVtXQ8Ek+BWaMMfkRvDui4qqGBsXmKo52odfQ2KPhy0+nn2DGGGP6kMAmAokvEXT1hrJRh/sQmDHG5Edgq4bCRDlj8ghvmfUaMsYEWPBKBK5R+NefPRSGu8nno602bpAxJrACWyLoMFSE9RoyxgRYgBNBtH2ZRq1qyBgTWAFMBG5YiPhEYDeUGWMCLICJwH3kaKR9mVUNGWMCLICJwJ3wO1QNWa8hY0xwBTARJGkj6MoNZcYYs4exRABWNWSMCTRLBOCqhoJ3KIwxBiwReKxqyBgTYL4mAhE5VUSWicgKEbk6yfpZIrJJRBa6ny/5GQ/QXgVk3UeNMQbwcYgJEQkDtwMnAbXAfBGZq6pLEjb9i6pe7lccSQLzfluvIWOMAfwda+gIYIWqrgQQkdnADCAxEeRWrGroyauhfxU01cPWVTBmWl7DMsaYfPGzamgUsCbuea1blugcEVkkIg+KyOhkOxKRS0RkgYgs2LRpU8+iim8U/vuXYMnD3uMDP9Wz/RpjTB+V78bifwA1qjoZ+Bfwx2QbqeqdqjpVVacOG9bDCWESewdFozD2GNjv5J7t1xhj+ig/E8FaIP4Kv9ota6OqW1S1yT29C/B/xpfEtgDrOmqMCTg/z4DzgQkiMk5ESoDzgbnxG4jIiLinZwFLfYzHvWliicDmIjDGBJtvZ0BVbRWRy4GngDBwt6ouFpEbgQWqOhf4uoicBbQCdcAsv+Jp0ykR2F3Fxphg8/VSWFUfBx5PWHZd3ONrgGv8jKGTxERgXUeNMQEXvMrx2H0EMdGoVQ0ZYwLNEkG0FULBOwzGGBNjZ0CrGjLGBJwlAhtwzhgTcJYIbMA5Y0zAWSLQqFUNGWMCzRKB3UdgjAk4SwRWNWSMCThLBNZryBgTcJYIdm+xXkPGmECzRACw68N8R2CMMXkTzEvh76yE3ZvhnUfhPzdCS2O+IzLGmLwJZiIoH+r9bHw735EYY0zeBbtqyNoGjDEm4Ikg1lsocSA6Y4wJkIAnAvfxVfMbhzHG5FGwEwGWAIwxJuCJwLGqIWNMgAU8EbgEYFVDxpgAC3gisARgjDEBTwSOVQ0ZYwLMEoExxgScJQJjjAk4SwTGGBNwviYCETlVRJaJyAoRuTrNdueIiIrIVD/jMcYY05lviUBEwsDtwCeAScBMEZmUZLuBwDeAV/yKJaVYt1HrPmqMCTA/SwRHACtUdaWqNgOzgRlJtvshcBOQ+7Ggw8Xe76LSnL+1McYUCj8TwShgTdzzWresjYgcBoxW1cfS7UhELhGRBSKyYNOmTb0X4b4nwrFXwOk/7719GmNMH5O3xmIRCQE/B67MtK2q3qmqU1V16rBhw3oviFAYTvxfb24CY4wJKD8TwVpgdNzzarcsZiBwEDBPRFYBRwFzrcHYGGNyy89EMB+YICLjRKQEOB+YG1upqttVtUpVa1S1BngZOEtVF/gYkzHGmAS+JQJVbQUuB54ClgJzVHWxiNwoImf59b7GGGO6xte5GlX1ceDxhGXXpdh2up+xGGOMSc7uLDbGmICzRGCMMQFnicAYYwLOEoExxgScaB8bZ0dENgGru/nyKmBzL4bjp74Sa1+JE/pOrH0lTug7sVqcMFZVk96R2+cSQU+IyAJV7RM3rPWVWPtKnNB3Yu0rcULfidXiTM+qhowxJuAsERhjTMAFLRHcme8AuqCvxNpX4oS+E2tfiRP6TqwWZxqBaiMwxhjTWdBKBMYYYxJYIjDGmIALTCIQkVNFZJmIrBCRq/Mcy2gReUZElojIYhH5hlteKSL/EpHl7neFWy4icpuLfZGb2S2X8YZF5A0RedQ9Hycir7h4/uKGGUdESt3zFW59TY7jHCIiD4rIOyKyVESmFeIxFZFvub/72yLygIiUFcoxFZG7ReRDEXk7blmXj6GIXOS2Xy4iF+Uozv9zf/tFIvKQiAyJW3eNi3OZiJwSt9z380KyWOPWXSkiKiJV7nl+jqmq7vE/QBh4DxgPlABvApPyGM8I4DD3eCDwLjAJ+ClwtVt+NXCTe3wa8AQgeBP4vJLjeK8A/gw86p7PAc53j38LXOYe/w/wW/f4fOAvOY7zj8CX3OMSYEihHVO86VrfB/rFHctZhXJMgeOAw4C345Z16RgClcBK97vCPa7IQZwnA0Xu8U1xcU5y3/lSYJw7F4RzdV5IFqtbPhpvmP7VQFU+j6nv//iF8ANMA56Ke34NcE2+44qL5xHgJGAZMMItGwEsc4/vAGbGbd+2XQ5iqwb+A3wceNT9g26O+8K1HVv3Tz3NPS5y20mO4hzsTrCSsLygjintc3lXumP0KHBKIR1ToCbhBNulYwjMBO6IW95hO7/iTFh3NnC/e9zh+x47prk8LySLFXgQOARYRXsiyMsxDUrVUOzLF1PrluWdK+pPAV4B9lLV9W7VBmAv9zif8d8CXAVE3fOhwDb1Jh5KjKUtTrd+u9s+F8YBm4A/uGqsu0SknAI7pqq6FrgZ+ABYj3eMXqMwj2lMV49hIXzfLsa7siZNPHmLU0RmAGtV9c2EVXmJNSiJoCCJyADgb8A3VbU+fp16aT+vfXtF5AzgQ1V9LZ9xZKkIr/j9G1WdAuzCq8ZoUyDHtAKYgZe4RgLlwKn5jKkrCuEYZiIi3wdagfvzHUsyItIf+B6QdJKufAhKIliLVx8XU+2W5Y2IFOMlgftV9e9u8UYRGeHWjwA+dMvzFf8xwFkisgqYjVc9dCswRERis9vFx9IWp1s/GNiSgzjBu0KqVdVX3PMH8RJDoR3TE4H3VXWTqrYAf8c7zoV4TGO6egzz9n0TkVnAGcAFLmmRJp58xbkP3oXAm+67VQ28LiJ75yvWoCSC+cAE1zOjBK/RbW6+ghERAX4PLFXVn8etmgvEegNchNd2EFv+edej4Chge1xR3Teqeo2qVqtqDd4xe1pVLwCeAc5NEWcs/nPd9jm5elTVDcAaEdnfLToBWEKBHVO8KqGjRKS/+z+IxVlwxzROV4/hU8DJIlLhSkAnu2W+EpFT8aoxz1LV3Qnxn+96YI0DJgCvkqfzgqq+parDVbXGfbdq8TqPbCBfx9SPhpFC/MFrjX8Xr5fA9/Mcy7F4xetFwEL3cxpe3e9/gOXAv4FKt70At7vY3wKm5iHm6bT3GhqP90VaAfwVKHXLy9zzFW79+BzHeCiwwB3Xh/F6VxTcMQVuAN4B3gb+hNebpSCOKfAAXttFC94J6ovdOYZ4dfQr3M8XchTnCrx69Nh36rdx23/fxbkM+ETcct/PC8liTVi/ivbG4rwcUxtiwhhjAi4oVUPGGGNSsERgjDEBZ4nAGGMCzhKBMcYEnCUCY4wJOEsExiQQkYiILIz76bVRKUWkJtkolMbkU1HmTYwJnAZVPTTfQRiTK1YiMCZLIrJKRH4qIm+JyKsisq9bXiMiT7vx4/8jImPc8r3cuPhvup+j3a7CIvI78eYk+KeI9MvbhzIGSwTGJNMvoWroM3HrtqvqwcCv8EZmBfgl8EdVnYw30NltbvltwLOqegjeuEeL3fIJwO2qeiCwDTjH109jTAZ2Z7ExCURkp6oOSLJ8FfBxVV3pBg3coKpDRWQz3nj9LW75elWtEpFNQLWqNsXtowb4l6pOcM+/CxSr6o9y8NGMScpKBMZ0jaZ43BVNcY8jWFudyTNLBMZ0zWfifr/kHr+IN3IlwAXA8+7xf4DLoG3e58G5CtKYrrArEWM66yciC+OeP6mqsS6kFSKyCO+qfqZb9jW8mdG+gzdL2hfc8m8Ad4rIF/Gu/C/DG4XSmIJibQTGZMm1EUxV1c35jsWY3mRVQ8YYE3BWIjDGmICzEoExxgScJQJjjAk4SwTGGBNwlgiMMSbgLBEYY0zA/f/5XaJynSSD6gAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Defining random seeds to enable reproducibility\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(1)\n",
    " \n",
    "import random\n",
    "random.seed(1)\n",
    " \n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "from tensorflow.python.client import device_lib\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "print(device_lib.list_local_devices())\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "\n",
    "def load_patras_dataset():\n",
    "    \"\"\"\n",
    "    Loads the datasets encoded in .pkl files and returns its decoded form.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        A list of n-dimensional arrays representing the subjects samples that will be used to \\\\\n",
    "        train the drunkenness classification model.\n",
    "    list\n",
    "        A list of n-dimensional arrays representing the subjects samples that will be used to \\\\\n",
    "        test the drunkenness classification model.\n",
    "    ndarray\n",
    "        A n-dimensional array representing the training set labels.\n",
    "    ndarray\n",
    "        A n-dimensional array representing the test set labels.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Loading Sober-Drunk Face Dataset, from Patras University\")\n",
    "    \n",
    "    # Defining the sample and label sets filenames\n",
    "    sets = [\n",
    "        \"Insert the unbalanced x_training-set.pkl file path here\",\n",
    "        \"Insert the unbalanced x_test-set.pkl file path here\",\n",
    "        \"Insert the unbalanced y_training-set.pkl file path here\",\n",
    "        \"Insert the unbalanced y_test-set.pkl file path here\"\n",
    "    ]\n",
    " \n",
    "    # Defining an empty list for storing the decoded datasets\n",
    "    loaded_datasets = []\n",
    " \n",
    "    # Iterating over the dataset files\n",
    "    for set_ in sets:\n",
    "        # Opening the .pkl file in read mode\n",
    "        with open(set_, 'rb') as file:\n",
    "            # Appending the decoded dataset to the datasets list\n",
    "            loaded_datasets.append(pickle.load(file))\n",
    "    \n",
    "    # Unpacking the datasets list into individual subsets\n",
    "    x_train, x_test, y_train, y_test = loaded_datasets\n",
    "    \n",
    "    # Converting the label list to the n-dimensional array format\n",
    "    y_train= np.array(y_train)\n",
    "    y_test = np.array(y_test)\n",
    " \n",
    "    # Printing the dataset length\n",
    "    print(\"\\nSamples total: {0}\".format((len(x_train)) + (len(x_test))))\n",
    "    \n",
    "    print(\"\\nDataset splitting: \")\n",
    "    \n",
    "    # Printing the training and test sets length\n",
    "    print(\"\\nTraining set: {0}\".format(len(x_train)))\n",
    "    print(\"Test set: {0}\".format(len(x_test)))\n",
    "    \n",
    "    # Returning the training and test sets, and its respective labels\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "\n",
    "def min_max_norm(dataset):\n",
    "    \"\"\"\n",
    "    Normalizes the keyframes according to the minimum-maximum norm, \\\\\n",
    "    such that pixel values ranges from 0 to 1.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : list\n",
    "        A list of n-dimensional arrays representing the subjects keyframes.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ndarray\n",
    "        A n-dimensional array representing keyframes with pixel values ranging from 0 to 1.\n",
    "    \"\"\"\n",
    "\n",
    "    # Converting the dataset type from list to n-dimensional array\n",
    "    dataset = np.asarray(dataset, dtype=\"int16\")\n",
    "\n",
    "    # Finding the keyframes minimum and maximum values\n",
    "    x_min = dataset.min(axis=(1, 2), keepdims=True)\n",
    "    x_max = dataset.max(axis=(1, 2), keepdims=True)\n",
    "\n",
    "    # Applying the minimum-maximum norm to each keyframe\n",
    "    norm_dataset = (dataset - x_min) / (x_max - x_min)\n",
    "\n",
    "    # Printing the minimum and maximum values from a given sample for sanity check\n",
    "    print(\"\\nMinMax normalization\")\n",
    "    print(\"dataset shape: \", norm_dataset.shape)\n",
    "    print(\"min: \", norm_dataset[0].min())\n",
    "    print(\"max: \", norm_dataset[0].max())\n",
    "\n",
    "    # Returning the normalized dataset\n",
    "    return norm_dataset\n",
    "\n",
    "\n",
    "def visualize_model_history(hist):\n",
    "    \"\"\"\n",
    "    Displays the training process loss and accuracy history from a given fold of the \\\\\n",
    "    stratified cross-validation procedure.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    hist : dictionary\n",
    "        A dictionary conataining loss and accuracy values lists from the training and \\\\\n",
    "        validation sets.\n",
    "    \"\"\"\n",
    "\n",
    "    # Displaying the training and validation loss history\n",
    "    plt.figure()\n",
    "    plt.plot(hist.history['loss'])\n",
    "    plt.plot(hist.history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Val'], loc='upper right')\n",
    "    \n",
    "    # Displaying the training and validation accuracy history\n",
    "    plt.figure()\n",
    "    plt.plot(hist.history['accuracy'])\n",
    "    plt.plot(hist.history['val_accuracy'])\n",
    "    plt.title('Model accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Val'], loc='lower right')\n",
    "    \n",
    "    # Showing the training process loss and accuracy history\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visualize_model_history_cv(training_loss, training_accuracy, validation_loss, validation_accuracy):\n",
    "    \"\"\"\n",
    "    Displays the training process loss and accuracy history throughout the \\\\\n",
    "    stratified k-fold cross-validation procedure.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    training_loss : list\n",
    "        A list conataining the loss values from the training set throught the \\\\\n",
    "        stratified k-fold cross-validation procedure.\n",
    "    training_accuracy : list\n",
    "        A list conataining the accuracy values from the training set throught the \\\\\n",
    "        stratified k-fold cross-validation procedure.\n",
    "    validation_loss : list\n",
    "        A list conataining the loss values from the validation set throught the \\\\\n",
    "        stratified k-fold cross-validation procedure.\n",
    "    validation_accuracy : list\n",
    "        A list conataining the accuracy values from the validation set throught the \\\\\n",
    "        stratified k-fold cross-validation procedure.\n",
    "    \"\"\"\n",
    "\n",
    "    # Displaying and saving the training and validation loss history\n",
    "    plt.figure()\n",
    "    plt.plot(training_loss)\n",
    "    plt.plot(validation_loss)\n",
    "    plt.title('Model loss (cross-validation history)')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Val'], loc='upper right')\n",
    "    plt.savefig(\"final_model_tl_learning-curve.pdf\", dpi=600, bbox_inches='tight', pad_inches=0)\n",
    "    \n",
    "    # Displaying the training and validation accuracy history\n",
    "    plt.figure()\n",
    "    plt.plot(training_accuracy)\n",
    "    plt.plot(validation_accuracy)\n",
    "    plt.title('Model accuracy (cross-validation history)')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Val'], loc='upper right')\n",
    "    \n",
    "    # Showing the training process loss and accuracy history\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def transfer_learning():\n",
    "    \"\"\"\n",
    "    Performs the transfer learning strategy from a previously trained base \\\\ \n",
    "    drunkenness classification model. \\\\ \n",
    "    \n",
    "    In order to leverage the base model representation capability, we transferred \\\\ \n",
    "    this basic knowledge to a more robust classifier capable to identify \\\\ \n",
    "    drunkenness-related features variations. For such, we considered images from all \\\\\n",
    "    acquisitions periods, in which the pixels behavior tends to change over time \\\\ \n",
    "    accordingly to the subjects BAC.\n",
    "    \"\"\"\n",
    "\n",
    "    # Loading the training and test datasets\n",
    "    x_train, x_test, y_train, y_test = load_patras_dataset()\n",
    "\n",
    "    # Applying the min-max normalization\n",
    "    x_train = min_max_norm(x_train)\n",
    "    x_test = min_max_norm(x_test)\n",
    "    \n",
    "    # Reshaping datsets to the tensor format (channel last)\n",
    "    x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], x_train.shape[2], 3)\n",
    "    x_test = x_test.reshape(x_test.shape[0], x_test.shape[1], x_test.shape[2], 3)\n",
    "\n",
    "    print(\"\\n Loading pre-trained model\")\n",
    "\n",
    "    # Loading the base drunkenness classification model\n",
    "    model = models.load_model('vgg16_sober-drunk_ft-model_bs30_cb-4-5_fold-2.h5')\n",
    "\n",
    "    # Printing the base model summary\n",
    "    model.summary()\n",
    "    \n",
    "    print(\"\\nRemoving top layers\")\n",
    "\n",
    "    # Removing the base drunkenness classifier output layer\n",
    "    model = keras.Sequential(model.layers[:-2])\n",
    "\n",
    "    # Printing the base model structure again for sanity check\n",
    "    model.summary()\n",
    "    \n",
    "    # Iterating over the model layers\n",
    "    for layer in model.layers:\n",
    "        # Freezing the layers weights\n",
    "        layer.trainable = False\n",
    "        # Printing the layers 'trainable' parameter for sanity check\n",
    "        print(\"{}: {}\".format(layer.name, layer.trainable))\n",
    " \n",
    "    print(\"\\nAdding new dense layers\\n\")\n",
    "\n",
    "    # Adding a Flatten layer to the base model output\n",
    "    model.add(Flatten(name=\"flatten\"))\n",
    "\n",
    "    # Adding a hidden layer to the base model output\n",
    "    model.add(Dense(units=10, activation='relu', name=\"dense_layer_1\"))\n",
    "\n",
    "    # Adding an output layer to classify the received features as sober (0) or drunk (1)\n",
    "    model.add(Dense(units=1, activation='sigmoid', name=\"output\"))\n",
    "    \n",
    "    # Iterating over the model layers\n",
    "    for layer in model.layers:\n",
    "        # Printing the layers 'trainable' parameter for sanity check\n",
    "        print(\"{}: {}\".format(layer.name, layer.trainable))\n",
    "\n",
    "\n",
    "    # Shuffling the training set organization\n",
    "    x_train, y_train = shuffle(x_train, y_train, random_state=1)\n",
    "    \n",
    "    # Defining the stratified cross-validation folds\n",
    "    folds = list(StratifiedKFold(n_splits=7, shuffle=False, random_state=None).split(x_train, y_train))\n",
    "    \n",
    "    # Defining the optmization function\n",
    "    adam = optimizers.Adam(learning_rate=1e-5, amsgrad=True)\n",
    "    \n",
    "    # Defining the early stopping callback\n",
    "    callback = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=5, restore_best_weights=True, verbose=1)\n",
    "    \n",
    "    # Compiling the model\n",
    "    model.compile(loss=keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "                optimizer=adam,\n",
    "                metrics=['accuracy'])\n",
    "    \n",
    "    # Printing the final model summary\n",
    "    model.summary()\n",
    "\n",
    "    print(\"\\n\")\n",
    " \n",
    "    # Instantiating empty lists for storing the model training and validation loss history\n",
    "    cv_training_acc = []\n",
    "    cv_training_loss = []\n",
    "    \n",
    "    # Instantiating empty lists for storing the model training and validation accuracy history\n",
    "    cv_val_acc = []\n",
    "    cv_val_loss = []\n",
    "    \n",
    "    # Instantiating an empty list for storing the model classification accuracies on the test set samples\n",
    "    fold_acc = []\n",
    "    \n",
    "    # Defining weights for sober and drunk classes in order to prevent the class  \n",
    "    # imbalance from influencing the loss function\n",
    "    weight_for_0 = (1 / 35)*(140)/2.0 \n",
    "    weight_for_1 = (1 / 105)*(140)/2.0\n",
    "    \n",
    "    class_weights = {0: weight_for_0, 1: weight_for_1}\n",
    "    \n",
    "    # Iterating over the stratified cross-validation folds\n",
    "    for j, (train_idx, val_idx) in enumerate(folds):\n",
    "        print('\\nFold ',j)\n",
    "        # Defining the training and validation sets\n",
    "        X_train_cv, y_train_cv = x_train[train_idx], y_train[train_idx]\n",
    "        X_valid_cv, y_valid_cv = x_train[val_idx], y_train[val_idx]\n",
    "\n",
    "        print(\"\\nTraining with {0} samples and validating with {1} samples\\n\".format(len(X_train_cv), len(X_valid_cv)))\n",
    "\n",
    "        # Fitting the model\n",
    "        history = model.fit(x=X_train_cv, y=y_train_cv, \n",
    "                          validation_data=(X_valid_cv, y_valid_cv),\n",
    "                          shuffle=False,\n",
    "                          batch_size=60,\n",
    "                          #callbacks=[callback],\n",
    "                          epochs=205,\n",
    "                          class_weight=class_weights,\n",
    "                          verbose=1)\n",
    "        \n",
    "        # Updating the training loss and accuracy history lists\n",
    "        cv_training_acc += history.history['accuracy']\n",
    "        cv_training_loss += history.history['loss']\n",
    "\n",
    "        # Updating the validation loss and accuracy history lists\n",
    "        cv_val_acc += history.history['val_accuracy']\n",
    "        cv_val_loss += history.history['val_loss']\n",
    "        \n",
    "        # Evaluating the model on unseen data\n",
    "        test_loss, test_acc = model.evaluate(x_test, y_test, verbose=1)\n",
    "\n",
    "        # Appending the model classification accuracies to the test set accuracies list\n",
    "        fold_acc.append(test_acc)\n",
    "\n",
    "        # Printing the model classification accuracy on unseen data\n",
    "        print('\\nFold ',j)\n",
    "        print(\"\\nTest accuracy: \", test_acc)\n",
    "        print(\"Test loss: \", test_loss)\n",
    "        \n",
    "        # Saving the model obtained in the stratified cross-validation j-th fold\n",
    "        model.save('sober-drunk_final-model-VGG16_fold-{0}.h5'.format(j))\n",
    "    \n",
    "    # Printing the model classification accuracy in each fold of the stratified \n",
    "    # cross-validation\n",
    "    print(\"\\nK-Fold accuracy: \", fold_acc)\n",
    "\n",
    "    # Printing the model average accuracy along the stratified k-fold cross-validation \n",
    "    # procedure\n",
    "    print(\"\\nAverage accuracy: \", np.mean(fold_acc))\n",
    "\n",
    "    # Printing the standard deviation of accuracies obtained throughout the \n",
    "    # stratified k-fold cross-validation procedure\n",
    "    print(\"K-Fold Standard deviation: \", np.std(fold_acc))\n",
    "    \n",
    "\n",
    "    print(\"\\nClassification report: \")\n",
    "\n",
    "    # Obtaining the test samples class probabilities\n",
    "    y_prob = model.predict(x_test)\n",
    "\n",
    "    # Obtaining the binary label from the model output probabilities\n",
    "    y_hat = (y_prob > 0.5).astype(int)\n",
    "    \n",
    "    # Printing the classification performance report\n",
    "    report = classification_report(y_test, y_hat, target_names=['sober', 'drunk'])\n",
    "    print(report)\n",
    "    \n",
    "    print(\"\\nConfusion Matrix: \")\n",
    "\n",
    "    # Printing the confusion matrix\n",
    "    matrix = confusion_matrix(y_test, y_hat)\n",
    "    print(matrix)\n",
    "    \n",
    "    tn, fp, fn, tp = matrix.ravel()\n",
    "    print(\"\\nTrue Negatives: \", tn)\n",
    "    print(\"False Positives: \", fp)\n",
    "    print(\"False Negatives: \", fn)\n",
    "    print(\"True Positives: \", tp)\n",
    "    \n",
    "    # Displaying the training process history\n",
    "    visualize_model_history_cv(cv_training_loss, cv_training_acc, cv_val_loss, cv_val_acc)\n",
    "    visualize_model_history(history)\n",
    "    \n",
    "\n",
    "# Running the transfer learning from base model to the final model\n",
    "transfer_learning()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 ('virtual-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 247.518346,
   "end_time": "2022-06-14T19:30:15.611280",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-06-14T19:26:08.092934",
   "version": "2.3.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "91dd42dc31c8286e87f4f857c1cf087015eed96189f5f719a42d322017809b0b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
