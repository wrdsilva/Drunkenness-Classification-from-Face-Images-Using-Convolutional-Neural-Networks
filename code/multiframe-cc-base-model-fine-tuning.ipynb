{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1604319",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-03-15T01:43:53.147784Z",
     "iopub.status.busy": "2023-03-15T01:43:53.146980Z",
     "iopub.status.idle": "2023-03-15T02:10:53.608718Z",
     "shell.execute_reply": "2023-03-15T02:10:53.607725Z"
    },
    "papermill": {
     "duration": 1620.468551,
     "end_time": "2023-03-15T02:10:53.610931",
     "exception": false,
     "start_time": "2023-03-15T01:43:53.142380",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Defining random seeds to enable reproducibility\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(1)\n",
    " \n",
    "import random\n",
    "random.seed(1)\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.python.client import device_lib\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "print(device_lib.list_local_devices())\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "\n",
    "def load_dataset():\n",
    "    \"\"\"\n",
    "    Loads the datasets encoded in .pkl files and returns its decoded form.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        A list of n-dimensional arrays representing the subjects samples that will be used to \\\\\n",
    "        train the drunkenness classification model.\n",
    "    ndarray\n",
    "        A n-dimensional array representing the samples labels.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Loading Sober-Drunk Face Dataset, from Patras University\")\n",
    "    \n",
    "    # Defining the sample and label sets filenames\n",
    "    sets = [\n",
    "        \"Insert the x_balanced.pkl file path here\",\n",
    "        \"Insert the y_balanced.pkl file path here\"   \n",
    "    ]\n",
    "    \n",
    "    # Defining an empty list for storing the decoded dataset\n",
    "    loaded_datasets = []\n",
    " \n",
    "    # Iterating over the dataset files\n",
    "    for set_ in sets:\n",
    "        # Opening the .pkl file in read mode\n",
    "        with open(set_, 'rb') as file:\n",
    "            # Appending the decoded dataset to the dataset list\n",
    "            loaded_datasets.append(pickle.load(file))\n",
    "    \n",
    "    # Unpacking the dataset list into individual subsets\n",
    "    x, y = loaded_datasets\n",
    "    \n",
    "    # Converting the label list to the n-dimensional array format\n",
    "    y_arr= np.array(y)\n",
    "    \n",
    "    # Printing the dataset length for sanity check\n",
    "    print(\"\\nSamples total: {0}\".format(len(x)))   \n",
    "    \n",
    "    # Slicing the frame sequences of each subject for selecting the\n",
    "    # frames sampled at each 5 Hz\n",
    "    x = x[::5]\n",
    "    y_arr = y_arr[::5]\n",
    "    \n",
    "    # Printing the dataset length after slicing for sanity check\n",
    "    print(\"\\nSamples total after slicing: {0}\".format(len(x)))\n",
    "    \n",
    "    # Returning the samples set and its respective labels\n",
    "    return x, y_arr\n",
    "\n",
    "\n",
    "def min_max_norm(dataset):\n",
    "    \"\"\"\n",
    "    Normalizes the keyframes according to the minimum-maximum norm, \\\\\n",
    "    such that pixel values ranges from 0 to 1.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : list\n",
    "        A list of n-dimensional arrays representing the subjects keyframes.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ndarray\n",
    "        A n-dimensional array representing keyframes with pixel values ranging from 0 to 1.\n",
    "    \"\"\"\n",
    "\n",
    "    # Converting the dataset type from list to n-dimensional array\n",
    "    dataset = np.asarray(dataset, dtype=\"int16\")\n",
    "\n",
    "    # Finding the keyframes minimum and maximum values\n",
    "    x_min = dataset.min(axis=(1, 2), keepdims=True)\n",
    "    x_max = dataset.max(axis=(1, 2), keepdims=True)\n",
    "\n",
    "    # Applying the minimum-maximum norm to each keyframe\n",
    "    norm_dataset = (dataset - x_min) / (x_max - x_min)\n",
    "\n",
    "    # Printing the minimum and maximum values from a given sample for sanity check\n",
    "    print(\"\\nMinMax normalization\")\n",
    "    print(\"dataset shape: \", norm_dataset.shape)\n",
    "    print(\"min: \", norm_dataset[0].min())\n",
    "    print(\"max: \", norm_dataset[0].max())\n",
    "\n",
    "    # Returning the normalized dataset\n",
    "    return norm_dataset\n",
    "\n",
    "\n",
    "def visualize_model_history(hist):\n",
    "    \"\"\"\n",
    "    Displays the training process loss and accuracy history.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    hist : dictionary\n",
    "        A dictionary conataining loss and accuracy values lists from the training and \\\\\n",
    "        test sets.\n",
    "    \"\"\"\n",
    "\n",
    "    # Displaying and saving the training and test loss history\n",
    "    plt.figure()\n",
    "    plt.plot(hist.history['loss'], linewidth=2, alpha=0.85)\n",
    "    plt.plot(hist.history['val_loss'], linewidth=2, alpha=0.85)\n",
    "    sns.despine()\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper right', frameon=False)\n",
    "    plt.savefig(\"loss-curve.pdf\", dpi=600, bbox_inches='tight', pad_inches=0.1)\n",
    "    \n",
    "    # Displaying and saving the training and test accuracy history\n",
    "    plt.figure()\n",
    "    plt.plot(hist.history['accuracy'], linewidth=2, alpha=0.85)\n",
    "    plt.plot(hist.history['val_accuracy'], linewidth=2, alpha=0.85)\n",
    "    sns.despine()\n",
    "    plt.title('Model accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='lower right', frameon=False)\n",
    "    plt.savefig(\"accuracy-curve.pdf\", dpi=600, bbox_inches='tight', pad_inches=0.1)\n",
    "    \n",
    "    # Showing the training process loss and accuracy history\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def build_model():\n",
    "    \"\"\"\n",
    "    Loads the base drunkenness classification model, \\\\\n",
    "    unfreezes the deepest convolutional layers weights, \\\\\n",
    "    re-compiles the model and returns it.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Sequential\n",
    "        A single layer binary classification model.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\n Loading pre-trained model\")\n",
    "    \n",
    "    # Loading the pre-trained base drunkenness classification model\n",
    "    model = models.load_model('sober-drunk-cc_vgg16_tl-model_tts_70-30.h5')\n",
    "    \n",
    "    # Printing the base model summary\n",
    "    model.summary()\n",
    "    \n",
    "    # Iterating over the layers from the convolutional block 5\n",
    "    for layer in model.layers[15:]:\n",
    "        # Unfreezing the layers weights\n",
    "        layer.trainable = True\n",
    "    \n",
    "    # Iterating over the base model layers\n",
    "    for layer in model.layers:\n",
    "        # Printing the layers 'trainable' parameter for sanity check\n",
    "        print(\"{}: {}\".format(layer.name, layer.trainable))\n",
    "    \n",
    "    # Defining the optmization function\n",
    "    adam = optimizers.Adam(learning_rate=1e-6, epsilon=1e-3, amsgrad=False)\n",
    "    \n",
    "    print(\"\\nRe-compiling model...\")\n",
    "\n",
    "    # Compiling the model\n",
    "    model.compile(loss=keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "                  optimizer=adam,\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    # Printing the model summary\n",
    "    model.summary()\n",
    "    \n",
    "    # Returning the sequential model\n",
    "    return model\n",
    "    \n",
    "    \n",
    "def hyperparameter_search():\n",
    "    \"\"\"\n",
    "    Defines a custom training loop to enable the k-fold cross-validation \\\\\n",
    "    during the epsilon hyperparameter search.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    Since we need to search the value of only one hyperparameter (epsilon) \\\\\n",
    "    and the search space is small ([1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 0, 1]), \\\\\n",
    "    we performed the hyperparameter search manually in order to be able to analyze the \\\\\n",
    "    model behavior more carefully when changing the values of epsilon.\n",
    "\n",
    "    During the epsilon hyperparameter search, we noticed that the same number of epochs \\\\\n",
    "    from the transfer learning stage (400) was enough for fine-tuning the base model.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Loading the Sober-Drunk Dataset samples\n",
    "    x, y = load_dataset()\n",
    "    \n",
    "    # Defining the training and test subsets using the random train-test split strategy\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=1, stratify=y)\n",
    "    \n",
    "    # Applying the min-max normalization\n",
    "    x_train = min_max_norm(x_train)\n",
    "    x_test = min_max_norm(x_test)\n",
    "    \n",
    "    # Reshaping datsets to the tensor format (channel last)\n",
    "    x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], x_train.shape[2], 3)\n",
    "    x_test = x_test.reshape(x_test.shape[0], x_test.shape[1], x_test.shape[2], 3)\n",
    "    \n",
    "    # Defining the data augmentation generator\n",
    "    aug = ImageDataGenerator(rotation_range=20, zoom_range=0.15, width_shift_range=0.2,\n",
    "\t\t\t\t\t\t\t height_shift_range=0.2, shear_range=0.15, horizontal_flip=False, fill_mode=\"constant\")\n",
    "    \n",
    "    # Defining the early stopping callback\n",
    "    callback = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=5, restore_best_weights=True, verbose=1)\n",
    "    \n",
    "    # Defining the stratified cross-validation folds\n",
    "    folds = list(StratifiedKFold(n_splits=5, shuffle=False, random_state=None).split(x_train, y_train))\n",
    "    \n",
    "    # Instantiating an empty list to store the model classification performance on each fold\n",
    "    # this list will be used to calculate the model average performance throughout the\n",
    "    # stratified cross-validation process\n",
    "    fold_acc = []\n",
    "    \n",
    "    # Iterating over the stratified cross-validation folds\n",
    "    for j, (train_idx, val_idx) in enumerate(folds):\n",
    "        print('\\nFold ',j)\n",
    "        \n",
    "        # building a keras sequential model with a hyperparameter manually defined\n",
    "        model = build_model()\n",
    "\n",
    "        # Defining the training and validation sets\n",
    "        X_train_cv, y_train_cv = x_train[train_idx], y_train[train_idx]\n",
    "        X_valid_cv, y_valid_cv = x_train[val_idx], y_train[val_idx]\n",
    "        \n",
    "        print(\"\\nTraining with {0} samples and validating with {1} samples\\n\".format(len(X_train_cv), len(X_valid_cv)))\n",
    "        \n",
    "        # Fitting the model\n",
    "        history = model.fit(aug.flow(X_train_cv, y_train_cv, batch_size=60, shuffle=False, seed=1),\n",
    "\t\t\t\t\t\t\tsteps_per_epoch=len(X_train_cv) // 60,\n",
    "\t\t\t\t\t\t\tvalidation_data=(X_valid_cv, y_valid_cv),\n",
    "\t\t\t\t\t\t\tvalidation_steps=len(X_valid_cv) // 60,\n",
    "\t\t\t\t\t\t\tshuffle=False,\n",
    "\t\t\t\t\t\t\tepochs=400,\n",
    "\t\t\t\t\t\t\tverbose=1)\n",
    "    \n",
    "        # Evaluating the model on validation data\n",
    "        val_loss, val_acc = model.evaluate(X_valid_cv, y_valid_cv, verbose=0)\n",
    "        \n",
    "        # Appending the model classification performance on the k-th fold to the cross-validation\n",
    "        # accuracies list\n",
    "        fold_acc.append(val_acc)\n",
    "\n",
    "        # printing the model loss and classification performance on the k-th fold\n",
    "        print('\\nFold ',j)\n",
    "        print(\"\\nValidation accuracy: \", val_acc)\n",
    "        print(\"Validation loss: \", val_loss)\n",
    "\n",
    "        print(\"\\nClassification report: \")\n",
    "\n",
    "        # Obtaining the validation samples class probabilities\n",
    "        y_prob = model.predict(X_valid_cv)\n",
    "        # Obtaining the binary label from the model output probabilities\n",
    "        y_hat = (y_prob > 0.5).astype(int)\n",
    "\n",
    "        # Printing the classification performance report\n",
    "        report = classification_report(X_valid_cv, y_valid_cv, target_names=['sober', 'drunk'])\n",
    "        print(report)\n",
    "    \n",
    "        print(\"\\nConfusion Matrix: \")\n",
    "\n",
    "        # Printing the confusion matrix\n",
    "        matrix = confusion_matrix(y_valid_cv, y_hat)\n",
    "        print(matrix)\n",
    "    \n",
    "        tn, fp, fn, tp = matrix.ravel()\n",
    "        print(\"\\nTrue Negatives: \", tn)\n",
    "        print(\"False Positives: \", fp)\n",
    "        print(\"False Negatives: \", fn)\n",
    "        print(\"True Positives: \", tp)\n",
    "        \n",
    "        # Displaying the training process history\n",
    "        visualize_model_history(history)\n",
    "\n",
    "    # Printing the model classification accuracy on each fold of the stratified \n",
    "    # cross-validation\n",
    "    print(\"\\nK-Fold accuracy: \", fold_acc)\n",
    "    \n",
    "    # Printing the model average accuracy along the stratified k-fold cross-validation \n",
    "    # procedure\n",
    "    print(\"\\nAverage accuracy: \", np.mean(fold_acc))\n",
    "    \n",
    "    # Printing the standard deviation of accuracies obtained throughout the \n",
    "    # stratified k-fold cross-validation procedure\n",
    "    print(\"K-Fold Standard deviation: \", np.std(fold_acc))\n",
    "\n",
    "\n",
    "def fine_tuning():\n",
    "    \"\"\"\n",
    "    Fine-tunes the base drunkenness classification model deepest convolutional \\\\ \n",
    "    layers.\n",
    "\n",
    "    This function loads the previously trained drunkenness classification model, \\\\\n",
    "    unfreezes the deepestes convolutional layers weights and retrains the model \\\\ \n",
    "    in order to induce such layers to extract high-level drunkenness-related \\\\ \n",
    "    features.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Loading the Sober-Drunk Dataset samples\n",
    "    x, y = load_dataset()\n",
    "    \n",
    "    # Defining the training and test subsets using the random train-test split strategy\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=1, stratify=y)\n",
    "    \n",
    "    # Applying the min-max normalization\n",
    "    x_train = min_max_norm(x_train)\n",
    "    x_test = min_max_norm(x_test)\n",
    "    \n",
    "    # Reshaping datsets to the tensor format (channel last)\n",
    "    x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], x_train.shape[2], 3)\n",
    "    x_test = x_test.reshape(x_test.shape[0], x_test.shape[1], x_test.shape[2], 3)\n",
    "\n",
    "    # Defining the data augmentation generator\n",
    "    aug = ImageDataGenerator(rotation_range=20, zoom_range=0.15, width_shift_range=0.2,\n",
    "\t\t\t\t\t\t\t height_shift_range=0.2, shear_range=0.15, horizontal_flip=False, fill_mode=\"constant\")\n",
    "    \n",
    "    # Defining a functional model for fine-tuning\n",
    "    model = build_model()\n",
    "    \n",
    "    # Fitting the model\n",
    "    history = model.fit(aug.flow(x_train, y_train, batch_size=60, shuffle=False, seed=1),\n",
    "\t\t\t\t\t\tsteps_per_epoch=len(x_train) // 60,\n",
    "\t\t\t\t\t\tvalidation_data=(x_test, y_test),\n",
    "\t\t\t\t\t\tshuffle=False,\n",
    "\t\t\t\t\t\tepochs=400,\n",
    "\t\t\t\t\t\tverbose=1)\n",
    "    \n",
    "    # Evaluating the model on unseen data\n",
    "    test_loss, test_acc = model.evaluate(x_test, y_test, verbose=1)\n",
    "    \n",
    "    # Saving the fine-tunned base drunkenness classification model\n",
    "    model.save('sober-drunk-cc_vgg16_ft-model_data_aug_tts_70-30.h5')\n",
    "    \n",
    "    print(\"\\nClassification report: \")\n",
    "    \n",
    "    # Obtaining the test samples class probabilities\n",
    "    y_prob = model.predict(x_test)\n",
    "    # Obtaining the binary label from the model output probabilities\n",
    "    y_hat = (y_prob > 0.5).astype(int)\n",
    "    \n",
    "    # Printing the classification performance report\n",
    "    report = classification_report(y_test, y_hat, target_names=['sober', 'drunk'])\n",
    "    print(report)\n",
    "    \n",
    "    print(\"\\nConfusion Matrix: \")\n",
    "\n",
    "    # Printing the confusion matrix\n",
    "    matrix = confusion_matrix(y_test, y_hat)\n",
    "    print(matrix)\n",
    "    \n",
    "    tn, fp, fn, tp = matrix.ravel()\n",
    "    print(\"\\nTrue Negatives: \", tn)\n",
    "    print(\"False Positives: \", fp)\n",
    "    print(\"False Negatives: \", fn)\n",
    "    print(\"True Positives: \", tp)\n",
    "    \n",
    "    # Displaying the training process history\n",
    "    visualize_model_history(history)\n",
    "\n",
    "\n",
    "# Running the fine-tuning\n",
    "fine_tuning()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1632.210776,
   "end_time": "2023-03-15T02:10:56.935385",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-03-15T01:43:44.724609",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
