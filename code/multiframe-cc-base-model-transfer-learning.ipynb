{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9620a687",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-03-14T17:14:21.700608Z",
     "iopub.status.busy": "2023-03-14T17:14:21.700000Z",
     "iopub.status.idle": "2023-03-14T17:24:59.476031Z",
     "shell.execute_reply": "2023-03-14T17:24:59.474736Z"
    },
    "papermill": {
     "duration": 638.017398,
     "end_time": "2023-03-14T17:24:59.713269",
     "exception": false,
     "start_time": "2023-03-14T17:14:21.695871",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Defining random seeds to enable reproducibility\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(1)\n",
    " \n",
    "import random\n",
    "random.seed(1)\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.python.client import device_lib\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "print(device_lib.list_local_devices())\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "\n",
    "def load_dataset():\n",
    "    \"\"\"\n",
    "    Loads the datasets encoded in .pkl files and returns its decoded form.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        A list of n-dimensional arrays representing the subjects samples that will be used to \\\\\n",
    "        train the drunkenness classification model.\n",
    "    ndarray\n",
    "        A n-dimensional array representing the samples labels.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Loading Sober-Drunk Face Dataset, from Patras University\")\n",
    "    \n",
    "    # Defining the sample and label sets filenames\n",
    "    sets = [\n",
    "        \"Insert the x_balanced.pkl file path here\",\n",
    "        \"Insert the y_balanced.pkl file path here\"   \n",
    "    ]\n",
    "    \n",
    "    # Defining an empty list for storing the decoded dataset\n",
    "    loaded_datasets = []\n",
    " \n",
    "    # Iterating over the dataset files\n",
    "    for set_ in sets:\n",
    "        # Opening the .pkl file in read mode\n",
    "        with open(set_, 'rb') as file:\n",
    "            # Appending the decoded dataset to the dataset list\n",
    "            loaded_datasets.append(pickle.load(file))\n",
    "    \n",
    "    # Unpacking the dataset list into individual subsets\n",
    "    x, y = loaded_datasets\n",
    "    \n",
    "    # Converting the label list to the n-dimensional array format\n",
    "    y_arr= np.array(y)\n",
    "    \n",
    "    # Printing the dataset length for sanity check\n",
    "    print(\"\\nSamples total: {0}\".format(len(x)))   \n",
    "    \n",
    "    # Slicing the frame sequences of each subject for selecting the\n",
    "    # frames sampled at each 5 Hz\n",
    "    x = x[::5]\n",
    "    y_arr = y_arr[::5]\n",
    "    \n",
    "    # Printing the dataset length after slicing for sanity check\n",
    "    print(\"\\nSamples total after slicing: {0}\".format(len(x)))\n",
    "    \n",
    "    # Returning the samples set and its respective labels\n",
    "    return x, y_arr\n",
    "\n",
    "\n",
    "def min_max_norm(dataset):\n",
    "    \"\"\"\n",
    "    Normalizes the keyframes according to the minimum-maximum norm, \\\\\n",
    "    such that pixel values ranges from 0 to 1.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : list\n",
    "        A list of n-dimensional arrays representing the subjects keyframes.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ndarray\n",
    "        A n-dimensional array representing keyframes with pixel values ranging from 0 to 1.\n",
    "    \"\"\"\n",
    "\n",
    "    # Converting the dataset type from list to n-dimensional array\n",
    "    dataset = np.asarray(dataset, dtype=\"int16\")\n",
    "\n",
    "    # Finding the keyframes minimum and maximum values\n",
    "    x_min = dataset.min(axis=(1, 2), keepdims=True)\n",
    "    x_max = dataset.max(axis=(1, 2), keepdims=True)\n",
    "\n",
    "    # Applying the minimum-maximum norm to each keyframe\n",
    "    norm_dataset = (dataset - x_min) / (x_max - x_min)\n",
    "\n",
    "    # Printing the minimum and maximum values from a given sample for sanity check\n",
    "    print(\"\\nMinMax normalization\")\n",
    "    print(\"dataset shape: \", norm_dataset.shape)\n",
    "    print(\"min: \", norm_dataset[0].min())\n",
    "    print(\"max: \", norm_dataset[0].max())\n",
    "\n",
    "    # Returning the normalized dataset\n",
    "    return norm_dataset\n",
    "\n",
    "\n",
    "def visualize_model_history(hist):\n",
    "    \"\"\"\n",
    "    Displays the training process loss and accuracy history.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    hist : dictionary\n",
    "        A dictionary conataining loss and accuracy values lists from the training and \\\\\n",
    "        test sets.\n",
    "    \"\"\"\n",
    "\n",
    "    # Displaying and saving the training and test loss history\n",
    "    plt.figure()\n",
    "    plt.plot(hist.history['loss'], linewidth=2, alpha=0.85)\n",
    "    plt.plot(hist.history['val_loss'], linewidth=2, alpha=0.85)\n",
    "    sns.despine()\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper right', frameon=False)\n",
    "    plt.savefig(\"loss-curve.pdf\", dpi=600, bbox_inches='tight', pad_inches=0.1)\n",
    "    \n",
    "    # Displaying and saving the training and test accuracy history\n",
    "    plt.figure()\n",
    "    plt.plot(hist.history['accuracy'], linewidth=2, alpha=0.85)\n",
    "    plt.plot(hist.history['val_accuracy'], linewidth=2, alpha=0.85)\n",
    "    sns.despine()\n",
    "    plt.title('Model accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='lower right', frameon=False)\n",
    "    plt.savefig(\"accuracy-curve.pdf\", dpi=600, bbox_inches='tight', pad_inches=0.1)\n",
    "    \n",
    "    # Showing the training process loss and accuracy history\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def build_model(x_train):\n",
    "    \"\"\"\n",
    "    Builds a keras sequential model and returns it.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x_train : ndarray\n",
    "        A n-dimensional array representing the training samples.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Sequential\n",
    "        A single layer binary classification model.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Finding the thermal images shape\n",
    "    imsize = (x_train.shape[1],x_train.shape[2],x_train.shape[3])\n",
    "\n",
    "    # Defining the pre-trained model new input layer shape\n",
    "    inp = keras.layers.Input(shape=(imsize[0], imsize[1], imsize[2]))\n",
    "\n",
    "    # Defining a VGG pre-trained model with new input shape and without dense layers\n",
    "    base_model = keras.applications.VGG16(include_top=False, weights='imagenet', input_tensor=inp,\n",
    "                                          input_shape=(imsize[0], imsize[1], imsize[2]))\n",
    "    \n",
    "    # Defining the VGG model output as the last pooling layer feature maps\n",
    "    output = base_model.layers[-1].output\n",
    "    \n",
    "    # Changing the model 'trainable' parameter to False,\n",
    "    # this will prevent the base model from updating its\n",
    "    # weights during the new dense layers training process\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    \n",
    "    # Iterating over the model layers\n",
    "    for layer in base_model.layers:\n",
    "        # Freezing the layers weights\n",
    "        layer.trainable = False\n",
    "        # Printing the layers 'trainable' parameter for sanity check\n",
    "        print(\"{}: {}\".format(layer.name, layer.trainable))\n",
    "    \n",
    "    # Adding a Flatten layer to the base model output\n",
    "    flat = Flatten(name='flatten')(output)\n",
    "    \n",
    "    # Defining an output layer to classify the received features as sober (0) or drunk (1)\n",
    "    prediction = keras.layers.Dense(units=1, activation='sigmoid', name='output')\n",
    "    \n",
    "    # Stacking the base model convolutional layers and the new dense layer for \n",
    "    # drunkenness classification\n",
    "    x = flat\n",
    "    x = prediction(x)\n",
    "    \n",
    "    # Defining a functional model for transfer learning\n",
    "    model = models.Model(inp, x)\n",
    "    \n",
    "    # Defining the optmization function\n",
    "    adam = optimizers.Adam(learning_rate=1e-5, amsgrad=False)\n",
    "    \n",
    "    # Compiling the model\n",
    "    model.compile(loss=keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "                  optimizer=adam,\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    # Printing the model summary\n",
    "    model.summary()\n",
    "    \n",
    "    # Returning the sequential model\n",
    "    return model\n",
    "\n",
    "\n",
    "def transfer_learning():\n",
    "    \"\"\"\n",
    "    Performs the transfer learning strategy considering a base model pre-trained \\\\ \n",
    "    with a large scale image dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    # Loading the Sober-Drunk Dataset samples\n",
    "    x, y = load_dataset()\n",
    "    \n",
    "    # Defining the training and test subsets using the random train-test split strategy\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=1, stratify=y)\n",
    "      \n",
    "    # Applying the min-max normalization\n",
    "    x_train = min_max_norm(x_train)\n",
    "    x_test = min_max_norm(x_test)\n",
    "\n",
    "    # Reshaping datsets to the tensor format (channel last)\n",
    "    x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], x_train.shape[2], 3)\n",
    "    x_test = x_test.reshape(x_test.shape[0], x_test.shape[1], x_test.shape[2], 3)\n",
    "    \n",
    "    # Defining a functional model for transfer learning       \n",
    "    model = build_model(x_train)\n",
    "    \n",
    "    # Fitting the model\n",
    "    history = model.fit(x=x_train, y=y_train,\n",
    "                        validation_data=(x_test, y_test),\n",
    "                        shuffle=False,\n",
    "                        batch_size=60,\n",
    "                        epochs=400,   \n",
    "                        verbose=1)\n",
    "    \n",
    "    # Evaluating the model on unseen data\n",
    "    test_loss, test_acc = model.evaluate(x_test, y_test, verbose=1)\n",
    "    \n",
    "    # Saving the obtained base drunkenness classification model\n",
    "    model.save('sober-drunk-cc_vgg16_tl-model_tts_70-30.h5')\n",
    "    \n",
    "    print(\"\\nClassification report: \")\n",
    "    \n",
    "    # Obtaining the test samples class probabilities\n",
    "    y_prob = model.predict(x_test)\n",
    "    # Obtaining the binary label from the model output probabilities\n",
    "    y_hat = (y_prob > 0.5).astype(int)\n",
    "    \n",
    "    # Printing the classification performance report\n",
    "    report = classification_report(y_test, y_hat, target_names=['sober', 'drunk'])\n",
    "    print(report)\n",
    "    \n",
    "    print(\"\\nConfusion Matrix: \")\n",
    "\n",
    "    # Printing the confusion matrix\n",
    "    matrix = confusion_matrix(y_test, y_hat)\n",
    "    print(matrix)\n",
    "    \n",
    "    tn, fp, fn, tp = matrix.ravel()\n",
    "    print(\"\\nTrue Negatives: \", tn)\n",
    "    print(\"False Positives: \", fp)\n",
    "    print(\"False Negatives: \", fn)\n",
    "    print(\"True Positives: \", tp)\n",
    "    \n",
    "    # Displaying the training process history\n",
    "    visualize_model_history(history)\n",
    "\n",
    "\n",
    "# Running the transfer learning\n",
    "transfer_learning()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 650.787919,
   "end_time": "2023-03-14T17:25:03.841671",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-03-14T17:14:13.053752",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
