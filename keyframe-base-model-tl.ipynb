{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9750da9",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-05-24T19:01:57.691334Z",
     "iopub.status.busy": "2022-05-24T19:01:57.690906Z",
     "iopub.status.idle": "2022-05-24T19:03:52.477499Z",
     "shell.execute_reply": "2022-05-24T19:03:52.476784Z"
    },
    "papermill": {
     "duration": 114.795067,
     "end_time": "2022-05-24T19:03:52.479794",
     "exception": false,
     "start_time": "2022-05-24T19:01:57.684727",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-24 19:02:04.994452: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-05-24 19:02:05.055566: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-24 19:02:05.211856: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-24 19:02:05.212776: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-24 19:02:07.534408: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-24 19:02:07.535331: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-24 19:02:07.536117: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-24 19:02:07.537638: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /device:GPU:0 with 15403 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n",
      "2022-05-24 19:02:07.544918: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-24 19:02:07.545827: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-24 19:02:07.546569: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 11131005432198956786\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 16152002560\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 3926290696750968882\n",
      "physical_device_desc: \"device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\"\n",
      "]\n",
      "Num GPUs Available:  1\n",
      "Loading Sober-Drunk Face Dataset, from Patras University\n",
      "\n",
      "Samples total: 80\n",
      "\n",
      "Dataset splitting: \n",
      "\n",
      "Training set: 70\n",
      "Test set: 10\n",
      "\n",
      "MinMax normalization\n",
      "dataset shape:  (70, 128, 160, 3)\n",
      "min:  0.0\n",
      "max:  1.0\n",
      "\n",
      "MinMax normalization\n",
      "dataset shape:  (10, 128, 160, 3)\n",
      "min:  0.0\n",
      "max:  1.0\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "   16384/58889256 [..............................] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-24 19:02:07.897162: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-24 19:02:07.898133: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-24 19:02:07.898798: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-24 19:02:07.899515: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-24 19:02:07.900206: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-24 19:02:07.900888: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15403 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58892288/58889256 [==============================] - 0s 0us/step\n",
      "58900480/58889256 [==============================] - 0s 0us/step\n",
      "input_1: False\n",
      "block1_conv1: False\n",
      "block1_conv2: False\n",
      "block1_pool: False\n",
      "block2_conv1: False\n",
      "block2_conv2: False\n",
      "block2_pool: False\n",
      "block3_conv1: False\n",
      "block3_conv2: False\n",
      "block3_conv3: False\n",
      "block3_pool: False\n",
      "block4_conv1: False\n",
      "block4_conv2: False\n",
      "block4_conv3: False\n",
      "block4_pool: False\n",
      "block5_conv1: False\n",
      "block5_conv2: False\n",
      "block5_conv3: False\n",
      "block5_pool: False\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 128, 160, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 128, 160, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 128, 160, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 64, 80, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 64, 80, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 64, 80, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 32, 40, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 32, 40, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 32, 40, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 32, 40, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 16, 20, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 16, 20, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 16, 20, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 16, 20, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 8, 10, 512)        0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 8, 10, 512)        2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 8, 10, 512)        2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 8, 10, 512)        2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 4, 5, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 10240)             0         \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 1)                 10241     \n",
      "=================================================================\n",
      "Total params: 14,724,929\n",
      "Trainable params: 10,241\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n",
      "\n",
      "\n",
      "\n",
      "Fold  0\n",
      "\n",
      "Training with 56 samples and validating with 14 samples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-24 19:02:09.131379: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-24 19:02:10.954574: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 11s 2s/step - loss: 0.7120 - accuracy: 0.5179 - val_loss: 0.7572 - val_accuracy: 0.5000\n",
      "Epoch 2/205\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.7087 - accuracy: 0.5357 - val_loss: 0.7554 - val_accuracy: 0.5000\n",
      "Epoch 3/205\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.7056 - accuracy: 0.5357 - val_loss: 0.7537 - val_accuracy: 0.4286\n",
      "Epoch 4/205\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.7027 - accuracy: 0.5536 - val_loss: 0.7521 - val_accuracy: 0.2857\n",
      "Epoch 5/205\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.7000 - accuracy: 0.5357 - val_loss: 0.7507 - val_accuracy: 0.2857\n",
      "Epoch 6/205\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.6975 - accuracy: 0.5357 - val_loss: 0.7495 - val_accuracy: 0.2143\n",
      "Epoch 7/205\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.6952 - accuracy: 0.5357 - val_loss: 0.7484 - val_accuracy: 0.2143\n",
      "Epoch 8/205\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.6930 - accuracy: 0.5179 - val_loss: 0.7474 - val_accuracy: 0.2143\n",
      "Epoch 9/205\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.6910 - accuracy: 0.5179 - val_loss: 0.7466 - val_accuracy: 0.2143\n",
      "Epoch 10/205\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.6892 - accuracy: 0.5179 - val_loss: 0.7458 - val_accuracy: 0.2143\n",
      "Epoch 11/205\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.6876 - accuracy: 0.5179 - val_loss: 0.7453 - val_accuracy: 0.2143\n",
      "Epoch 12/205\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.6860 - accuracy: 0.5179 - val_loss: 0.7448 - val_accuracy: 0.2143\n",
      "Epoch 13/205\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.6847 - accuracy: 0.5536 - val_loss: 0.7444 - val_accuracy: 0.1429\n",
      "Epoch 14/205\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.6834 - accuracy: 0.5714 - val_loss: 0.7441 - val_accuracy: 0.1429\n",
      "Epoch 15/205\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.6823 - accuracy: 0.5714 - val_loss: 0.7439 - val_accuracy: 0.1429\n",
      "Epoch 16/205\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.6812 - accuracy: 0.5714 - val_loss: 0.7437 - val_accuracy: 0.2143\n",
      "Epoch 17/205\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.6802 - accuracy: 0.5893 - val_loss: 0.7436 - val_accuracy: 0.2857\n",
      "Epoch 18/205\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.6793 - accuracy: 0.5893 - val_loss: 0.7436 - val_accuracy: 0.2857\n",
      "Epoch 19/205\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.6785 - accuracy: 0.5893 - val_loss: 0.7435 - val_accuracy: 0.3571\n",
      "Epoch 20/205\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.6777 - accuracy: 0.5893 - val_loss: 0.7436 - val_accuracy: 0.3571\n",
      "Epoch 21/205\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.6769 - accuracy: 0.5714 - val_loss: 0.7436 - val_accuracy: 0.3571\n",
      "Epoch 22/205\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.6762 - accuracy: 0.5714 - val_loss: 0.7437 - val_accuracy: 0.3571\n",
      "Epoch 23/205\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.6755 - accuracy: 0.5893 - val_loss: 0.7438 - val_accuracy: 0.3571\n",
      "Epoch 24/205\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.6748 - accuracy: 0.5714 - val_loss: 0.7438 - val_accuracy: 0.3571\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00024: early stopping\n",
      "1/1 [==============================] - 1s 548ms/step - loss: 0.6287 - accuracy: 0.7000\n",
      "\n",
      "Fold  0\n",
      "\n",
      "Test accuracy:  0.699999988079071\n",
      "Test loss:  0.6287091970443726\n",
      "\n",
      "Fold  1\n",
      "\n",
      "Training with 56 samples and validating with 14 samples\n",
      "\n",
      "Epoch 1/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.7029 - accuracy: 0.5179 - val_loss: 0.6421 - val_accuracy: 0.5714\n",
      "Epoch 2/205\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.7023 - accuracy: 0.5179 - val_loss: 0.6416 - val_accuracy: 0.5714\n",
      "Epoch 3/205\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.7018 - accuracy: 0.5179 - val_loss: 0.6411 - val_accuracy: 0.5714\n",
      "Epoch 4/205\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.7013 - accuracy: 0.5179 - val_loss: 0.6407 - val_accuracy: 0.5714\n",
      "Epoch 5/205\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.7007 - accuracy: 0.5357 - val_loss: 0.6403 - val_accuracy: 0.5714\n",
      "Epoch 6/205\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.7002 - accuracy: 0.5357 - val_loss: 0.6401 - val_accuracy: 0.5714\n",
      "Epoch 7/205\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.6997 - accuracy: 0.5357 - val_loss: 0.6398 - val_accuracy: 0.5714\n",
      "Epoch 8/205\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.6992 - accuracy: 0.5179 - val_loss: 0.6396 - val_accuracy: 0.5714\n",
      "Epoch 9/205\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.6986 - accuracy: 0.5357 - val_loss: 0.6394 - val_accuracy: 0.5714\n",
      "Epoch 10/205\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.6981 - accuracy: 0.5536 - val_loss: 0.6392 - val_accuracy: 0.5714\n",
      "Epoch 11/205\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6976 - accuracy: 0.5714 - val_loss: 0.6390 - val_accuracy: 0.5714\n",
      "Epoch 12/205\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.6971 - accuracy: 0.5714 - val_loss: 0.6389 - val_accuracy: 0.5714\n",
      "Epoch 13/205\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.6966 - accuracy: 0.5714 - val_loss: 0.6387 - val_accuracy: 0.5714\n",
      "Epoch 14/205\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.6961 - accuracy: 0.5714 - val_loss: 0.6386 - val_accuracy: 0.5714\n",
      "Epoch 15/205\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.6956 - accuracy: 0.5714 - val_loss: 0.6385 - val_accuracy: 0.5714\n",
      "Epoch 16/205\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6950 - accuracy: 0.5714 - val_loss: 0.6383 - val_accuracy: 0.5714\n",
      "Epoch 17/205\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.6945 - accuracy: 0.5714 - val_loss: 0.6382 - val_accuracy: 0.5714\n",
      "Epoch 18/205\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.6940 - accuracy: 0.5714 - val_loss: 0.6381 - val_accuracy: 0.5714\n",
      "Epoch 19/205\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6935 - accuracy: 0.5714 - val_loss: 0.6380 - val_accuracy: 0.5714\n",
      "Epoch 20/205\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.6930 - accuracy: 0.5714 - val_loss: 0.6379 - val_accuracy: 0.5714\n",
      "Epoch 21/205\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.6925 - accuracy: 0.5714 - val_loss: 0.6377 - val_accuracy: 0.5714\n",
      "Epoch 22/205\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.6920 - accuracy: 0.5714 - val_loss: 0.6376 - val_accuracy: 0.5714\n",
      "Epoch 23/205\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.6915 - accuracy: 0.5714 - val_loss: 0.6375 - val_accuracy: 0.5714\n",
      "Epoch 24/205\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.6910 - accuracy: 0.5893 - val_loss: 0.6374 - val_accuracy: 0.5714\n",
      "Epoch 25/205\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.6905 - accuracy: 0.5893 - val_loss: 0.6373 - val_accuracy: 0.5714\n",
      "Epoch 26/205\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.6900 - accuracy: 0.5893 - val_loss: 0.6372 - val_accuracy: 0.5714\n",
      "Epoch 27/205\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.6895 - accuracy: 0.6071 - val_loss: 0.6370 - val_accuracy: 0.5714\n",
      "Epoch 28/205\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.6890 - accuracy: 0.6250 - val_loss: 0.6369 - val_accuracy: 0.5714\n",
      "Epoch 29/205\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6885 - accuracy: 0.6250 - val_loss: 0.6368 - val_accuracy: 0.5714\n",
      "Epoch 30/205\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.6880 - accuracy: 0.6250 - val_loss: 0.6367 - val_accuracy: 0.5714\n",
      "Epoch 31/205\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6875 - accuracy: 0.6250 - val_loss: 0.6366 - val_accuracy: 0.5714\n",
      "Epoch 32/205\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.6870 - accuracy: 0.6250 - val_loss: 0.6364 - val_accuracy: 0.5714\n",
      "Epoch 33/205\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6865 - accuracy: 0.6250 - val_loss: 0.6363 - val_accuracy: 0.5714\n",
      "Epoch 34/205\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.6860 - accuracy: 0.6250 - val_loss: 0.6362 - val_accuracy: 0.5714\n",
      "Epoch 35/205\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6855 - accuracy: 0.6250 - val_loss: 0.6361 - val_accuracy: 0.5714\n",
      "Epoch 36/205\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6850 - accuracy: 0.6250 - val_loss: 0.6360 - val_accuracy: 0.5714\n",
      "Epoch 37/205\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.6845 - accuracy: 0.6250 - val_loss: 0.6358 - val_accuracy: 0.5714\n",
      "Epoch 38/205\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.6840 - accuracy: 0.6250 - val_loss: 0.6357 - val_accuracy: 0.5714\n",
      "Epoch 39/205\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6835 - accuracy: 0.6250 - val_loss: 0.6356 - val_accuracy: 0.5714\n",
      "Epoch 40/205\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.6831 - accuracy: 0.6250 - val_loss: 0.6355 - val_accuracy: 0.5714\n",
      "Epoch 41/205\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.6826 - accuracy: 0.6250 - val_loss: 0.6354 - val_accuracy: 0.5714\n",
      "Epoch 42/205\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6821 - accuracy: 0.6250 - val_loss: 0.6353 - val_accuracy: 0.5714\n",
      "Epoch 43/205\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.6816 - accuracy: 0.6250 - val_loss: 0.6351 - val_accuracy: 0.5714\n",
      "Epoch 44/205\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6811 - accuracy: 0.6250 - val_loss: 0.6350 - val_accuracy: 0.5714\n",
      "Epoch 45/205\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.6806 - accuracy: 0.6250 - val_loss: 0.6349 - val_accuracy: 0.5714\n",
      "Epoch 46/205\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6801 - accuracy: 0.6250 - val_loss: 0.6348 - val_accuracy: 0.5714\n",
      "Epoch 47/205\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.6796 - accuracy: 0.6250 - val_loss: 0.6347 - val_accuracy: 0.5714\n",
      "Epoch 48/205\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.6791 - accuracy: 0.6250 - val_loss: 0.6345 - val_accuracy: 0.5714\n",
      "Epoch 49/205\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.6786 - accuracy: 0.6607 - val_loss: 0.6344 - val_accuracy: 0.5714\n",
      "Epoch 50/205\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.6781 - accuracy: 0.6607 - val_loss: 0.6343 - val_accuracy: 0.5714\n",
      "Epoch 51/205\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.6777 - accuracy: 0.6607 - val_loss: 0.6342 - val_accuracy: 0.5714\n",
      "Epoch 52/205\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6772 - accuracy: 0.6607 - val_loss: 0.6340 - val_accuracy: 0.5714\n",
      "Epoch 53/205\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.6767 - accuracy: 0.6607 - val_loss: 0.6339 - val_accuracy: 0.5714\n",
      "Epoch 54/205\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.6762 - accuracy: 0.6607 - val_loss: 0.6338 - val_accuracy: 0.5714\n",
      "Epoch 55/205\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.6757 - accuracy: 0.6607 - val_loss: 0.6337 - val_accuracy: 0.5714\n",
      "Epoch 56/205\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.6752 - accuracy: 0.6607 - val_loss: 0.6336 - val_accuracy: 0.5714\n",
      "Epoch 57/205\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.6747 - accuracy: 0.6607 - val_loss: 0.6334 - val_accuracy: 0.5714\n",
      "Epoch 58/205\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.6743 - accuracy: 0.6607 - val_loss: 0.6333 - val_accuracy: 0.5714\n",
      "Epoch 59/205\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.6738 - accuracy: 0.6607 - val_loss: 0.6332 - val_accuracy: 0.5714\n",
      "Epoch 60/205\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.6733 - accuracy: 0.6607 - val_loss: 0.6331 - val_accuracy: 0.5714\n",
      "Epoch 61/205\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.6728 - accuracy: 0.6607 - val_loss: 0.6329 - val_accuracy: 0.5714\n",
      "Epoch 62/205\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.6723 - accuracy: 0.6607 - val_loss: 0.6328 - val_accuracy: 0.5714\n",
      "Epoch 63/205\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.6718 - accuracy: 0.6607 - val_loss: 0.6327 - val_accuracy: 0.5714\n",
      "Epoch 64/205\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.6714 - accuracy: 0.6607 - val_loss: 0.6326 - val_accuracy: 0.5714\n",
      "Epoch 65/205\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.6709 - accuracy: 0.6607 - val_loss: 0.6324 - val_accuracy: 0.5714\n",
      "Epoch 66/205\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.6704 - accuracy: 0.6607 - val_loss: 0.6323 - val_accuracy: 0.5714\n",
      "Epoch 67/205\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.6699 - accuracy: 0.6607 - val_loss: 0.6322 - val_accuracy: 0.5714\n",
      "Epoch 68/205\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.6694 - accuracy: 0.6607 - val_loss: 0.6321 - val_accuracy: 0.5714\n",
      "Epoch 69/205\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6690 - accuracy: 0.6607 - val_loss: 0.6319 - val_accuracy: 0.5714\n",
      "Epoch 70/205\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.6685 - accuracy: 0.6607 - val_loss: 0.6318 - val_accuracy: 0.5714\n",
      "Epoch 71/205\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.6680 - accuracy: 0.6607 - val_loss: 0.6317 - val_accuracy: 0.5714\n",
      "Epoch 72/205\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.6675 - accuracy: 0.6607 - val_loss: 0.6315 - val_accuracy: 0.5714\n",
      "Epoch 73/205\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.6670 - accuracy: 0.6607 - val_loss: 0.6314 - val_accuracy: 0.5714\n",
      "Epoch 74/205\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6666 - accuracy: 0.6607 - val_loss: 0.6313 - val_accuracy: 0.5714\n",
      "Epoch 75/205\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.6661 - accuracy: 0.6786 - val_loss: 0.6312 - val_accuracy: 0.5714\n",
      "Epoch 76/205\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.6656 - accuracy: 0.6786 - val_loss: 0.6310 - val_accuracy: 0.5714\n",
      "Epoch 77/205\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.6651 - accuracy: 0.6786 - val_loss: 0.6309 - val_accuracy: 0.5714\n",
      "Epoch 78/205\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.6647 - accuracy: 0.6786 - val_loss: 0.6308 - val_accuracy: 0.5714\n",
      "Epoch 79/205\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6642 - accuracy: 0.6786 - val_loss: 0.6306 - val_accuracy: 0.5714\n",
      "Epoch 80/205\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.6637 - accuracy: 0.6786 - val_loss: 0.6305 - val_accuracy: 0.5714\n",
      "Epoch 81/205\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6633 - accuracy: 0.6786 - val_loss: 0.6304 - val_accuracy: 0.5714\n",
      "Epoch 82/205\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.6628 - accuracy: 0.6786 - val_loss: 0.6303 - val_accuracy: 0.5714\n",
      "Epoch 83/205\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.6623 - accuracy: 0.6786 - val_loss: 0.6301 - val_accuracy: 0.6429\n",
      "Epoch 84/205\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.6618 - accuracy: 0.6786 - val_loss: 0.6300 - val_accuracy: 0.6429\n",
      "Epoch 85/205\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.6614 - accuracy: 0.6786 - val_loss: 0.6299 - val_accuracy: 0.6429\n",
      "Epoch 86/205\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.6609 - accuracy: 0.6786 - val_loss: 0.6297 - val_accuracy: 0.6429\n",
      "Epoch 87/205\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.6604 - accuracy: 0.6964 - val_loss: 0.6296 - val_accuracy: 0.6429\n",
      "Epoch 88/205\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.6600 - accuracy: 0.6964 - val_loss: 0.6295 - val_accuracy: 0.5714\n",
      "Epoch 89/205\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6595 - accuracy: 0.6964 - val_loss: 0.6294 - val_accuracy: 0.5714\n",
      "Epoch 90/205\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.6590 - accuracy: 0.6964 - val_loss: 0.6292 - val_accuracy: 0.5714\n",
      "Epoch 91/205\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6586 - accuracy: 0.6964 - val_loss: 0.6291 - val_accuracy: 0.5714\n",
      "Epoch 92/205\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.6581 - accuracy: 0.6964 - val_loss: 0.6290 - val_accuracy: 0.5714\n",
      "Epoch 93/205\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.6576 - accuracy: 0.7143 - val_loss: 0.6288 - val_accuracy: 0.5714\n",
      "Epoch 94/205\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.6572 - accuracy: 0.7143 - val_loss: 0.6287 - val_accuracy: 0.5714\n",
      "Epoch 95/205\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.6567 - accuracy: 0.7143 - val_loss: 0.6286 - val_accuracy: 0.5714\n",
      "Epoch 96/205\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.6562 - accuracy: 0.7143 - val_loss: 0.6284 - val_accuracy: 0.5714\n",
      "Epoch 97/205\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.6558 - accuracy: 0.7143 - val_loss: 0.6283 - val_accuracy: 0.5714\n",
      "Epoch 98/205\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.6553 - accuracy: 0.7143 - val_loss: 0.6282 - val_accuracy: 0.5714\n",
      "Epoch 99/205\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.6548 - accuracy: 0.7143 - val_loss: 0.6280 - val_accuracy: 0.5714\n",
      "Epoch 100/205\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6544 - accuracy: 0.7143 - val_loss: 0.6279 - val_accuracy: 0.5714\n",
      "Epoch 101/205\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.6539 - accuracy: 0.7143 - val_loss: 0.6278 - val_accuracy: 0.5714\n",
      "Epoch 102/205\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.6534 - accuracy: 0.7143 - val_loss: 0.6276 - val_accuracy: 0.5714\n",
      "Epoch 103/205\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6530 - accuracy: 0.7321 - val_loss: 0.6275 - val_accuracy: 0.5714\n",
      "Epoch 104/205\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.6525 - accuracy: 0.7321 - val_loss: 0.6274 - val_accuracy: 0.5714\n",
      "Epoch 105/205\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.6521 - accuracy: 0.7321 - val_loss: 0.6272 - val_accuracy: 0.5714\n",
      "Epoch 106/205\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.6516 - accuracy: 0.7321 - val_loss: 0.6271 - val_accuracy: 0.5714\n",
      "Epoch 107/205\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.6511 - accuracy: 0.7321 - val_loss: 0.6270 - val_accuracy: 0.5714\n",
      "Epoch 108/205\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.6507 - accuracy: 0.7321 - val_loss: 0.6268 - val_accuracy: 0.5714\n",
      "Epoch 109/205\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.6502 - accuracy: 0.7321 - val_loss: 0.6267 - val_accuracy: 0.5714\n",
      "Epoch 110/205\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.6498 - accuracy: 0.7321 - val_loss: 0.6266 - val_accuracy: 0.5714\n",
      "Epoch 111/205\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.6493 - accuracy: 0.7321 - val_loss: 0.6264 - val_accuracy: 0.5714\n",
      "Epoch 112/205\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6488 - accuracy: 0.7321 - val_loss: 0.6263 - val_accuracy: 0.5714\n",
      "Epoch 113/205\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.6484 - accuracy: 0.7321 - val_loss: 0.6262 - val_accuracy: 0.5714\n",
      "Epoch 114/205\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6479 - accuracy: 0.7321 - val_loss: 0.6260 - val_accuracy: 0.5714\n",
      "Epoch 115/205\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.6475 - accuracy: 0.7321 - val_loss: 0.6259 - val_accuracy: 0.5714\n",
      "Epoch 116/205\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6470 - accuracy: 0.7321 - val_loss: 0.6258 - val_accuracy: 0.5714\n",
      "Epoch 117/205\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6466 - accuracy: 0.7321 - val_loss: 0.6256 - val_accuracy: 0.5714\n",
      "Epoch 118/205\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.6461 - accuracy: 0.7321 - val_loss: 0.6255 - val_accuracy: 0.5714\n",
      "Epoch 119/205\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.6457 - accuracy: 0.7321 - val_loss: 0.6253 - val_accuracy: 0.5714\n",
      "Epoch 120/205\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6452 - accuracy: 0.7321 - val_loss: 0.6252 - val_accuracy: 0.5714\n",
      "Epoch 121/205\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.6448 - accuracy: 0.7321 - val_loss: 0.6251 - val_accuracy: 0.5714\n",
      "Epoch 122/205\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6443 - accuracy: 0.7321 - val_loss: 0.6249 - val_accuracy: 0.5714\n",
      "Epoch 123/205\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6439 - accuracy: 0.7321 - val_loss: 0.6248 - val_accuracy: 0.5714\n",
      "Epoch 124/205\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.6434 - accuracy: 0.7321 - val_loss: 0.6247 - val_accuracy: 0.5714\n",
      "Epoch 125/205\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.6430 - accuracy: 0.7321 - val_loss: 0.6245 - val_accuracy: 0.5714\n",
      "Epoch 126/205\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.6425 - accuracy: 0.7321 - val_loss: 0.6244 - val_accuracy: 0.5714\n",
      "Epoch 127/205\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.6421 - accuracy: 0.7321 - val_loss: 0.6243 - val_accuracy: 0.5714\n",
      "Epoch 128/205\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6416 - accuracy: 0.7321 - val_loss: 0.6241 - val_accuracy: 0.5714\n",
      "Epoch 129/205\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.6412 - accuracy: 0.7321 - val_loss: 0.6240 - val_accuracy: 0.5714\n",
      "Epoch 130/205\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.6407 - accuracy: 0.7321 - val_loss: 0.6238 - val_accuracy: 0.5714\n",
      "Epoch 131/205\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6403 - accuracy: 0.7321 - val_loss: 0.6237 - val_accuracy: 0.5714\n",
      "Epoch 132/205\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.6398 - accuracy: 0.7321 - val_loss: 0.6236 - val_accuracy: 0.5714\n",
      "Epoch 133/205\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.6394 - accuracy: 0.7321 - val_loss: 0.6234 - val_accuracy: 0.5714\n",
      "Epoch 134/205\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6389 - accuracy: 0.7321 - val_loss: 0.6233 - val_accuracy: 0.5714\n",
      "Epoch 135/205\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.6385 - accuracy: 0.7321 - val_loss: 0.6232 - val_accuracy: 0.5714\n",
      "Epoch 136/205\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6380 - accuracy: 0.7321 - val_loss: 0.6230 - val_accuracy: 0.5714\n",
      "Epoch 137/205\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.6376 - accuracy: 0.7321 - val_loss: 0.6229 - val_accuracy: 0.5714\n",
      "Epoch 138/205\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.6371 - accuracy: 0.7321 - val_loss: 0.6227 - val_accuracy: 0.5714\n",
      "Epoch 139/205\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.6367 - accuracy: 0.7321 - val_loss: 0.6226 - val_accuracy: 0.5714\n",
      "Epoch 140/205\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.6363 - accuracy: 0.7321 - val_loss: 0.6225 - val_accuracy: 0.5714\n",
      "Epoch 141/205\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.6358 - accuracy: 0.7321 - val_loss: 0.6223 - val_accuracy: 0.5714\n",
      "Epoch 142/205\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.6354 - accuracy: 0.7321 - val_loss: 0.6222 - val_accuracy: 0.6429\n",
      "Epoch 143/205\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.6349 - accuracy: 0.7321 - val_loss: 0.6221 - val_accuracy: 0.6429\n",
      "Epoch 144/205\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.6345 - accuracy: 0.7321 - val_loss: 0.6219 - val_accuracy: 0.6429\n",
      "Epoch 145/205\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.6341 - accuracy: 0.7321 - val_loss: 0.6218 - val_accuracy: 0.6429\n",
      "Epoch 146/205\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6336 - accuracy: 0.7321 - val_loss: 0.6216 - val_accuracy: 0.6429\n",
      "Epoch 147/205\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6332 - accuracy: 0.7321 - val_loss: 0.6215 - val_accuracy: 0.6429\n",
      "Epoch 148/205\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6327 - accuracy: 0.7321 - val_loss: 0.6214 - val_accuracy: 0.6429\n",
      "Epoch 149/205\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.6323 - accuracy: 0.7321 - val_loss: 0.6212 - val_accuracy: 0.6429\n",
      "Epoch 150/205\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.6319 - accuracy: 0.7321 - val_loss: 0.6211 - val_accuracy: 0.6429\n",
      "Epoch 151/205\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.6314 - accuracy: 0.7321 - val_loss: 0.6209 - val_accuracy: 0.6429\n",
      "Epoch 152/205\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.6310 - accuracy: 0.7321 - val_loss: 0.6208 - val_accuracy: 0.6429\n",
      "Epoch 153/205\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.6305 - accuracy: 0.7321 - val_loss: 0.6207 - val_accuracy: 0.6429\n",
      "Epoch 154/205\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6301 - accuracy: 0.7321 - val_loss: 0.6205 - val_accuracy: 0.6429\n",
      "Epoch 155/205\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.6297 - accuracy: 0.7500 - val_loss: 0.6204 - val_accuracy: 0.6429\n",
      "Epoch 156/205\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6292 - accuracy: 0.7679 - val_loss: 0.6202 - val_accuracy: 0.6429\n",
      "Epoch 157/205\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.6288 - accuracy: 0.7679 - val_loss: 0.6201 - val_accuracy: 0.6429\n",
      "Epoch 158/205\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.6284 - accuracy: 0.7679 - val_loss: 0.6200 - val_accuracy: 0.6429\n",
      "Epoch 159/205\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6279 - accuracy: 0.7857 - val_loss: 0.6198 - val_accuracy: 0.6429\n",
      "Epoch 160/205\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.6275 - accuracy: 0.7857 - val_loss: 0.6197 - val_accuracy: 0.6429\n",
      "Epoch 161/205\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.6271 - accuracy: 0.7857 - val_loss: 0.6195 - val_accuracy: 0.6429\n",
      "Epoch 162/205\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.6266 - accuracy: 0.8036 - val_loss: 0.6194 - val_accuracy: 0.6429\n",
      "Epoch 163/205\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.6262 - accuracy: 0.7857 - val_loss: 0.6193 - val_accuracy: 0.6429\n",
      "Epoch 164/205\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.6258 - accuracy: 0.7857 - val_loss: 0.6191 - val_accuracy: 0.6429\n",
      "Epoch 165/205\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.6253 - accuracy: 0.7857 - val_loss: 0.6190 - val_accuracy: 0.6429\n",
      "Epoch 166/205\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6249 - accuracy: 0.7857 - val_loss: 0.6188 - val_accuracy: 0.6429\n",
      "Epoch 167/205\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.6245 - accuracy: 0.7857 - val_loss: 0.6187 - val_accuracy: 0.6429\n",
      "Epoch 168/205\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6241 - accuracy: 0.7857 - val_loss: 0.6186 - val_accuracy: 0.6429\n",
      "Epoch 169/205\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.6236 - accuracy: 0.7857 - val_loss: 0.6184 - val_accuracy: 0.6429\n",
      "Epoch 170/205\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.6232 - accuracy: 0.7857 - val_loss: 0.6183 - val_accuracy: 0.6429\n",
      "Epoch 171/205\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.6228 - accuracy: 0.7857 - val_loss: 0.6181 - val_accuracy: 0.6429\n",
      "Epoch 172/205\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.6223 - accuracy: 0.7857 - val_loss: 0.6180 - val_accuracy: 0.6429\n",
      "Epoch 173/205\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.6219 - accuracy: 0.7857 - val_loss: 0.6178 - val_accuracy: 0.6429\n",
      "Epoch 174/205\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6215 - accuracy: 0.7857 - val_loss: 0.6177 - val_accuracy: 0.6429\n",
      "Epoch 175/205\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6211 - accuracy: 0.7857 - val_loss: 0.6176 - val_accuracy: 0.6429\n",
      "Epoch 176/205\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.6206 - accuracy: 0.7857 - val_loss: 0.6174 - val_accuracy: 0.6429\n",
      "Epoch 177/205\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.6202 - accuracy: 0.7857 - val_loss: 0.6173 - val_accuracy: 0.6429\n",
      "Epoch 178/205\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6198 - accuracy: 0.7857 - val_loss: 0.6171 - val_accuracy: 0.6429\n",
      "Epoch 179/205\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.6194 - accuracy: 0.7857 - val_loss: 0.6170 - val_accuracy: 0.6429\n",
      "Epoch 180/205\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6189 - accuracy: 0.7857 - val_loss: 0.6169 - val_accuracy: 0.6429\n",
      "Epoch 181/205\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6185 - accuracy: 0.7857 - val_loss: 0.6167 - val_accuracy: 0.6429\n",
      "Epoch 182/205\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.6181 - accuracy: 0.7857 - val_loss: 0.6166 - val_accuracy: 0.6429\n",
      "Epoch 183/205\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.6177 - accuracy: 0.7857 - val_loss: 0.6164 - val_accuracy: 0.6429\n",
      "Epoch 184/205\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.6173 - accuracy: 0.7857 - val_loss: 0.6163 - val_accuracy: 0.6429\n",
      "Epoch 185/205\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.6168 - accuracy: 0.7857 - val_loss: 0.6161 - val_accuracy: 0.6429\n",
      "Epoch 186/205\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.6164 - accuracy: 0.7857 - val_loss: 0.6160 - val_accuracy: 0.6429\n",
      "Epoch 187/205\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.6160 - accuracy: 0.7857 - val_loss: 0.6159 - val_accuracy: 0.6429\n",
      "Epoch 188/205\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.6156 - accuracy: 0.7857 - val_loss: 0.6157 - val_accuracy: 0.6429\n",
      "Epoch 189/205\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.6152 - accuracy: 0.7857 - val_loss: 0.6156 - val_accuracy: 0.6429\n",
      "Epoch 190/205\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6147 - accuracy: 0.7857 - val_loss: 0.6154 - val_accuracy: 0.6429\n",
      "Epoch 191/205\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6143 - accuracy: 0.7857 - val_loss: 0.6153 - val_accuracy: 0.6429\n",
      "Epoch 192/205\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.6139 - accuracy: 0.7857 - val_loss: 0.6151 - val_accuracy: 0.6429\n",
      "Epoch 193/205\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.6135 - accuracy: 0.7857 - val_loss: 0.6150 - val_accuracy: 0.6429\n",
      "Epoch 194/205\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.6131 - accuracy: 0.7857 - val_loss: 0.6149 - val_accuracy: 0.6429\n",
      "Epoch 195/205\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.6127 - accuracy: 0.7857 - val_loss: 0.6147 - val_accuracy: 0.6429\n",
      "Epoch 196/205\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.6122 - accuracy: 0.7857 - val_loss: 0.6146 - val_accuracy: 0.6429\n",
      "Epoch 197/205\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.6118 - accuracy: 0.7857 - val_loss: 0.6144 - val_accuracy: 0.6429\n",
      "Epoch 198/205\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.6114 - accuracy: 0.7857 - val_loss: 0.6143 - val_accuracy: 0.6429\n",
      "Epoch 199/205\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.6110 - accuracy: 0.8036 - val_loss: 0.6141 - val_accuracy: 0.6429\n",
      "Epoch 200/205\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.6106 - accuracy: 0.8036 - val_loss: 0.6140 - val_accuracy: 0.6429\n",
      "Epoch 201/205\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6102 - accuracy: 0.8036 - val_loss: 0.6139 - val_accuracy: 0.6429\n",
      "Epoch 202/205\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6097 - accuracy: 0.8036 - val_loss: 0.6137 - val_accuracy: 0.6429\n",
      "Epoch 203/205\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6093 - accuracy: 0.8036 - val_loss: 0.6136 - val_accuracy: 0.6429\n",
      "Epoch 204/205\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.6089 - accuracy: 0.8036 - val_loss: 0.6134 - val_accuracy: 0.6429\n",
      "Epoch 205/205\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.6085 - accuracy: 0.8036 - val_loss: 0.6133 - val_accuracy: 0.6429\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.5842 - accuracy: 0.9000\n",
      "\n",
      "Fold  1\n",
      "\n",
      "Test accuracy:  0.8999999761581421\n",
      "Test loss:  0.5841730237007141\n",
      "\n",
      "Fold  2\n",
      "\n",
      "Training with 56 samples and validating with 14 samples\n",
      "\n",
      "Epoch 1/205\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 0.6168 - accuracy: 0.7857 - val_loss: 0.5782 - val_accuracy: 0.7857\n",
      "Epoch 2/205\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.6164 - accuracy: 0.7857 - val_loss: 0.5780 - val_accuracy: 0.7857\n",
      "Epoch 3/205\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.6159 - accuracy: 0.7857 - val_loss: 0.5778 - val_accuracy: 0.7857\n",
      "Epoch 4/205\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.6154 - accuracy: 0.7857 - val_loss: 0.5777 - val_accuracy: 0.7857\n",
      "Epoch 5/205\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.6148 - accuracy: 0.7857 - val_loss: 0.5777 - val_accuracy: 0.7857\n",
      "Epoch 6/205\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.6142 - accuracy: 0.7857 - val_loss: 0.5776 - val_accuracy: 0.7857\n",
      "Epoch 7/205\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.6136 - accuracy: 0.7857 - val_loss: 0.5776 - val_accuracy: 0.7857\n",
      "Epoch 8/205\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.6130 - accuracy: 0.7857 - val_loss: 0.5776 - val_accuracy: 0.7857\n",
      "Epoch 9/205\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.6124 - accuracy: 0.7857 - val_loss: 0.5776 - val_accuracy: 0.7857\n",
      "Epoch 10/205\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.6117 - accuracy: 0.7857 - val_loss: 0.5776 - val_accuracy: 0.7857\n",
      "Epoch 11/205\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.6111 - accuracy: 0.7857 - val_loss: 0.5776 - val_accuracy: 0.7857\n",
      "Epoch 12/205\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.6104 - accuracy: 0.7857 - val_loss: 0.5777 - val_accuracy: 0.7857\n",
      "Epoch 13/205\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6098 - accuracy: 0.7857 - val_loss: 0.5777 - val_accuracy: 0.7857\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00013: early stopping\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.5834 - accuracy: 0.9000\n",
      "\n",
      "Fold  2\n",
      "\n",
      "Test accuracy:  0.8999999761581421\n",
      "Test loss:  0.5833666920661926\n",
      "\n",
      "Fold  3\n",
      "\n",
      "Training with 56 samples and validating with 14 samples\n",
      "\n",
      "Epoch 1/205\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.6154 - accuracy: 0.7500 - val_loss: 0.5652 - val_accuracy: 0.9286\n",
      "Epoch 2/205\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.6149 - accuracy: 0.7500 - val_loss: 0.5648 - val_accuracy: 0.9286\n",
      "Epoch 3/205\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6144 - accuracy: 0.7500 - val_loss: 0.5644 - val_accuracy: 0.9286\n",
      "Epoch 4/205\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.6138 - accuracy: 0.7500 - val_loss: 0.5642 - val_accuracy: 0.9286\n",
      "Epoch 5/205\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6133 - accuracy: 0.7500 - val_loss: 0.5640 - val_accuracy: 0.9286\n",
      "Epoch 6/205\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.6127 - accuracy: 0.7500 - val_loss: 0.5638 - val_accuracy: 0.9286\n",
      "Epoch 7/205\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.6122 - accuracy: 0.7500 - val_loss: 0.5637 - val_accuracy: 0.9286\n",
      "Epoch 8/205\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.6116 - accuracy: 0.7500 - val_loss: 0.5636 - val_accuracy: 0.9286\n",
      "Epoch 9/205\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6111 - accuracy: 0.7500 - val_loss: 0.5635 - val_accuracy: 0.9286\n",
      "Epoch 10/205\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.6105 - accuracy: 0.7679 - val_loss: 0.5635 - val_accuracy: 0.9286\n",
      "Epoch 11/205\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.6099 - accuracy: 0.7679 - val_loss: 0.5635 - val_accuracy: 0.9286\n",
      "Epoch 12/205\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6094 - accuracy: 0.7679 - val_loss: 0.5634 - val_accuracy: 0.9286\n",
      "Epoch 13/205\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.6088 - accuracy: 0.7679 - val_loss: 0.5634 - val_accuracy: 0.9286\n",
      "Epoch 14/205\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6083 - accuracy: 0.7857 - val_loss: 0.5634 - val_accuracy: 0.9286\n",
      "Epoch 15/205\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.6077 - accuracy: 0.7857 - val_loss: 0.5634 - val_accuracy: 0.9286\n",
      "Epoch 16/205\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.6072 - accuracy: 0.7857 - val_loss: 0.5634 - val_accuracy: 1.0000\n",
      "Epoch 17/205\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.6067 - accuracy: 0.8036 - val_loss: 0.5634 - val_accuracy: 1.0000\n",
      "Epoch 18/205\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.6061 - accuracy: 0.8036 - val_loss: 0.5634 - val_accuracy: 1.0000\n",
      "Epoch 19/205\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6056 - accuracy: 0.8036 - val_loss: 0.5634 - val_accuracy: 1.0000\n",
      "Epoch 20/205\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.6051 - accuracy: 0.7857 - val_loss: 0.5634 - val_accuracy: 1.0000\n",
      "Epoch 21/205\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.6045 - accuracy: 0.7857 - val_loss: 0.5634 - val_accuracy: 1.0000\n",
      "Epoch 22/205\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6040 - accuracy: 0.7857 - val_loss: 0.5634 - val_accuracy: 1.0000\n",
      "Epoch 23/205\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.6035 - accuracy: 0.7857 - val_loss: 0.5634 - val_accuracy: 1.0000\n",
      "Epoch 24/205\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6029 - accuracy: 0.7857 - val_loss: 0.5634 - val_accuracy: 1.0000\n",
      "Epoch 25/205\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6024 - accuracy: 0.8036 - val_loss: 0.5634 - val_accuracy: 1.0000\n",
      "Epoch 26/205\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.6019 - accuracy: 0.8036 - val_loss: 0.5634 - val_accuracy: 1.0000\n",
      "Epoch 27/205\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.6014 - accuracy: 0.8036 - val_loss: 0.5634 - val_accuracy: 1.0000\n",
      "Epoch 28/205\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6009 - accuracy: 0.8036 - val_loss: 0.5634 - val_accuracy: 1.0000\n",
      "Epoch 29/205\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.6004 - accuracy: 0.8036 - val_loss: 0.5634 - val_accuracy: 0.9286\n",
      "Epoch 30/205\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.5999 - accuracy: 0.8036 - val_loss: 0.5634 - val_accuracy: 0.9286\n",
      "Epoch 31/205\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.5994 - accuracy: 0.8036 - val_loss: 0.5633 - val_accuracy: 0.9286\n",
      "Epoch 32/205\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.5988 - accuracy: 0.8036 - val_loss: 0.5633 - val_accuracy: 0.9286\n",
      "Epoch 33/205\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.5983 - accuracy: 0.8036 - val_loss: 0.5633 - val_accuracy: 0.9286\n",
      "Epoch 34/205\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.5978 - accuracy: 0.8036 - val_loss: 0.5633 - val_accuracy: 0.9286\n",
      "Epoch 35/205\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.5974 - accuracy: 0.8036 - val_loss: 0.5633 - val_accuracy: 0.9286\n",
      "Epoch 36/205\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.5969 - accuracy: 0.8036 - val_loss: 0.5633 - val_accuracy: 0.9286\n",
      "Epoch 37/205\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.5964 - accuracy: 0.8036 - val_loss: 0.5633 - val_accuracy: 0.9286\n",
      "Epoch 38/205\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.5959 - accuracy: 0.8036 - val_loss: 0.5633 - val_accuracy: 0.9286\n",
      "Epoch 39/205\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.5954 - accuracy: 0.8036 - val_loss: 0.5633 - val_accuracy: 0.9286\n",
      "Epoch 40/205\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.5949 - accuracy: 0.8036 - val_loss: 0.5633 - val_accuracy: 0.9286\n",
      "Epoch 41/205\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.5944 - accuracy: 0.8036 - val_loss: 0.5633 - val_accuracy: 0.9286\n",
      "Epoch 42/205\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.5939 - accuracy: 0.8036 - val_loss: 0.5633 - val_accuracy: 0.9286\n",
      "Epoch 43/205\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.5935 - accuracy: 0.8214 - val_loss: 0.5633 - val_accuracy: 0.9286\n",
      "Epoch 44/205\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.5930 - accuracy: 0.8214 - val_loss: 0.5632 - val_accuracy: 0.9286\n",
      "Epoch 45/205\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.5925 - accuracy: 0.8214 - val_loss: 0.5632 - val_accuracy: 0.9286\n",
      "Epoch 46/205\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.5920 - accuracy: 0.8214 - val_loss: 0.5632 - val_accuracy: 0.9286\n",
      "Epoch 47/205\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.5916 - accuracy: 0.8214 - val_loss: 0.5632 - val_accuracy: 0.9286\n",
      "Epoch 48/205\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.5911 - accuracy: 0.8214 - val_loss: 0.5632 - val_accuracy: 0.9286\n",
      "Epoch 49/205\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.5906 - accuracy: 0.8214 - val_loss: 0.5632 - val_accuracy: 0.9286\n",
      "Epoch 50/205\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.5902 - accuracy: 0.8214 - val_loss: 0.5631 - val_accuracy: 0.9286\n",
      "Epoch 51/205\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.5897 - accuracy: 0.8214 - val_loss: 0.5631 - val_accuracy: 0.9286\n",
      "Epoch 52/205\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.5892 - accuracy: 0.8214 - val_loss: 0.5631 - val_accuracy: 0.9286\n",
      "Epoch 53/205\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.5888 - accuracy: 0.8214 - val_loss: 0.5631 - val_accuracy: 0.9286\n",
      "Epoch 54/205\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.5883 - accuracy: 0.8214 - val_loss: 0.5631 - val_accuracy: 0.9286\n",
      "Epoch 55/205\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.5878 - accuracy: 0.8214 - val_loss: 0.5630 - val_accuracy: 0.9286\n",
      "Epoch 56/205\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.5874 - accuracy: 0.8214 - val_loss: 0.5630 - val_accuracy: 0.9286\n",
      "Epoch 57/205\n",
      "2/2 [==============================] - 0s 197ms/step - loss: 0.5869 - accuracy: 0.8214 - val_loss: 0.5630 - val_accuracy: 0.9286\n",
      "Epoch 58/205\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.5865 - accuracy: 0.8214 - val_loss: 0.5630 - val_accuracy: 0.9286\n",
      "Epoch 59/205\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.5860 - accuracy: 0.8214 - val_loss: 0.5629 - val_accuracy: 0.9286\n",
      "Epoch 60/205\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.5856 - accuracy: 0.8214 - val_loss: 0.5629 - val_accuracy: 0.9286\n",
      "Epoch 61/205\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.5851 - accuracy: 0.8393 - val_loss: 0.5629 - val_accuracy: 0.9286\n",
      "Epoch 62/205\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.5847 - accuracy: 0.8393 - val_loss: 0.5629 - val_accuracy: 0.9286\n",
      "Epoch 63/205\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.5842 - accuracy: 0.8393 - val_loss: 0.5628 - val_accuracy: 0.9286\n",
      "Epoch 64/205\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.5838 - accuracy: 0.8393 - val_loss: 0.5628 - val_accuracy: 0.9286\n",
      "Epoch 65/205\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.5833 - accuracy: 0.8393 - val_loss: 0.5628 - val_accuracy: 0.9286\n",
      "Epoch 66/205\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.5829 - accuracy: 0.8393 - val_loss: 0.5628 - val_accuracy: 0.9286\n",
      "Epoch 67/205\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.5825 - accuracy: 0.8393 - val_loss: 0.5627 - val_accuracy: 0.9286\n",
      "Epoch 68/205\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.5820 - accuracy: 0.8393 - val_loss: 0.5627 - val_accuracy: 0.9286\n",
      "Epoch 69/205\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.5816 - accuracy: 0.8393 - val_loss: 0.5627 - val_accuracy: 0.9286\n",
      "Epoch 70/205\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.5811 - accuracy: 0.8393 - val_loss: 0.5626 - val_accuracy: 0.9286\n",
      "Epoch 71/205\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.5807 - accuracy: 0.8393 - val_loss: 0.5626 - val_accuracy: 0.9286\n",
      "Epoch 72/205\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.5803 - accuracy: 0.8393 - val_loss: 0.5626 - val_accuracy: 0.9286\n",
      "Epoch 73/205\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.5798 - accuracy: 0.8393 - val_loss: 0.5625 - val_accuracy: 0.9286\n",
      "Epoch 74/205\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.5794 - accuracy: 0.8393 - val_loss: 0.5625 - val_accuracy: 0.9286\n",
      "Epoch 75/205\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.5790 - accuracy: 0.8393 - val_loss: 0.5625 - val_accuracy: 0.9286\n",
      "Epoch 76/205\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.5785 - accuracy: 0.8393 - val_loss: 0.5624 - val_accuracy: 0.9286\n",
      "Epoch 77/205\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.5781 - accuracy: 0.8393 - val_loss: 0.5624 - val_accuracy: 0.9286\n",
      "Epoch 78/205\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.5777 - accuracy: 0.8393 - val_loss: 0.5624 - val_accuracy: 0.9286\n",
      "Epoch 79/205\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.5773 - accuracy: 0.8393 - val_loss: 0.5623 - val_accuracy: 0.9286\n",
      "Epoch 80/205\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.5768 - accuracy: 0.8393 - val_loss: 0.5623 - val_accuracy: 0.9286\n",
      "Epoch 81/205\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.5764 - accuracy: 0.8393 - val_loss: 0.5622 - val_accuracy: 0.9286\n",
      "Epoch 82/205\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.5760 - accuracy: 0.8393 - val_loss: 0.5622 - val_accuracy: 0.9286\n",
      "Epoch 83/205\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.5756 - accuracy: 0.8393 - val_loss: 0.5622 - val_accuracy: 0.9286\n",
      "Epoch 84/205\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.5751 - accuracy: 0.8393 - val_loss: 0.5621 - val_accuracy: 0.9286\n",
      "Epoch 85/205\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.5747 - accuracy: 0.8393 - val_loss: 0.5621 - val_accuracy: 0.9286\n",
      "Epoch 86/205\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.5743 - accuracy: 0.8393 - val_loss: 0.5621 - val_accuracy: 0.9286\n",
      "Epoch 87/205\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.5739 - accuracy: 0.8393 - val_loss: 0.5620 - val_accuracy: 0.9286\n",
      "Epoch 88/205\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.5735 - accuracy: 0.8393 - val_loss: 0.5620 - val_accuracy: 0.9286\n",
      "Epoch 89/205\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.5730 - accuracy: 0.8393 - val_loss: 0.5619 - val_accuracy: 0.9286\n",
      "Epoch 90/205\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.5726 - accuracy: 0.8393 - val_loss: 0.5619 - val_accuracy: 0.9286\n",
      "Epoch 91/205\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.5722 - accuracy: 0.8393 - val_loss: 0.5618 - val_accuracy: 0.9286\n",
      "Epoch 92/205\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.5718 - accuracy: 0.8393 - val_loss: 0.5618 - val_accuracy: 0.9286\n",
      "Epoch 93/205\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.5714 - accuracy: 0.8393 - val_loss: 0.5618 - val_accuracy: 0.9286\n",
      "Epoch 94/205\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.5710 - accuracy: 0.8393 - val_loss: 0.5617 - val_accuracy: 0.9286\n",
      "Epoch 95/205\n",
      "2/2 [==============================] - 0s 115ms/step - loss: 0.5706 - accuracy: 0.8393 - val_loss: 0.5617 - val_accuracy: 0.9286\n",
      "Epoch 96/205\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.5702 - accuracy: 0.8393 - val_loss: 0.5616 - val_accuracy: 0.9286\n",
      "Epoch 97/205\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.5697 - accuracy: 0.8393 - val_loss: 0.5616 - val_accuracy: 0.9286\n",
      "Epoch 98/205\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.5693 - accuracy: 0.8393 - val_loss: 0.5615 - val_accuracy: 0.9286\n",
      "Epoch 99/205\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.5689 - accuracy: 0.8393 - val_loss: 0.5615 - val_accuracy: 0.9286\n",
      "Epoch 100/205\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.5685 - accuracy: 0.8393 - val_loss: 0.5614 - val_accuracy: 0.9286\n",
      "Epoch 101/205\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.5681 - accuracy: 0.8393 - val_loss: 0.5614 - val_accuracy: 0.9286\n",
      "Epoch 102/205\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.5677 - accuracy: 0.8393 - val_loss: 0.5614 - val_accuracy: 0.9286\n",
      "Epoch 103/205\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.5673 - accuracy: 0.8393 - val_loss: 0.5613 - val_accuracy: 0.9286\n",
      "Epoch 104/205\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.5669 - accuracy: 0.8393 - val_loss: 0.5613 - val_accuracy: 0.9286\n",
      "Epoch 105/205\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.5665 - accuracy: 0.8393 - val_loss: 0.5612 - val_accuracy: 0.9286\n",
      "Epoch 106/205\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.5661 - accuracy: 0.8393 - val_loss: 0.5612 - val_accuracy: 0.9286\n",
      "Epoch 107/205\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.5657 - accuracy: 0.8393 - val_loss: 0.5611 - val_accuracy: 0.9286\n",
      "Epoch 108/205\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.5653 - accuracy: 0.8393 - val_loss: 0.5611 - val_accuracy: 0.9286\n",
      "Epoch 109/205\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.5649 - accuracy: 0.8393 - val_loss: 0.5610 - val_accuracy: 0.9286\n",
      "Epoch 110/205\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.5645 - accuracy: 0.8393 - val_loss: 0.5610 - val_accuracy: 0.9286\n",
      "Epoch 111/205\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.5641 - accuracy: 0.8393 - val_loss: 0.5609 - val_accuracy: 0.9286\n",
      "Epoch 112/205\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.5637 - accuracy: 0.8393 - val_loss: 0.5609 - val_accuracy: 0.9286\n",
      "Epoch 113/205\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.5633 - accuracy: 0.8393 - val_loss: 0.5608 - val_accuracy: 0.9286\n",
      "Epoch 114/205\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.5629 - accuracy: 0.8393 - val_loss: 0.5608 - val_accuracy: 0.9286\n",
      "Epoch 115/205\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.5625 - accuracy: 0.8571 - val_loss: 0.5607 - val_accuracy: 0.9286\n",
      "Epoch 116/205\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.5621 - accuracy: 0.8571 - val_loss: 0.5607 - val_accuracy: 0.9286\n",
      "Epoch 117/205\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.5618 - accuracy: 0.8750 - val_loss: 0.5606 - val_accuracy: 0.9286\n",
      "Epoch 118/205\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.5614 - accuracy: 0.8750 - val_loss: 0.5606 - val_accuracy: 0.9286\n",
      "Epoch 119/205\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.5610 - accuracy: 0.8750 - val_loss: 0.5605 - val_accuracy: 0.9286\n",
      "Epoch 120/205\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.5606 - accuracy: 0.8750 - val_loss: 0.5604 - val_accuracy: 0.9286\n",
      "Epoch 121/205\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.5602 - accuracy: 0.8750 - val_loss: 0.5604 - val_accuracy: 0.9286\n",
      "Epoch 122/205\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.5598 - accuracy: 0.8750 - val_loss: 0.5603 - val_accuracy: 0.9286\n",
      "Epoch 123/205\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.5594 - accuracy: 0.8750 - val_loss: 0.5603 - val_accuracy: 0.9286\n",
      "Epoch 124/205\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.5590 - accuracy: 0.8750 - val_loss: 0.5602 - val_accuracy: 0.9286\n",
      "Epoch 125/205\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.5586 - accuracy: 0.8750 - val_loss: 0.5602 - val_accuracy: 0.9286\n",
      "Epoch 126/205\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.5583 - accuracy: 0.8750 - val_loss: 0.5601 - val_accuracy: 0.9286\n",
      "Epoch 127/205\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.5579 - accuracy: 0.8750 - val_loss: 0.5601 - val_accuracy: 0.9286\n",
      "Epoch 128/205\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.5575 - accuracy: 0.8750 - val_loss: 0.5600 - val_accuracy: 0.9286\n",
      "Epoch 129/205\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.5571 - accuracy: 0.8750 - val_loss: 0.5600 - val_accuracy: 0.9286\n",
      "Epoch 130/205\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.5567 - accuracy: 0.8750 - val_loss: 0.5599 - val_accuracy: 0.9286\n",
      "Epoch 131/205\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.5563 - accuracy: 0.8750 - val_loss: 0.5598 - val_accuracy: 0.9286\n",
      "Epoch 132/205\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.5560 - accuracy: 0.8750 - val_loss: 0.5598 - val_accuracy: 0.9286\n",
      "Epoch 133/205\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.5556 - accuracy: 0.8750 - val_loss: 0.5597 - val_accuracy: 0.9286\n",
      "Epoch 134/205\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.5552 - accuracy: 0.8750 - val_loss: 0.5597 - val_accuracy: 0.9286\n",
      "Epoch 135/205\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.5548 - accuracy: 0.8750 - val_loss: 0.5596 - val_accuracy: 0.9286\n",
      "Epoch 136/205\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.5544 - accuracy: 0.8750 - val_loss: 0.5596 - val_accuracy: 0.9286\n",
      "Epoch 137/205\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.5541 - accuracy: 0.8750 - val_loss: 0.5595 - val_accuracy: 0.9286\n",
      "Epoch 138/205\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.5537 - accuracy: 0.8750 - val_loss: 0.5594 - val_accuracy: 0.9286\n",
      "Epoch 139/205\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.5533 - accuracy: 0.8750 - val_loss: 0.5594 - val_accuracy: 0.9286\n",
      "Epoch 140/205\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.5529 - accuracy: 0.8750 - val_loss: 0.5593 - val_accuracy: 0.9286\n",
      "Epoch 141/205\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.5526 - accuracy: 0.8750 - val_loss: 0.5593 - val_accuracy: 0.9286\n",
      "Epoch 142/205\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.5522 - accuracy: 0.8750 - val_loss: 0.5592 - val_accuracy: 0.9286\n",
      "Epoch 143/205\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.5518 - accuracy: 0.8750 - val_loss: 0.5592 - val_accuracy: 0.9286\n",
      "Epoch 144/205\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.5514 - accuracy: 0.8750 - val_loss: 0.5591 - val_accuracy: 0.9286\n",
      "Epoch 145/205\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.5511 - accuracy: 0.8750 - val_loss: 0.5590 - val_accuracy: 0.9286\n",
      "Epoch 146/205\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.5507 - accuracy: 0.8750 - val_loss: 0.5590 - val_accuracy: 0.9286\n",
      "Epoch 147/205\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.5503 - accuracy: 0.8750 - val_loss: 0.5589 - val_accuracy: 0.9286\n",
      "Epoch 148/205\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.5500 - accuracy: 0.8750 - val_loss: 0.5589 - val_accuracy: 0.9286\n",
      "Epoch 149/205\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.5496 - accuracy: 0.8750 - val_loss: 0.5588 - val_accuracy: 0.9286\n",
      "Epoch 150/205\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.5492 - accuracy: 0.8750 - val_loss: 0.5587 - val_accuracy: 0.9286\n",
      "Epoch 151/205\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.5488 - accuracy: 0.8750 - val_loss: 0.5587 - val_accuracy: 0.9286\n",
      "Epoch 152/205\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.5485 - accuracy: 0.8750 - val_loss: 0.5586 - val_accuracy: 0.9286\n",
      "Epoch 153/205\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.5481 - accuracy: 0.8750 - val_loss: 0.5585 - val_accuracy: 0.9286\n",
      "Epoch 154/205\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.5477 - accuracy: 0.8750 - val_loss: 0.5585 - val_accuracy: 0.9286\n",
      "Epoch 155/205\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.5474 - accuracy: 0.8750 - val_loss: 0.5584 - val_accuracy: 0.9286\n",
      "Epoch 156/205\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.5470 - accuracy: 0.8750 - val_loss: 0.5584 - val_accuracy: 0.9286\n",
      "Epoch 157/205\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.5466 - accuracy: 0.8750 - val_loss: 0.5583 - val_accuracy: 0.9286\n",
      "Epoch 158/205\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.5463 - accuracy: 0.8750 - val_loss: 0.5582 - val_accuracy: 0.9286\n",
      "Epoch 159/205\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.5459 - accuracy: 0.8750 - val_loss: 0.5582 - val_accuracy: 0.9286\n",
      "Epoch 160/205\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.5456 - accuracy: 0.8750 - val_loss: 0.5581 - val_accuracy: 0.9286\n",
      "Epoch 161/205\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.5452 - accuracy: 0.8750 - val_loss: 0.5581 - val_accuracy: 0.9286\n",
      "Epoch 162/205\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.5448 - accuracy: 0.8750 - val_loss: 0.5580 - val_accuracy: 0.9286\n",
      "Epoch 163/205\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.5445 - accuracy: 0.8750 - val_loss: 0.5579 - val_accuracy: 0.9286\n",
      "Epoch 164/205\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.5441 - accuracy: 0.8750 - val_loss: 0.5579 - val_accuracy: 0.9286\n",
      "Epoch 165/205\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.5437 - accuracy: 0.8750 - val_loss: 0.5578 - val_accuracy: 0.9286\n",
      "Epoch 166/205\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.5434 - accuracy: 0.8750 - val_loss: 0.5577 - val_accuracy: 0.9286\n",
      "Epoch 167/205\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.5430 - accuracy: 0.8750 - val_loss: 0.5577 - val_accuracy: 0.9286\n",
      "Epoch 168/205\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.5427 - accuracy: 0.8750 - val_loss: 0.5576 - val_accuracy: 0.9286\n",
      "Epoch 169/205\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.5423 - accuracy: 0.8750 - val_loss: 0.5575 - val_accuracy: 0.9286\n",
      "Epoch 170/205\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.5419 - accuracy: 0.8750 - val_loss: 0.5575 - val_accuracy: 0.9286\n",
      "Epoch 171/205\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.5416 - accuracy: 0.8750 - val_loss: 0.5574 - val_accuracy: 0.9286\n",
      "Epoch 172/205\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.5412 - accuracy: 0.8750 - val_loss: 0.5573 - val_accuracy: 0.9286\n",
      "Epoch 173/205\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.5409 - accuracy: 0.8750 - val_loss: 0.5573 - val_accuracy: 0.9286\n",
      "Epoch 174/205\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.5405 - accuracy: 0.8750 - val_loss: 0.5572 - val_accuracy: 0.9286\n",
      "Epoch 175/205\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.5402 - accuracy: 0.8750 - val_loss: 0.5572 - val_accuracy: 0.9286\n",
      "Epoch 176/205\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.5398 - accuracy: 0.8750 - val_loss: 0.5571 - val_accuracy: 0.9286\n",
      "Epoch 177/205\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.5394 - accuracy: 0.8929 - val_loss: 0.5570 - val_accuracy: 0.9286\n",
      "Epoch 178/205\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.5391 - accuracy: 0.8929 - val_loss: 0.5570 - val_accuracy: 0.9286\n",
      "Epoch 179/205\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.5387 - accuracy: 0.8929 - val_loss: 0.5569 - val_accuracy: 0.9286\n",
      "Epoch 180/205\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.5384 - accuracy: 0.9107 - val_loss: 0.5568 - val_accuracy: 0.9286\n",
      "Epoch 181/205\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.5380 - accuracy: 0.9107 - val_loss: 0.5568 - val_accuracy: 0.9286\n",
      "Epoch 182/205\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.5377 - accuracy: 0.9107 - val_loss: 0.5567 - val_accuracy: 0.9286\n",
      "Epoch 183/205\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.5373 - accuracy: 0.9107 - val_loss: 0.5566 - val_accuracy: 0.9286\n",
      "Epoch 184/205\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.5370 - accuracy: 0.9107 - val_loss: 0.5566 - val_accuracy: 0.9286\n",
      "Epoch 185/205\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.5366 - accuracy: 0.9107 - val_loss: 0.5565 - val_accuracy: 0.9286\n",
      "Epoch 186/205\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.5363 - accuracy: 0.9107 - val_loss: 0.5564 - val_accuracy: 0.9286\n",
      "Epoch 187/205\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.5359 - accuracy: 0.9107 - val_loss: 0.5564 - val_accuracy: 0.9286\n",
      "Epoch 188/205\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.5356 - accuracy: 0.9107 - val_loss: 0.5563 - val_accuracy: 0.9286\n",
      "Epoch 189/205\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.5352 - accuracy: 0.9107 - val_loss: 0.5562 - val_accuracy: 0.9286\n",
      "Epoch 190/205\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.5349 - accuracy: 0.9107 - val_loss: 0.5562 - val_accuracy: 0.9286\n",
      "Epoch 191/205\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.5345 - accuracy: 0.9107 - val_loss: 0.5561 - val_accuracy: 0.9286\n",
      "Epoch 192/205\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.5342 - accuracy: 0.9107 - val_loss: 0.5560 - val_accuracy: 0.9286\n",
      "Epoch 193/205\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.5338 - accuracy: 0.9107 - val_loss: 0.5560 - val_accuracy: 0.9286\n",
      "Epoch 194/205\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.5335 - accuracy: 0.9107 - val_loss: 0.5559 - val_accuracy: 0.9286\n",
      "Epoch 195/205\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.5331 - accuracy: 0.9107 - val_loss: 0.5558 - val_accuracy: 0.9286\n",
      "Epoch 196/205\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.5328 - accuracy: 0.9286 - val_loss: 0.5558 - val_accuracy: 0.9286\n",
      "Epoch 197/205\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.5325 - accuracy: 0.9286 - val_loss: 0.5557 - val_accuracy: 0.9286\n",
      "Epoch 198/205\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.5321 - accuracy: 0.9286 - val_loss: 0.5556 - val_accuracy: 0.9286\n",
      "Epoch 199/205\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.5318 - accuracy: 0.9286 - val_loss: 0.5556 - val_accuracy: 0.9286\n",
      "Epoch 200/205\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.5314 - accuracy: 0.9286 - val_loss: 0.5555 - val_accuracy: 0.9286\n",
      "Epoch 201/205\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.5311 - accuracy: 0.9286 - val_loss: 0.5554 - val_accuracy: 0.9286\n",
      "Epoch 202/205\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.5307 - accuracy: 0.9286 - val_loss: 0.5553 - val_accuracy: 0.9286\n",
      "Epoch 203/205\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.5304 - accuracy: 0.9286 - val_loss: 0.5553 - val_accuracy: 0.9286\n",
      "Epoch 204/205\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.5300 - accuracy: 0.9286 - val_loss: 0.5552 - val_accuracy: 0.9286\n",
      "Epoch 205/205\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.5297 - accuracy: 0.9286 - val_loss: 0.5551 - val_accuracy: 0.9286\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.5575 - accuracy: 0.9000\n",
      "\n",
      "Fold  3\n",
      "\n",
      "Test accuracy:  0.8999999761581421\n",
      "Test loss:  0.5575143098831177\n",
      "\n",
      "Fold  4\n",
      "\n",
      "Training with 56 samples and validating with 14 samples\n",
      "\n",
      "Epoch 1/205\n",
      "2/2 [==============================] - 0s 117ms/step - loss: 0.5349 - accuracy: 0.9107 - val_loss: 0.5330 - val_accuracy: 1.0000\n",
      "Epoch 2/205\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.5345 - accuracy: 0.9107 - val_loss: 0.5328 - val_accuracy: 1.0000\n",
      "Epoch 3/205\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.5342 - accuracy: 0.9107 - val_loss: 0.5326 - val_accuracy: 1.0000\n",
      "Epoch 4/205\n",
      "2/2 [==============================] - 0s 110ms/step - loss: 0.5337 - accuracy: 0.8929 - val_loss: 0.5325 - val_accuracy: 1.0000\n",
      "Epoch 5/205\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.5333 - accuracy: 0.8929 - val_loss: 0.5325 - val_accuracy: 1.0000\n",
      "Epoch 6/205\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.5329 - accuracy: 0.8929 - val_loss: 0.5325 - val_accuracy: 1.0000\n",
      "Epoch 7/205\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.5324 - accuracy: 0.8929 - val_loss: 0.5325 - val_accuracy: 1.0000\n",
      "Epoch 8/205\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.5319 - accuracy: 0.8929 - val_loss: 0.5325 - val_accuracy: 1.0000\n",
      "Epoch 9/205\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.5314 - accuracy: 0.8929 - val_loss: 0.5326 - val_accuracy: 1.0000\n",
      "Epoch 10/205\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.5309 - accuracy: 0.8750 - val_loss: 0.5327 - val_accuracy: 1.0000\n",
      "Epoch 11/205\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.5305 - accuracy: 0.8750 - val_loss: 0.5327 - val_accuracy: 1.0000\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00011: early stopping\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.5569 - accuracy: 0.9000\n",
      "\n",
      "Fold  4\n",
      "\n",
      "Test accuracy:  0.8999999761581421\n",
      "Test loss:  0.5569461584091187\n",
      "\n",
      "K-Fold accuracy:  [0.699999988079071, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421]\n",
      "\n",
      "Average accuracy:  0.8599999785423279\n",
      "K-Fold Standard deviation:  0.07999999523162842\n",
      "\n",
      "Classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       sober       1.00      0.80      0.89         5\n",
      "       drunk       0.83      1.00      0.91         5\n",
      "\n",
      "    accuracy                           0.90        10\n",
      "   macro avg       0.92      0.90      0.90        10\n",
      "weighted avg       0.92      0.90      0.90        10\n",
      "\n",
      "\n",
      "AUC ROC: \n",
      "0.9600000000000001\n",
      "\n",
      "Confusion Matrix: \n",
      "[[4 1]\n",
      " [0 5]]\n",
      "\n",
      "True Negatives:  4\n",
      "False Positives:  1\n",
      "False Negatives:  0\n",
      "True Positives:  5\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Defining random seeds to enable reproducibility\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(1)\n",
    " \n",
    "import random\n",
    "random.seed(1)\n",
    " \n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.python.client import device_lib\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "print(device_lib.list_local_devices())\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "\n",
    "def load_patras_dataset():\n",
    "    \"\"\"\n",
    "    Loads the datasets encoded in .pkl files and returns its decoded form.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        A list of n-dimensional arrays representing the subjects samples that will be used to \\\\\n",
    "        train the drunkenness classification model.\n",
    "    list\n",
    "        A list of n-dimensional arrays representing the subjects samples that will be used to \\\\\n",
    "        test the drunkenness classification model.\n",
    "    ndarray\n",
    "        A n-dimensional array representing the training set labels.\n",
    "    ndarray\n",
    "        A n-dimensional array representing the test set labels.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Loading Sober-Drunk Face Dataset, from Patras University\")\n",
    "    \n",
    "    # Defining the sample and label sets filenames\n",
    "    sets = [\n",
    "        \"Insert the balanced x_training-set.pkl file path here\",\n",
    "        \"Insert the balanced x_test-set.pkl file path here\",\n",
    "        \"Insert the balanced y_training-set.pkl file path here\",\n",
    "        \"Insert the balanced y_test-set.pkl file path here\"\n",
    "    ]\n",
    "    \n",
    "    # Defining an empty list for storing the decoded datasets\n",
    "    loaded_datasets = []\n",
    " \n",
    "    # Iterating over the dataset files\n",
    "    for set_ in sets:\n",
    "        # Opening the .pkl file in read mode\n",
    "        with open(set_, 'rb') as file:\n",
    "            # Appending the decoded dataset to the datasets list\n",
    "            loaded_datasets.append(pickle.load(file))\n",
    "    \n",
    "    # Unpacking the datasets list into individual subsets\n",
    "    x_train, x_test, y_train, y_test = loaded_datasets\n",
    "    \n",
    "    # Converting the label list to the n-dimensional array format\n",
    "    y_train= np.array(y_train)\n",
    "    y_test = np.array(y_test)\n",
    "    \n",
    "    # Printing the dataset length\n",
    "    print(\"\\nSamples total: {0}\".format((len(x_train)) + (len(x_test))))\n",
    "    \n",
    "    print(\"\\nDataset splitting: \")\n",
    "    \n",
    "    # Printing the training and test sets length\n",
    "    print(\"\\nTraining set: {0}\".format(len(x_train)))\n",
    "    print(\"Test set: {0}\".format(len(x_test)))\n",
    "    \n",
    "    # Returning the training and test sets, and its respective labels\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "\n",
    "def min_max_norm(dataset):\n",
    "    \"\"\"\n",
    "    Normalizes the keyframes according to the minimum-maximum norm, such that pixel \\\\\n",
    "    values ranges from 0 to 1.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : list\n",
    "        A list of n-dimensional arrays representing the subjects keyframes.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ndarray\n",
    "        A n-dimensional array representing keyframes with pixel values ranging from \\\\\n",
    "        0 to 1.\n",
    "    \"\"\"\n",
    "\n",
    "    # Converting the dataset type from list to n-dimensional array\n",
    "    dataset = np.asarray(dataset, dtype=\"int16\")\n",
    "\n",
    "    # Finding the keyframes minimum and maximum values\n",
    "    x_min = dataset.min(axis=(1, 2), keepdims=True)\n",
    "    x_max = dataset.max(axis=(1, 2), keepdims=True)\n",
    "\n",
    "    # Applying the minimum-maximum norm to each keyframe\n",
    "    norm_dataset = (dataset - x_min) / (x_max - x_min)\n",
    "\n",
    "    # Printing the minimum and maximum values from a given sample for sanity check\n",
    "    print(\"\\nMinMax normalization\")\n",
    "    print(\"dataset shape: \", norm_dataset.shape)\n",
    "    print(\"min: \", norm_dataset[0].min())\n",
    "    print(\"max: \", norm_dataset[0].max())\n",
    "\n",
    "    # Returning the normalized dataset\n",
    "    return norm_dataset\n",
    "\n",
    "\n",
    "def visualize_model_history(hist):\n",
    "    \"\"\"\n",
    "    Displays the training process loss and accuracy history from a given fold of the \\\\\n",
    "    stratified cross-validation procedure.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    hist : dictionary\n",
    "        A dictionary conataining loss and accuracy values lists from the training and \\\\\n",
    "        validation sets.\n",
    "    \"\"\"\n",
    "\n",
    "    # Displaying the training and validation loss history\n",
    "    plt.figure()\n",
    "    plt.plot(hist.history['loss'])\n",
    "    plt.plot(hist.history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Val'], loc='upper right')\n",
    "    \n",
    "    # Displaying the training and validation accuracy history\n",
    "    plt.figure()\n",
    "    plt.plot(hist.history['accuracy'])\n",
    "    plt.plot(hist.history['val_accuracy'])\n",
    "    plt.title('Model accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Val'], loc='lower right')\n",
    "    \n",
    "    # Showing the training process loss and accuracy history\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visualize_model_history_cv(training_loss, training_accuracy, validation_loss, validation_accuracy):\n",
    "    \"\"\"\n",
    "    Displays the training process loss and accuracy history throughout the \\\\\n",
    "    stratified k-fold cross-validation procedure.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    training_loss : list\n",
    "        A list conataining the loss values from the training set throught the \\\\\n",
    "        stratified k-fold cross-validation procedure.\n",
    "    training_accuracy : list\n",
    "        A list conataining the accuracy values from the training set throught the \\\\\n",
    "        stratified k-fold cross-validation procedure.\n",
    "    validation_loss : list\n",
    "        A list conataining the loss values from the validation set throught the \\\\\n",
    "        stratified k-fold cross-validation procedure.\n",
    "    validation_accuracy : list\n",
    "        A list conataining the accuracy values from the validation set throught the \\\\\n",
    "        stratified k-fold cross-validation procedure.\n",
    "    \"\"\"\n",
    "\n",
    "    # Displaying and saving the training and validation loss history\n",
    "    plt.figure()\n",
    "    plt.plot(training_loss)\n",
    "    plt.plot(validation_loss)\n",
    "    plt.title('Model loss (cross-validation history)')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Val'], loc='upper right')\n",
    "    plt.savefig(\"tl_learning-curve.pdf\", dpi=600, bbox_inches='tight', pad_inches=0)\n",
    "    \n",
    "    # Displaying the training and validation accuracy history\n",
    "    plt.figure()\n",
    "    plt.plot(training_accuracy)\n",
    "    plt.plot(validation_accuracy)\n",
    "    plt.title('Model accuracy (cross-validation history)')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Val'], loc='upper right')\n",
    "    \n",
    "    # Showing the training process loss and accuracy history\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def transfer_learning():\n",
    "    \"\"\"\n",
    "    Performs the transfer learning strategy considering a base model pre-trained \\\\ \n",
    "    with a large scale image dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    # Loading the training and test datasets\n",
    "    x_train, x_test, y_train, y_test = load_patras_dataset()\n",
    "\n",
    "    # Applying the min-max normalization\n",
    "    x_train = min_max_norm(x_train)\n",
    "    x_test = min_max_norm(x_test)\n",
    "    \n",
    "    # Reshaping datsets to the tensor format (channel last)\n",
    "    x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], x_train.shape[2], 3)\n",
    "    x_test = x_test.reshape(x_test.shape[0], x_test.shape[1], x_test.shape[2], 3)\n",
    "\n",
    "    # Finding the thermal images shape\n",
    "    imsize = (x_train.shape[1],x_train.shape[2],x_train.shape[3])\n",
    "\n",
    "    # Defining the pre-trained model new input layer shape\n",
    "    inp = keras.layers.Input(shape=(imsize[0], imsize[1], imsize[2]))\n",
    "    \n",
    "    # Defining a VGG pre-trained model with new input shape and without dense layers\n",
    "    base_model = keras.applications.VGG16(include_top=False, weights='imagenet', input_tensor=inp,\n",
    "                                          input_shape=(imsize[0], imsize[1], imsize[2]))\n",
    "    \n",
    "    # Defining the VGG model output as the last pooling layer feature maps\n",
    "    output = base_model.layers[-1].output\n",
    "    \n",
    "    # Changing the model 'trainable' parameter to False,\n",
    "    # this will prevent the base model from updating its\n",
    "    # weights during the new dense layers training process\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    # Iterating over the base model layers\n",
    "    for layer in base_model.layers:\n",
    "        # Freezing the layers weights\n",
    "        layer.trainable = False\n",
    "        # Printing the layers 'trainable' parameter for sanity check\n",
    "        print(\"{}: {}\".format(layer.name, layer.trainable))\n",
    "    \n",
    "    # Adding a Flatten layer to the base model output\n",
    "    flat = Flatten(name='flatten')(output)\n",
    "\n",
    "    # Defining an output layer to classify the received features as sober (0) or drunk (1)\n",
    "    prediction = keras.layers.Dense(units=1, activation='sigmoid', name='output')\n",
    "    \n",
    "    # Stacking the base model convolutional layers and the new dense layer for \n",
    "    # drunkenness classification\n",
    "    x = flat\n",
    "    x = prediction(x)\n",
    "    \n",
    "    # Defining a functional model for transfer learning\n",
    "    model = models.Model(inp, x)\n",
    "    \n",
    "\n",
    "    # Defining the stratified cross-validation folds\n",
    "    folds = list(StratifiedKFold(n_splits=5, shuffle=False, random_state=None).split(x_train, y_train))\n",
    "    \n",
    "    # Defining the optmization function\n",
    "    adam = optimizers.Adam(learning_rate=1e-5, amsgrad=False)\n",
    "    \n",
    "    # Defining the early stopping callback\n",
    "    callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)\n",
    "    \n",
    "    # Compiling the model\n",
    "    model.compile(loss=keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "                optimizer=adam,\n",
    "                metrics=['accuracy'])\n",
    "    \n",
    "    # Printing the model summary\n",
    "    model.summary()\n",
    "\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    # Instantiating empty lists for storing the model training and validation loss history\n",
    "    cv_training_acc = []\n",
    "    cv_training_loss = []\n",
    "    \n",
    "    # Instantiating empty lists for storing the model training and validation accuracy history\n",
    "    cv_val_acc = []\n",
    "    cv_val_loss = []\n",
    "    \n",
    "    # Instantiating an empty list for storing the model classification accuracies on the test set samples\n",
    "    fold_acc = []\n",
    "    \n",
    "    # Iterating over the stratified cross-validation folds\n",
    "    for j, (train_idx, val_idx) in enumerate(folds):\n",
    "        print('\\nFold ',j)\n",
    "        # Defining the training and validation sets\n",
    "        X_train_cv, y_train_cv = x_train[train_idx], y_train[train_idx]\n",
    "        X_valid_cv, y_valid_cv = x_train[val_idx], y_train[val_idx]\n",
    "\n",
    "        print(\"\\nTraining with {0} samples and validating with {1} samples\\n\".format(len(X_train_cv), len(X_valid_cv)))\n",
    "\n",
    "        # Fitting the model\n",
    "        history = model.fit(x=X_train_cv, y=y_train_cv, \n",
    "                          validation_data=(X_valid_cv, y_valid_cv),\n",
    "                          shuffle=False,\n",
    "                          batch_size=30,\n",
    "                          callbacks=[callback],\n",
    "                          epochs=205,\n",
    "                          verbose=1)\n",
    "        \n",
    "        # Updating the training loss and accuracy history lists\n",
    "        cv_training_acc += history.history['accuracy']\n",
    "        cv_training_loss += history.history['loss']\n",
    "\n",
    "        # Updating the validation loss and accuracy history lists\n",
    "        cv_val_acc += history.history['val_accuracy']\n",
    "        cv_val_loss += history.history['val_loss']\n",
    "        \n",
    "        # Evaluating the model on unseen data\n",
    "        test_loss, test_acc = model.evaluate(x_test, y_test, verbose=1)\n",
    "\n",
    "        # Appending the model classification accuracies to the test set accuracies list\n",
    "        fold_acc.append(test_acc)\n",
    "\n",
    "        # Printing the model classification accuracy on unseen data\n",
    "        print('\\nFold ',j)\n",
    "        print(\"\\nTest accuracy: \", test_acc)\n",
    "        print(\"Test loss: \", test_loss)\n",
    "        \n",
    "        # Saving the model obtained in the stratified cross-validation j-th fold\n",
    "        model.save('sober-drunk_tl-model-fold_{0}_205e.h5'.format(j))\n",
    "\n",
    "        # Uncomment to break the loop\n",
    "        #if j == 3:\n",
    "        #    break\n",
    "    \n",
    "    # Printing the model classification accuracy in each fold of the stratified \n",
    "    # cross-validation\n",
    "    print(\"\\nK-Fold accuracy: \", fold_acc)\n",
    "\n",
    "    # Printing the model average accuracy along the stratified k-fold cross-validation \n",
    "    # procedure\n",
    "    print(\"\\nAverage accuracy: \", np.mean(fold_acc))\n",
    "\n",
    "    # Printing the standard deviation of accuracies obtained throughout the \n",
    "    # stratified k-fold cross-validation procedure\n",
    "    print(\"K-Fold Standard deviation: \", np.std(fold_acc))\n",
    "    \n",
    "    \n",
    "    print(\"\\nClassification report: \")\n",
    "\n",
    "    # Obtaining the test samples class probabilities\n",
    "    y_prob = model.predict(x_test)\n",
    "\n",
    "    # Obtaining the binary label from the model output probabilities\n",
    "    y_hat = (y_prob > 0.5).astype(int)\n",
    "    \n",
    "    # Printing the classification performance report\n",
    "    report = classification_report(y_test, y_hat, target_names=['sober', 'drunk'])\n",
    "    print(report)\n",
    "    \n",
    "    # Printing the confusion matrix\n",
    "    print(\"\\nConfusion Matrix: \")\n",
    "    matrix = confusion_matrix(y_test, y_hat)\n",
    "    print(matrix)\n",
    "    \n",
    "    tn, fp, fn, tp = matrix.ravel()\n",
    "    print(\"\\nTrue Negatives: \", tn)\n",
    "    print(\"False Positives: \", fp)\n",
    "    print(\"False Negatives: \", fn)\n",
    "    print(\"True Positives: \", tp)\n",
    "    \n",
    "    # Displaying the training process history\n",
    "    visualize_model_history_cv(cv_training_loss, cv_training_acc, cv_val_loss, cv_val_acc)\n",
    "    visualize_model_history(history)\n",
    "    \n",
    "\n",
    "# Running the transfer learning\n",
    "transfer_learning()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 ('virtual-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 127.537754,
   "end_time": "2022-05-24T19:03:55.838992",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-05-24T19:01:48.301238",
   "version": "2.3.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "91dd42dc31c8286e87f4f857c1cf087015eed96189f5f719a42d322017809b0b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
